{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-00",
   "metadata": {},
   "source": "# Congressional Trading Signal Strategy\n## QPM Final Project — Chicago Booth\n\n**Objective:** Build and validate a systematic ETF trading strategy that aggregates Congressional trading disclosures into sector-level signals, tests whether those signals contain exploitable alpha, and evaluates strategy performance against standard factor models.\n\n**Hypotheses:**\n- **H1 (Market Underreaction):** `CAR[-1,+1] ≈ 0` but `CAR[+2,+20] > 0` for purchases — the market underreacts to congressional trade disclosures\n- **H2 (Committee Advantage):** Committee-relevant trades generate higher CARs than non-relevant trades\n\n**Pipeline:** Foundation → H1 Event Study → H2 Committee Analysis → Signal Engine → Backtest → Factor Regression\n\n**Critical Constraint:** All signals use `ReportDate` (disclosure date) — NEVER `TransactionDate`. The STOCK Act allows up to 45-day disclosure lag; this is a feature of the signal, not a bug."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-01",
   "metadata": {},
   "outputs": [],
   "source": "# ─── Imports ───\nimport os\nimport json\nimport warnings\nimport requests\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy import stats\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom tqdm import tqdm\ntry:\n    import pandas_datareader.data as web\nexcept ImportError:\n    web = None\ntry:\n    from fuzzywuzzy import fuzz, process as fuzz_process\nexcept ImportError:\n    fuzz = None\n    fuzz_process = None\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', '{:.4f}'.format)\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# ─── API & Environment ───\nQUIVER_API_KEY = os.environ.get('QUIVER_API_KEY', '')\nQUIVER_BASE_URL = 'https://api.quiverquant.com/beta'\n\n# ─── Sample Period ───\nSAMPLE_START          = '2016-01-01'\nSAMPLE_END            = '2025-12-31'\nESTIMATION_WINDOW_START = '2015-01-01'\n\n# ─── Event Study ───\nESTIMATION_WINDOW      = (-250, -30)\nEVENT_WINDOW_IMMEDIATE = (-1, 1)\nEVENT_WINDOW_DRIFT     = (2, 20)\nMIN_ESTIMATION_DAYS    = 120\n\n# ─── Signal Engine ───\nLOOKBACK_DAYS           = 45\nCONVICTION_THRESHOLD    = 0.80\nBACKTEST_LOOKBACK_DAYS  = 90\n\n# ─── Backtest ───\nREBALANCE_FREQUENCY   = 'ME'\nTRANSACTION_COST_BPS  = 10\n\n# ─── Amount Midpoints — STOCK Act range mapping ───\n# ASSUMPTION A1: Use midpoint of each STOCK Act range as dollar estimate.\n# Raw Amount is a lower bound; midpoints reduce systematic downward bias.\nAMOUNT_MIDPOINTS = {\n    1001:     8000,\n    15001:    32500,\n    50001:    75000,\n    100001:   175000,\n    250001:   375000,\n    500001:   750000,\n    1000001:  3000000,\n    5000001:  15000000,\n    25000001: 37500000,\n    50000001: 50000001,\n}\n\ndef get_amount_midpoint(amount):\n    \"\"\"Map STOCK Act lower bound to midpoint estimate (ASSUMPTION A1).\"\"\"\n    for lower in sorted(AMOUNT_MIDPOINTS.keys(), reverse=True):\n        if amount >= lower:\n            return AMOUNT_MIDPOINTS[lower]\n    return amount\n\n# ─── Sector Configuration ───\n# ASSUMPTION A2: Restrict to 7 sectors with high government intervention.\n# This is a design choice — document edge cases in assumptions.md.\nSECTOR_CONFIG = {\n    'Defense & Aerospace': {\n        'target_etf': 'XAR',\n        'committees': [\n            'House Armed Services', 'Senate Armed Services',\n            'House Appropriations', 'Senate Appropriations',\n            'House Foreign Affairs', 'Senate Foreign Relations',\n        ],\n        'gics_sector': 'Industrials',\n        'gics_industry_keywords': ['Aerospace', 'Defense'],\n        'tau_member': 2, 'tau_ticker': 3, 'min_transactions': 8,\n    },\n    'Healthcare & Pharmaceuticals': {\n        'target_etf': 'XHS',\n        'committees': [\n            'House Energy and Commerce', 'Senate HELP',\n            'House Ways and Means', 'Senate Finance',\n        ],\n        'gics_sector': 'Health Care',\n        'gics_industry_keywords': ['Health Care', 'Pharmaceuticals', 'Biotechnology', 'Medical'],\n        'tau_member': 2, 'tau_ticker': 3, 'min_transactions': 8,\n    },\n    'Technology': {\n        'target_etf': 'XLK',\n        'committees': [\n            'House Science, Space, and Technology', 'Senate Commerce',\n            'House Judiciary', 'Senate Judiciary',\n            'House Energy and Commerce',\n        ],\n        'gics_sector': 'Information Technology',\n        'gics_industry_keywords': ['Software', 'Technology', 'Semiconductor', 'Hardware', 'IT Services'],\n        'tau_member': 2, 'tau_ticker': 3, 'min_transactions': 8,\n    },\n    'Financial Services': {\n        'target_etf': 'XLF',\n        'committees': [\n            'House Financial Services', 'Senate Banking',\n            'House Ways and Means', 'Senate Finance',\n        ],\n        'gics_sector': 'Financials',\n        'gics_industry_keywords': ['Banks', 'Insurance', 'Capital Markets', 'Financial'],\n        'tau_member': 2, 'tau_ticker': 3, 'min_transactions': 8,\n    },\n    'Energy': {\n        'target_etf': 'XLE',\n        'committees': [\n            'House Energy and Commerce', 'Senate Energy and Natural Resources',\n            'House Natural Resources', 'Senate Environment and Public Works',\n        ],\n        'gics_sector': 'Energy',\n        'gics_industry_keywords': ['Oil', 'Gas', 'Energy'],\n        'tau_member': 2, 'tau_ticker': 3, 'min_transactions': 8,\n    },\n    'Utilities': {\n        'target_etf': 'XLU',\n        'committees': [\n            'House Energy and Commerce', 'Senate Energy and Natural Resources',\n            'House Natural Resources', 'Senate Environment and Public Works',\n        ],\n        'gics_sector': 'Utilities',\n        'gics_industry_keywords': ['Electric', 'Gas', 'Water', 'Utilities'],\n        'tau_member': 2, 'tau_ticker': 3, 'min_transactions': 5,\n    },\n    'Industrials': {\n        'target_etf': 'XLI',\n        'committees': [\n            'House Transportation and Infrastructure',\n            'Senate Commerce, Science, and Transportation',\n            'House Armed Services', 'Senate Armed Services',\n        ],\n        'gics_sector': 'Industrials',\n        'gics_industry_keywords': ['Industrial', 'Manufacturing', 'Transportation', 'Construction'],\n        'tau_member': 2, 'tau_ticker': 3, 'min_transactions': 8,\n    },\n}\n\nETF_TO_SECTOR = {v['target_etf']: k for k, v in SECTOR_CONFIG.items()}\n\n# ─── Paths ───\nDATA_DIR      = Path('data')\nRAW_DIR       = DATA_DIR / 'raw'\nPROCESSED_DIR = DATA_DIR / 'processed'\nEXTERNAL_DIR  = DATA_DIR / 'external'\nLOGS_DIR      = Path('logs')\n\nfor d in [RAW_DIR, PROCESSED_DIR, EXTERNAL_DIR, LOGS_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\nprint('Configuration loaded.')\nprint(f'Sample period: {SAMPLE_START} to {SAMPLE_END}')\nprint(f'Sectors tracked: {list(SECTOR_CONFIG.keys())}')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-02",
   "metadata": {},
   "source": "---\n## Phase 1: Data Pipeline\n### Modules: Quiver Client → Sector Mapper → Committee Mapper → Returns → Enrichment\n\n**Definition of Done:**\n- All data sources fetched, validated, and persisted to `data/processed/`\n- Master enriched dataset joins trades → sectors → committees\n- Schema validation passes with zero nulls in critical columns"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-03",
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════════════════\n# MODULE 1: Quiver Quant API Client\n# ═══════════════════════════════════════════════════════════════════════════\n\ndef fetch_live_trades(api_key: str) -> pd.DataFrame:\n    \"\"\"\n    Fetch congressional trades from Quiver Quant API.\n\n    API SCHEMA: Quiver V2 returns 'Filed' and 'Traded', NOT 'ReportDate'/'TransactionDate'.\n    These are renamed immediately on fetch. If neither 'Filed' nor 'ReportDate' is\n    present, a ValueError is raised with the actual column names found.\n    \"\"\"\n    headers = {'accept': 'application/json', 'X-CSRFToken': api_key}\n    url = f'{QUIVER_BASE_URL}/live/congresstrading'\n    try:\n        resp = requests.get(url, headers=headers, timeout=30)\n        resp.raise_for_status()\n        df = pd.DataFrame(resp.json())\n    except Exception as e:\n        print(f'[fetch_live_trades] API request failed: {e}')\n        return pd.DataFrame()\n\n    # Normalize column names immediately on fetch\n    if 'Filed' in df.columns:\n        df = df.rename(columns={'Filed': 'ReportDate', 'Traded': 'TransactionDate'})\n    elif 'ReportDate' not in df.columns:\n        raise ValueError(\n            f'Expected Filed or ReportDate column. Found: {list(df.columns)}'\n        )\n    return df\n\n\ndef load_historical(parquet_path) -> pd.DataFrame:\n    \"\"\"Load historical trades from parquet; normalize column names.\"\"\"\n    path = Path(parquet_path)\n    if not path.exists():\n        return pd.DataFrame()\n    df = pd.read_parquet(path)\n    if 'Filed' in df.columns:\n        df = df.rename(columns={'Filed': 'ReportDate', 'Traded': 'TransactionDate'})\n    return df\n\n\ndef sync_and_deduplicate(df_hist: pd.DataFrame, df_live: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Merge historical + live data, then deduplicate.\n\n    Dedup strategy (per CLAUDE.md priority order):\n      1. Use native transaction ID field if present ('id', 'transaction_id')\n      2. Composite key: (Name, Ticker, TransactionDate, Transaction, Amount)\n         — includes Amount to preserve multiple same-day trades of different sizes\n         (bug fix: prior code omitted Amount from dedup key)\n    \"\"\"\n    if df_hist.empty:\n        df = df_live.copy()\n    elif df_live.empty:\n        df = df_hist.copy()\n    else:\n        df = pd.concat([df_hist, df_live], ignore_index=True)\n\n    for col in ['ReportDate', 'TransactionDate']:\n        if col in df.columns:\n            df[col] = pd.to_datetime(df[col], errors='coerce')\n\n    # Check for native transaction ID\n    id_col = next((c for c in ['transaction_id', 'id'] if c in df.columns), None)\n    if id_col:\n        df = df.drop_duplicates(subset=[id_col])\n    else:\n        dedup_cols = [c for c in ['Name', 'Ticker', 'TransactionDate', 'Transaction', 'Amount']\n                      if c in df.columns]\n        df = df.drop_duplicates(subset=dedup_cols)\n\n    return df.reset_index(drop=True)\n\n\ndef validate_and_enrich_raw(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Validate raw trades; add Direction, AmountMidpoint, DisclosureLag.\"\"\"\n    df = df.copy()\n\n    # Drop non-Buy/Sell transaction types\n    valid_txn = {'Purchase', 'Sale'}\n    df = df[df['Transaction'].isin(valid_txn)].copy()\n\n    # Direction: +1 for Purchase, -1 for Sale\n    df['Direction'] = df['Transaction'].map({'Purchase': 1, 'Sale': -1})\n\n    # Amount midpoint (ASSUMPTION A1)\n    if 'Amount' in df.columns:\n        df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce').fillna(0)\n        df['AmountMidpoint'] = df['Amount'].apply(get_amount_midpoint)\n\n    # Disclosure lag in calendar days\n    if 'TransactionDate' in df.columns:\n        df['DisclosureLag'] = (df['ReportDate'] - df['TransactionDate']).dt.days\n    else:\n        df['DisclosureLag'] = np.nan\n\n    # Drop rows missing critical columns\n    critical = [c for c in ['ReportDate', 'Ticker', 'Name', 'Transaction'] if c in df.columns]\n    df = df.dropna(subset=critical)\n\n    # Filter to sample period\n    df = df[\n        (df['ReportDate'] >= SAMPLE_START) &\n        (df['ReportDate'] <= SAMPLE_END)\n    ].copy()\n\n    return df.reset_index(drop=True)\n\n\n# ─── Load Data ───\nprint('Loading congressional trading data...')\ndf_hist = load_historical(RAW_DIR / 'congress_trading.parquet')\n\nif not df_hist.empty:\n    if QUIVER_API_KEY:\n        df_live = fetch_live_trades(QUIVER_API_KEY)\n        df_raw = sync_and_deduplicate(df_hist, df_live)\n    else:\n        df_raw = df_hist.copy()\n        if 'Filed' in df_raw.columns:\n            df_raw = df_raw.rename(columns={'Filed': 'ReportDate', 'Traded': 'TransactionDate'})\n        for col in ['ReportDate', 'TransactionDate']:\n            if col in df_raw.columns:\n                df_raw[col] = pd.to_datetime(df_raw[col], errors='coerce')\nelse:\n    raise FileNotFoundError(\n        'No data found. Provide data/raw/congress_trading.parquet or set QUIVER_API_KEY.'\n    )\n\ndf_raw = validate_and_enrich_raw(df_raw)\nprint(f'Raw trades loaded and validated: {len(df_raw):,}')\nprint(f'Date range: {df_raw[\"ReportDate\"].min().date()} to {df_raw[\"ReportDate\"].max().date()}')\ndf_raw.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-04",
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════════════════\n# MODULE 2: Sector Mapper — Ticker → GICS → Project Sector\n# ═══════════════════════════════════════════════════════════════════════════\n\nGICS_CACHE_PATH = RAW_DIR / 'yfinance_gics_cache.json'\n\ndef _load_gics_cache() -> dict:\n    if GICS_CACHE_PATH.exists():\n        with open(GICS_CACHE_PATH) as f:\n            return json.load(f)\n    return {}\n\ndef _save_gics_cache(cache: dict):\n    with open(GICS_CACHE_PATH, 'w') as f:\n        json.dump(cache, f)\n\n\ndef get_gics_for_tickers(tickers: list) -> pd.DataFrame:\n    \"\"\"\n    Fetch GICS sector/industry for each ticker via yfinance.\n    Results are cached to data/raw/yfinance_gics_cache.json.\n    ASSUMPTION A3: Uses current GICS classification (not point-in-time).\n    \"\"\"\n    cache = _load_gics_cache()\n    uncached = [t for t in tickers if t not in cache]\n\n    if uncached:\n        print(f'Fetching GICS classification for {len(uncached)} uncached tickers...')\n        for ticker in tqdm(uncached, desc='yfinance GICS'):\n            try:\n                info = yf.Ticker(ticker).info\n                cache[ticker] = {\n                    'gics_sector':       info.get('sector', 'Unknown'),\n                    'gics_industry':     info.get('industry', 'Unknown'),\n                    'gics_sub_industry': info.get('industryDisp', info.get('industry', 'Unknown')),\n                    'fetched_date':      datetime.now().isoformat()[:10],\n                }\n            except Exception:\n                cache[ticker] = {\n                    'gics_sector': 'Unknown', 'gics_industry': 'Unknown',\n                    'gics_sub_industry': 'Unknown', 'fetched_date': 'error',\n                }\n        _save_gics_cache(cache)\n\n    rows = []\n    for ticker in tickers:\n        r = cache.get(ticker, {})\n        rows.append({\n            'Ticker':           ticker,\n            'gics_sector':      r.get('gics_sector', 'Unknown'),\n            'gics_industry':    r.get('gics_industry', 'Unknown'),\n            'gics_sub_industry':r.get('gics_sub_industry', 'Unknown'),\n        })\n    return pd.DataFrame(rows)\n\n\ndef map_to_project_sector(gics_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Map GICS industry to one of the 7 project sectors using keyword matching.\n    Tickers matching no sector are labeled 'Other' and excluded from signal engine.\n    \"\"\"\n    df = gics_df.copy()\n    df['project_sector'] = 'Other'\n\n    for sector, cfg in SECTOR_CONFIG.items():\n        keywords = cfg['gics_industry_keywords']\n        kw_match = df['gics_industry'].apply(\n            lambda x: any(kw.lower() in str(x).lower() for kw in keywords)\n        )\n        # Only assign if not yet mapped\n        df.loc[kw_match & (df['project_sector'] == 'Other'), 'project_sector'] = sector\n\n    return df\n\n\n# ─── Run Sector Mapping ───\nprint('Mapping tickers to GICS project sectors...')\nunique_tickers = df_raw['Ticker'].dropna().unique().tolist()\ngics_df       = get_gics_for_tickers(unique_tickers)\nticker_sector_map = map_to_project_sector(gics_df)\nticker_sector_map.to_parquet(PROCESSED_DIR / 'ticker_sector_map.parquet', index=False)\n\nprint('\\nProject sector distribution (top tickers):')\nprint(ticker_sector_map['project_sector'].value_counts())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-05",
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════════════════\n# MODULE 3: Committee Mapper — Legislator → Committee Assignment\n# ═══════════════════════════════════════════════════════════════════════════\n\n# ASSUMPTION A4: Committee assignments matched at Congress session level,\n# not exact date. Mid-session changes are rare; document in assumptions.md.\nCONGRESS_SESSIONS = {\n    114: ('2015-01-03', '2017-01-03'),\n    115: ('2017-01-03', '2019-01-03'),\n    116: ('2019-01-03', '2021-01-03'),\n    117: ('2021-01-03', '2023-01-03'),\n    118: ('2023-01-03', '2025-01-03'),\n    119: ('2025-01-03', '2027-01-03'),\n}\n\n# Official committee name → config short name mapping\nCOMMITTEE_NAME_MAP = {\n    'House Committee on Armed Services':           'House Armed Services',\n    'Senate Committee on Armed Services':          'Senate Armed Services',\n    'House Committee on Appropriations':           'House Appropriations',\n    'Senate Committee on Appropriations':          'Senate Appropriations',\n    'House Committee on Foreign Affairs':          'House Foreign Affairs',\n    'Senate Committee on Foreign Relations':       'Senate Foreign Relations',\n    'House Committee on Energy and Commerce':      'House Energy and Commerce',\n    'Senate Committee on Health, Education, Labor, and Pensions': 'Senate HELP',\n    'House Committee on Ways and Means':           'House Ways and Means',\n    'Senate Committee on Finance':                 'Senate Finance',\n    'House Committee on Science, Space, and Technology': 'House Science, Space, and Technology',\n    'Senate Committee on Commerce, Science, and Transportation': 'Senate Commerce',\n    'House Committee on the Judiciary':            'House Judiciary',\n    'Senate Committee on the Judiciary':           'Senate Judiciary',\n    'House Committee on Financial Services':       'House Financial Services',\n    'Senate Committee on Banking, Housing, and Urban Affairs': 'Senate Banking',\n    'Senate Committee on Energy and Natural Resources': 'Senate Energy and Natural Resources',\n    'House Committee on Natural Resources':        'House Natural Resources',\n    'Senate Committee on Environment and Public Works': 'Senate Environment and Public Works',\n    'House Committee on Transportation and Infrastructure': 'House Transportation and Infrastructure',\n}\n\n\ndef get_congress_session(trade_date) -> int:\n    \"\"\"Return Congress session number for a given date.\"\"\"\n    ts = pd.Timestamp(trade_date)\n    for session, (start, end) in CONGRESS_SESSIONS.items():\n        if pd.Timestamp(start) <= ts < pd.Timestamp(end):\n            return session\n    return 119  # default to current\n\n\ndef fetch_committee_rosters_from_api(congress_session: int) -> pd.DataFrame:\n    \"\"\"\n    Attempt to fetch committee rosters from congress.gov API.\n    Requires CONGRESS_API_KEY environment variable.\n    Returns empty DataFrame if unavailable.\n    \"\"\"\n    api_key = os.environ.get('CONGRESS_API_KEY', '')\n    if not api_key:\n        return pd.DataFrame()\n\n    url = (f'https://api.congress.gov/v3/committee?congress={congress_session}'\n           f'&api_key={api_key}&format=json&limit=250')\n    results = []\n    try:\n        resp = requests.get(url, timeout=30)\n        if resp.status_code == 200:\n            for cmte in resp.json().get('committees', []):\n                cmte_name = COMMITTEE_NAME_MAP.get(cmte.get('name', ''), cmte.get('name', ''))\n                for member in cmte.get('members', []):\n                    results.append({\n                        'member_name':      member.get('name', ''),\n                        'committee_name':   cmte_name,\n                        'congress_session': congress_session,\n                        'chamber':          cmte.get('chamber', ''),\n                    })\n    except Exception as e:\n        print(f'  Congress API error for session {congress_session}: {e}')\n\n    return pd.DataFrame(results) if results else pd.DataFrame()\n\n\ndef assign_committees_to_trades(trades_df: pd.DataFrame,\n                                roster_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Tag each trade with Is_Committee_Relevant.\n\n    If roster is available: fuzzy-match legislator names (threshold >= 85),\n    look up committee assignments at time of trade, check against SECTOR_CONFIG.\n    If roster is empty: conservative fallback — all False (logged as warning).\n    \"\"\"\n    df = trades_df.copy()\n    if 'TransactionDate' in df.columns:\n        df['Congress_Session'] = df['TransactionDate'].apply(get_congress_session)\n    else:\n        df['Congress_Session'] = 119\n\n    if roster_df.empty or 'member_name' not in roster_df.columns:\n        df['Committee_List']        = [[] for _ in range(len(df))]\n        df['Is_Committee_Relevant'] = False\n        print('WARNING: No committee roster available. All trades marked non-committee-relevant.')\n        print('         Set CONGRESS_API_KEY to enable H2 analysis.')\n        return df\n\n    # Build name lookup via fuzzy matching\n    roster_names = roster_df['member_name'].str.lower().str.strip().unique().tolist()\n    name_cache = {}\n    for name in df['Name'].str.lower().str.strip().unique():\n        if fuzz_process:\n            match, score = fuzz_process.extractOne(name, roster_names,\n                                                   scorer=fuzz.token_sort_ratio)\n            name_cache[name] = match if score >= 85 else None\n        else:\n            name_cache[name] = None\n\n    def get_committees(row):\n        matched = name_cache.get(str(row['Name']).lower().strip())\n        if matched is None:\n            return []\n        mask = (\n            (roster_df['member_name'].str.lower().str.strip() == matched) &\n            (roster_df['congress_session'] == row['Congress_Session'])\n        )\n        return roster_df.loc[mask, 'committee_name'].tolist()\n\n    def is_relevant(row):\n        sector = row.get('project_sector', 'Other')\n        if sector == 'Other' or sector not in SECTOR_CONFIG:\n            return False\n        sector_committees = set(SECTOR_CONFIG[sector]['committees'])\n        return bool(sector_committees & set(row.get('Committee_List', [])))\n\n    df['Committee_List']        = df.apply(get_committees, axis=1)\n    df['Is_Committee_Relevant'] = df.apply(is_relevant, axis=1)\n    return df\n\n\n# ─── Run Committee Mapping ───\nprint('Fetching committee rosters...')\nroster_frames = []\nfor session in CONGRESS_SESSIONS:\n    r = fetch_committee_rosters_from_api(session)\n    if not r.empty:\n        roster_frames.append(r)\n\nif roster_frames:\n    committee_roster = pd.concat(roster_frames, ignore_index=True)\n    committee_roster.to_parquet(PROCESSED_DIR / 'committee_roster.parquet', index=False)\n    print(f'Committee roster loaded: {len(committee_roster):,} assignments')\nelse:\n    committee_roster = pd.DataFrame()\n    print('No committee roster from API — using fallback (Is_Committee_Relevant = False).')\n    print('Set CONGRESS_API_KEY env variable to enable full committee analysis.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-06",
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════════════════\n# MODULES 4-6: Stock Returns, ETF Returns, Fama-French Factors\n# ═══════════════════════════════════════════════════════════════════════════\n\n# ─── Module 4: Stock Returns ───\ndef fetch_stock_returns(tickers: list, start: str, end: str) -> pd.DataFrame:\n    \"\"\"\n    Fetch daily adjusted-close returns for event study.\n    Returns computed as (adj_close_t / adj_close_{t-1}) - 1.\n    Caches to data/raw/stock_returns_cache.parquet.\n    \"\"\"\n    cache_path = RAW_DIR / 'stock_returns_cache.parquet'\n\n    if cache_path.exists():\n        cached = pd.read_parquet(cache_path)\n        cached_tickers = set(cached['Ticker'].unique())\n        need_tickers = [t for t in tickers if t not in cached_tickers]\n    else:\n        cached = pd.DataFrame()\n        need_tickers = tickers\n\n    if need_tickers:\n        print(f'Fetching returns for {len(need_tickers)} new tickers...')\n        frames = []\n        batch_size = 50\n        for i in tqdm(range(0, len(need_tickers), batch_size), desc='Stock returns'):\n            batch = need_tickers[i:i + batch_size]\n            try:\n                prices = yf.download(batch, start=start, end=end,\n                                     auto_adjust=True, progress=False)['Close']\n                if isinstance(prices, pd.Series):\n                    prices = prices.to_frame(batch[0])\n                ret = prices.pct_change().dropna(how='all')\n                df_long = (\n                    ret.reset_index()\n                       .melt(id_vars='Date', var_name='Ticker', value_name='ret')\n                       .rename(columns={'Date': 'date'})\n                       .dropna(subset=['ret'])\n                )\n                frames.append(df_long)\n            except Exception as e:\n                print(f'  Batch {i}: {e}')\n\n        if frames:\n            new_data = pd.concat(frames, ignore_index=True)\n            new_data['date'] = pd.to_datetime(new_data['date'])\n            stock_returns = (\n                pd.concat([cached, new_data], ignore_index=True)\n                  .drop_duplicates(subset=['date', 'Ticker'])\n            )\n            stock_returns.to_parquet(cache_path, index=False)\n        else:\n            stock_returns = cached\n    else:\n        stock_returns = cached\n        print(f'All {len(tickers)} tickers found in cache.')\n\n    stock_returns['date'] = pd.to_datetime(stock_returns['date'])\n    return stock_returns\n\n\n# ─── Module 5: ETF Returns ───\ndef fetch_etf_returns(etf_tickers: list, start: str, end: str) -> pd.DataFrame:\n    \"\"\"Fetch daily returns for sector ETFs + SPY benchmark.\"\"\"\n    all_etfs = list(set(etf_tickers + ['SPY']))\n    print(f'Fetching ETF returns for: {all_etfs}')\n    try:\n        prices = yf.download(all_etfs, start=start, end=end,\n                             auto_adjust=True, progress=False)['Close']\n        if isinstance(prices, pd.Series):\n            prices = prices.to_frame(all_etfs[0])\n        ret = prices.pct_change().dropna(how='all')\n        df_long = (\n            ret.reset_index()\n               .melt(id_vars='Date', var_name='etf_ticker', value_name='ret')\n               .rename(columns={'Date': 'date'})\n        )\n        df_long['date'] = pd.to_datetime(df_long['date'])\n        df_long.to_parquet(EXTERNAL_DIR / 'etf_returns.parquet', index=False)\n        print(f'ETF returns: {len(df_long):,} daily observations')\n        return df_long\n    except Exception as e:\n        print(f'ETF fetch failed: {e}')\n        return pd.DataFrame(columns=['date', 'etf_ticker', 'ret'])\n\n\n# ─── Module 6: Fama-French Factors ───\ndef fetch_ff5_mom(start: str, end: str) -> pd.DataFrame:\n    \"\"\"\n    Download FF5 + Momentum daily factors from Kenneth French Data Library.\n    Values are in % — divided by 100 for decimal returns.\n    COLUMN NAMES standardized: Mkt-RF → Mkt_RF, Mom column normalized.\n    \"\"\"\n    ff_path = EXTERNAL_DIR / 'ff5_mom_factors.csv'\n    if ff_path.exists():\n        df = pd.read_csv(ff_path, index_col='date', parse_dates=True)\n        print(f'FF5+Mom factors loaded from cache: {len(df):,} rows')\n        return df\n\n    if web is None:\n        print('pandas_datareader not installed — skipping factor download.')\n        return pd.DataFrame()\n\n    print('Downloading Fama-French 5 + Momentum factors (daily)...')\n    try:\n        ff5 = web.DataReader('F-F_Research_Data_5_Factors_2x3_daily',\n                             'famafrench', start=start, end=end)[0]\n        mom = web.DataReader('F-F_Momentum_Factor_daily',\n                             'famafrench', start=start, end=end)[0]\n        ff5.index = pd.to_datetime(ff5.index, format='%Y%m%d')\n        mom.index = pd.to_datetime(mom.index, format='%Y%m%d')\n        df = ff5.join(mom, how='inner') / 100.0\n        df.columns = [c.strip().replace('-', '_').replace(' ', '') for c in df.columns]\n        # Normalize momentum column name\n        mom_col = next((c for c in df.columns if 'mom' in c.lower() or c == 'WML'), None)\n        if mom_col and mom_col != 'Mom':\n            df = df.rename(columns={mom_col: 'Mom'})\n        df.index.name = 'date'\n        df.to_csv(ff_path)\n        print(f'FF5+Mom factors: {len(df):,} daily observations')\n        return df\n    except Exception as e:\n        print(f'Daily factors failed ({e}), trying monthly...')\n        try:\n            ff5 = web.DataReader('F-F_Research_Data_5_Factors_2x3',\n                                 'famafrench', start=start, end=end)[0]\n            mom = web.DataReader('F-F_Momentum_Factor',\n                                 'famafrench', start=start, end=end)[0]\n            ff5.index = pd.to_datetime(ff5.index.to_timestamp())\n            mom.index = pd.to_datetime(mom.index.to_timestamp())\n            df = ff5.join(mom, how='inner') / 100.0\n            df.columns = [c.strip().replace('-', '_').replace(' ', '') for c in df.columns]\n            mom_col = next((c for c in df.columns if 'mom' in c.lower() or c == 'WML'), None)\n            if mom_col and mom_col != 'Mom':\n                df = df.rename(columns={mom_col: 'Mom'})\n            df.index.name = 'date'\n            df.to_csv(ff_path)\n            print(f'FF5+Mom monthly factors: {len(df):,} rows')\n            return df\n        except Exception as e2:\n            print(f'Factor download unavailable: {e2}')\n            return pd.DataFrame()\n\n\n# ─── Fetch all return data ───\netf_tickers   = [cfg['target_etf'] for cfg in SECTOR_CONFIG.values()]\netf_returns   = fetch_etf_returns(etf_tickers, ESTIMATION_WINDOW_START, SAMPLE_END)\nff_factors    = fetch_ff5_mom(ESTIMATION_WINDOW_START, SAMPLE_END)\n\n# Market returns from SPY\nspy_daily = (\n    etf_returns[etf_returns['etf_ticker'] == 'SPY']\n    .set_index('date')['ret']\n    .rename('mkt_ret')\n)\n\nprint(f'\\nData fetch complete:')\nprint(f'  ETF returns: {len(etf_returns):,} rows')\nprint(f'  Market (SPY) returns: {len(spy_daily):,} days')\nprint(f'  FF5+Mom factors: {len(ff_factors):,} rows')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-07",
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════════════════\n# MODULE 7: Enrichment Pipeline — Master Join\n# ═══════════════════════════════════════════════════════════════════════════\n\ndef run_enrichment_pipeline(df_raw: pd.DataFrame,\n                            ticker_sector_map: pd.DataFrame,\n                            committee_roster: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Master join: trades → GICS sectors → project sectors → committee flags → ETF mapping.\n    Validates final schema and logs counts to console.\n    Saves to data/processed/trades_enriched.parquet.\n    \"\"\"\n    df = df_raw.copy()\n    n_raw = len(df)\n\n    # Step 1: Join sector mapping\n    df = df.merge(\n        ticker_sector_map[['Ticker', 'gics_sector', 'gics_industry', 'project_sector']],\n        on='Ticker', how='left'\n    )\n    df['project_sector'] = df['project_sector'].fillna('Other')\n    n_sector_matched = (df['project_sector'] != 'Other').sum()\n\n    # Step 2: Add target_etf\n    sector_to_etf = {k: v['target_etf'] for k, v in SECTOR_CONFIG.items()}\n    df['target_etf'] = df['project_sector'].map(sector_to_etf)\n\n    # Step 3: Assign committee memberships\n    df = assign_committees_to_trades(df, committee_roster)\n    n_committee_relevant = df['Is_Committee_Relevant'].sum()\n\n    # Step 4: Ensure all required columns exist\n    required = [\n        'ReportDate', 'TransactionDate', 'Ticker', 'Name',\n        'Transaction', 'Direction', 'Amount', 'AmountMidpoint',\n        'DisclosureLag', 'Chamber', 'gics_sector', 'gics_industry',\n        'project_sector', 'Committee_List', 'Is_Committee_Relevant', 'target_etf',\n    ]\n    for col in required:\n        if col not in df.columns:\n            df[col] = np.nan if col not in ['Committee_List'] else [[] for _ in range(len(df))]\n\n    df.to_parquet(PROCESSED_DIR / 'trades_enriched.parquet', index=False)\n\n    # Validation report\n    print('\\n' + '=' * 60)\n    print('ENRICHMENT VALIDATION REPORT')\n    print('=' * 60)\n    print(f'  Total raw trades:              {n_raw:>10,}')\n    print(f'  Matched to project sector:     {n_sector_matched:>10,} ({n_sector_matched/n_raw*100:.1f}%)')\n    print(f'  \"Other\" sector (excluded):     {(df[\"project_sector\"]==\"Other\").sum():>10,}')\n    print(f'  Committee-relevant trades:     {n_committee_relevant:>10,} ({n_committee_relevant/n_raw*100:.1f}%)')\n    print(f'  Final enriched dataset rows:   {len(df):>10,}')\n    print('=' * 60)\n\n    return df\n\n\ndf_enriched = run_enrichment_pipeline(df_raw, ticker_sector_map, committee_roster)\ndf_enriched.head()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-08",
   "metadata": {},
   "source": "---\n### Data Exploration & Quality Assessment"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-09",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\nfig.suptitle('Congressional Trading Data — Exploratory Analysis', fontsize=16, fontweight='bold')\n\n# 1. Monthly trade disclosures over time\nax = axes[0, 0]\nmonthly_trades = df_enriched.set_index('ReportDate').resample('ME').size()\nax.plot(monthly_trades.index, monthly_trades.values, linewidth=1.5, color='steelblue')\nax.set_title('Monthly Trade Disclosures Over Time')\nax.set_xlabel('Report Date')\nax.set_ylabel('Number of Trades')\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n\n# 2. Purchase vs. Sale\nax = axes[0, 1]\ndir_counts = df_enriched['Transaction'].value_counts()\nbars = ax.bar(dir_counts.index, dir_counts.values,\n              color=['steelblue', 'coral'], edgecolor='white')\nax.set_title('Purchase vs. Sale Distribution')\nax.set_ylabel('Count')\nfor bar, val in zip(bars, dir_counts.values):\n    ax.text(bar.get_x() + bar.get_width() / 2., bar.get_height() * 1.02,\n            f'{val:,}', ha='center', fontsize=10)\n\n# 3. Project sector breakdown\nax = axes[0, 2]\nsector_counts = df_enriched['project_sector'].value_counts()\ncolors = plt.cm.Set3(np.linspace(0, 1, len(sector_counts)))\nax.barh(sector_counts.index, sector_counts.values, color=colors)\nax.set_title('Trades by Project Sector')\nax.set_xlabel('Count')\n\n# 4. Disclosure lag\nax = axes[1, 0]\nlags = df_enriched['DisclosureLag'].dropna().clip(0, 120)\nax.hist(lags, bins=60, color='steelblue', alpha=0.7, edgecolor='white')\nax.axvline(45, color='red', linestyle='--', label='45-day STOCK Act deadline')\nax.set_title('Disclosure Lag Distribution')\nax.set_xlabel('Calendar Days (ReportDate − TransactionDate)')\nax.set_ylabel('Frequency')\nax.legend()\n\n# 5. Amount distribution\nax = axes[1, 1]\namounts = df_enriched['AmountMidpoint'].dropna()\namounts_pos = amounts[amounts > 0]\nax.hist(np.log10(amounts_pos), bins=50, color='mediumpurple', alpha=0.7, edgecolor='white')\nax.set_title('Trade Amount Distribution (log₁₀ scale)')\nax.set_xlabel('log₁₀(AmountMidpoint USD)')\nax.set_ylabel('Frequency')\n\n# 6. Committee relevance pie\nax = axes[1, 2]\ncomm_counts = df_enriched['Is_Committee_Relevant'].value_counts()\nlabels = ['Non-Relevant', 'Committee-Relevant']\nax.pie(comm_counts.values, labels=labels, colors=['lightcoral', 'steelblue'],\n       autopct='%1.1f%%', startangle=90)\nax.set_title('Committee Relevance of Trades')\n\nplt.tight_layout()\nplt.show()\n\nprint('\\n' + '=' * 60)\nprint('DATA QUALITY SUMMARY')\nprint('=' * 60)\nprint(f'Unique legislators: {df_enriched[\"Name\"].nunique():,}')\nprint(f'Unique tickers:     {df_enriched[\"Ticker\"].nunique():,}')\nprint(f'Date range:         {df_enriched[\"ReportDate\"].min().date()} → {df_enriched[\"ReportDate\"].max().date()}')\nif 'DisclosureLag' in df_enriched.columns:\n    print(f'Median lag (days):  {df_enriched[\"DisclosureLag\"].median():.0f}')\n    print(f'Lag > 45 days:      {(df_enriched[\"DisclosureLag\"] > 45).sum():,} '\n          f'({(df_enriched[\"DisclosureLag\"] > 45).mean()*100:.1f}%)')\nprint(f'Purchases:          {(df_enriched[\"Direction\"] == 1).sum():,}')\nprint(f'Sales:              {(df_enriched[\"Direction\"] == -1).sum():,}')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "---\n## Phase 2: H1 — Market Underreaction Event Study\n\n**Hypothesis:** If `CAR[-1,+1] ≈ 0` but `CAR[+2,+20] > 0` for purchases after disclosure, the market underreacts to congressional trade disclosures.\n\n**Event date = `ReportDate` (disclosure date) — NEVER `TransactionDate`.**\n\n**Definition of Done:**\n- CAR[-1,+1] and CAR[+2,+20] computed for all disclosure events\n- Cross-sectional t-test with buy/sell separation and Patell standardized test\n- Clear accept/reject conclusion for H1\n- Robustness: winsorized, lag ≤ 45 days, subperiod splits"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════════════════\n# MODULE 8: Market Model Estimation\n# ═══════════════════════════════════════════════════════════════════════════\n\n# Fetch stock returns for all unique tickers in the trade dataset\nprint('Fetching stock returns for event study...')\nunique_tickers = df_enriched['Ticker'].dropna().unique().tolist()\nstock_returns_df = fetch_stock_returns(unique_tickers, ESTIMATION_WINDOW_START, SAMPLE_END)\n\n# Build wide returns matrix (date × ticker) for fast indexing\nstock_rets = stock_returns_df.pivot(index='date', columns='Ticker', values='ret')\nstock_rets.index = pd.to_datetime(stock_rets.index)\nstock_rets = stock_rets.sort_index()\n\nprint(f'Return matrix: {stock_rets.shape[0]:,} dates × {stock_rets.shape[1]:,} tickers')\n\n\ndef fit_market_model(event_date: pd.Timestamp, ticker: str,\n                    stock_rets_matrix: pd.DataFrame,\n                    mkt_returns: pd.Series,\n                    estimation_window: tuple = (-250, -30),\n                    min_obs: int = 120) -> dict:\n    \"\"\"\n    OLS: stock_ret_t = alpha + beta * mkt_ret_t + epsilon\n    Estimation window in TRADING DAYS relative to event_date.\n    Returns None if insufficient data.\n    \"\"\"\n    if ticker not in stock_rets_matrix.columns:\n        return None\n\n    all_dates = list(stock_rets_matrix.index)\n    dates_before = [d for d in all_dates if d <= event_date]\n    if not dates_before:\n        return None\n\n    event_idx = all_dates.index(dates_before[-1])\n    start_idx = max(0, event_idx + estimation_window[0])\n    end_idx   = max(0, event_idx + estimation_window[1])\n\n    if end_idx <= start_idx:\n        return None\n\n    est_dates  = stock_rets_matrix.index[start_idx:end_idx]\n    stock_r    = stock_rets_matrix.loc[est_dates, ticker].dropna()\n    market_r   = mkt_returns.reindex(est_dates).dropna()\n    common     = stock_r.index.intersection(market_r.index)\n\n    if len(common) < min_obs:\n        return None\n\n    try:\n        y = stock_r.loc[common].values\n        X = sm.add_constant(market_r.loc[common].values)\n        res = sm.OLS(y, X).fit()\n        return {\n            'alpha':     res.params[0],\n            'beta':      res.params[1],\n            'sigma':     res.resid.std(),\n            'n_obs':     len(common),\n            'r_squared': res.rsquared,\n        }\n    except Exception:\n        return None\n\n\n# ─── Aggregate to disclosure-level events ───\n# CRITICAL: Event date = ReportDate (disclosure date)\nevents = (\n    df_enriched\n    .groupby(['ReportDate', 'Ticker', 'Direction'])\n    .agg(TotalAmount=('AmountMidpoint', 'sum'), n_trades=('Direction', 'count'))\n    .reset_index()\n)\nprint(f'\\nTotal disclosure events: {len(events):,}')\nprint(f'  Purchases: {(events[\"Direction\"] == 1).sum():,}')\nprint(f'  Sales:     {(events[\"Direction\"] == -1).sum():,}')\n\n# ─── Fit market models ───\nprint('\\nFitting market models (may take several minutes)...')\nmarket_models = {}\nfailed = 0\n\nfor _, row in tqdm(events.iterrows(), total=len(events), desc='Market Models'):\n    key = (pd.Timestamp(row['ReportDate']), row['Ticker'])\n    result = fit_market_model(\n        event_date=key[0], ticker=key[1],\n        stock_rets_matrix=stock_rets,\n        mkt_returns=spy_daily,\n        estimation_window=ESTIMATION_WINDOW,\n        min_obs=MIN_ESTIMATION_DAYS,\n    )\n    if result is not None:\n        market_models[key] = result\n    else:\n        failed += 1\n\nprint(f'\\nMarket models fitted:   {len(market_models):,}')\nprint(f'Events excluded:        {failed:,} ({failed/len(events)*100:.1f}%)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════════════════\n# MODULE 9: Abnormal Returns & CAR Computation\n# ═══════════════════════════════════════════════════════════════════════════\n\ndef compute_car(event_date: pd.Timestamp, ticker: str,\n                mm: dict,\n                stock_rets_matrix: pd.DataFrame,\n                mkt_returns: pd.Series,\n                window: tuple) -> tuple:\n    \"\"\"\n    Compute CAR[window[0], window[1]] in trading days relative to event_date.\n    AR_t = actual_ret_t − (alpha + beta × mkt_ret_t)\n\n    Returns (car, n_days). If stock is delisted during window, CAR is truncated\n    (NOT forward-filled). Returns (nan, 0) on failure.\n    \"\"\"\n    if ticker not in stock_rets_matrix.columns:\n        return np.nan, 0\n\n    try:\n        all_dates = list(stock_rets_matrix.index)\n        dates_before = [d for d in all_dates if d <= event_date]\n        if not dates_before:\n            return np.nan, 0\n\n        event_idx = all_dates.index(dates_before[-1])\n        start_idx = max(0, event_idx + window[0])\n        end_idx   = min(len(all_dates) - 1, event_idx + window[1])\n\n        if start_idx > end_idx:\n            return np.nan, 0\n\n        win_dates = stock_rets_matrix.index[start_idx:end_idx + 1]\n        stock_r   = stock_rets_matrix.loc[win_dates, ticker].dropna()\n        market_r  = mkt_returns.reindex(win_dates).dropna()\n        common    = stock_r.index.intersection(market_r.index)\n\n        if len(common) == 0:\n            return np.nan, 0\n\n        ar  = stock_r.loc[common] - (mm['alpha'] + mm['beta'] * market_r.loc[common])\n        return ar.sum(), len(common)\n    except Exception:\n        return np.nan, 0\n\n\n# ─── Compute CARs for all fitted events ───\nprint('Computing CARs...')\ncar_rows = []\n\nfor _, row in tqdm(events.iterrows(), total=len(events), desc='CAR Computation'):\n    key = (pd.Timestamp(row['ReportDate']), row['Ticker'])\n    if key not in market_models:\n        continue\n\n    mm = market_models[key]\n    car_imm,   n_imm   = compute_car(key[0], key[1], mm, stock_rets, spy_daily, EVENT_WINDOW_IMMEDIATE)\n    car_drift, n_drift = compute_car(key[0], key[1], mm, stock_rets, spy_daily, EVENT_WINDOW_DRIFT)\n    ar_day0,   _       = compute_car(key[0], key[1], mm, stock_rets, spy_daily, (0, 0))\n\n    car_rows.append({\n        'ReportDate':      row['ReportDate'],\n        'Ticker':          row['Ticker'],\n        'Direction':       row['Direction'],\n        'Transaction':     'Purchase' if row['Direction'] == 1 else 'Sale',\n        'TotalAmount':     row['TotalAmount'],\n        'mm_alpha':        mm['alpha'],\n        'mm_beta':         mm['beta'],\n        'mm_sigma':        mm['sigma'],\n        'mm_n_obs':        mm['n_obs'],\n        'mm_r2':           mm['r_squared'],\n        'CAR_immediate':   car_imm,\n        'CAR_drift':       car_drift,\n        'AR_day0':         ar_day0,\n        'n_days_immediate':n_imm,\n        'n_days_drift':    n_drift,\n    })\n\ndf_cars = pd.DataFrame(car_rows).dropna(subset=['CAR_immediate', 'CAR_drift'])\nprint(f'\\nEvents with valid CARs: {len(df_cars):,}')\nprint('\\nCAR summary statistics:')\nprint(df_cars[['CAR_immediate', 'CAR_drift', 'AR_day0']].describe().round(4))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════════════════\n# MODULE 10: H1 Statistical Tests\n# ═══════════════════════════════════════════════════════════════════════════\n\ndef run_h1_tests(cars_df: pd.DataFrame) -> dict:\n    \"\"\"\n    Cross-sectional t-tests: H0: mean CAR = 0 for each (direction, window) combination.\n    Also computes simplified Patell standardized test.\n    Reports: t-stat, p-value, n, mean_car, pct_positive.\n    \"\"\"\n    results = {}\n    for direction, label in [(1, 'buys'), (-1, 'sells')]:\n        subset = cars_df[cars_df['Direction'] == direction].copy()\n        for window_name, col, n_days_col in [\n            ('immediate', 'CAR_immediate', 'n_days_immediate'),\n            ('drift',     'CAR_drift',     'n_days_drift'),\n        ]:\n            cars = subset[col].dropna()\n            key  = f'{label}_{window_name}'\n            n    = len(cars)\n            if n < 10:\n                results[key] = {'mean_car': np.nan, 't_stat': np.nan,\n                                'p_value': np.nan, 'n': n,\n                                'pct_positive': np.nan, 't_patell': np.nan}\n                continue\n\n            t_stat, p_value = stats.ttest_1samp(cars, 0)\n\n            # Simplified Patell (1976) standardized test\n            sigma = subset.loc[cars.index, 'mm_sigma']\n            n_obs = subset.loc[cars.index, 'mm_n_obs']\n            n_win = subset.loc[cars.index, n_days_col].clip(lower=1)\n            std_cars = cars / (sigma * np.sqrt(n_win) + 1e-12)\n            t_patell = std_cars.mean() * np.sqrt(n) if n > 0 else np.nan\n\n            results[key] = {\n                'mean_car':    cars.mean(),\n                'median_car':  cars.median(),\n                't_stat':      t_stat,\n                'p_value':     p_value,\n                't_patell':    t_patell,\n                'n':           n,\n                'pct_positive':  (cars > 0).mean(),\n            }\n    return results\n\n\nh1_results = run_h1_tests(df_cars)\n\nprint('\\n' + '=' * 80)\nprint('H1 EVENT STUDY — Market Underreaction to Congressional Trade Disclosures')\nprint('=' * 80)\n\nfor key, res in h1_results.items():\n    label, window = key.split('_', 1)\n    win_str = 'CAR[-1,+1]' if window == 'immediate' else 'CAR[+2,+20]'\n    stars = ''\n    if not np.isnan(res.get('p_value', np.nan)):\n        p = res['p_value']\n        stars = '***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.10 else ''\n\n    print(f'\\n{label.upper()} — {win_str}:')\n    print(f'  Mean CAR:    {res[\"mean_car\"]*100 if not np.isnan(res.get(\"mean_car\",np.nan)) else \"N/A\":>8} %{stars}')\n    print(f'  t-statistic: {res.get(\"t_stat\",np.nan):>8.3f}')\n    print(f'  p-value:     {res.get(\"p_value\",np.nan):>8.4f}')\n    print(f'  Patell t:    {res.get(\"t_patell\",np.nan):>8.3f}')\n    print(f'  N events:    {res.get(\"n\",0):>8,}')\n    print(f'  % Positive:  {res.get(\"pct_positive\",np.nan)*100 if not np.isnan(res.get(\"pct_positive\",np.nan)) else \"N/A\":>7.1f} %')\n\n# Interpretation\nprint('\\n' + '=' * 80)\nprint('H1 INTERPRETATION')\nprint('=' * 80)\nbi_p = h1_results.get('buys_immediate', {}).get('p_value', 1)\nbd_p = h1_results.get('buys_drift',     {}).get('p_value', 1)\nbd_m = h1_results.get('buys_drift',     {}).get('mean_car', 0)\n\nif not np.isnan(bd_p):\n    if bd_p < 0.05 and bd_m > 0 and (np.isnan(bi_p) or bi_p > 0.10):\n        print('RESULT: UNDERREACTION CONFIRMED — CAR[-1,+1] ≈ 0, CAR[+2,+20] > 0 (p < 0.05)')\n        print('        Market initially ignores disclosures, then gradually incorporates information.')\n    elif bd_p < 0.05 and bd_m > 0:\n        print('RESULT: DRIFT SIGNIFICANT — but market also reacts at disclosure (both windows active)')\n    elif bd_p > 0.10:\n        print('RESULT: NO EXPLOITABLE SIGNAL — CAR[+2,+20] not statistically significant')\n    else:\n        print('RESULT: NEGATIVE DRIFT — unexpected direction; inspect data quality')\nelse:\n    print('RESULT: Insufficient data for H1 determination.')\n\n# ─── Visualization ───\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\nfig.suptitle('H1 Event Study: CAR Around Disclosure Date (Event = ReportDate)',\n             fontsize=14, fontweight='bold')\n\nfor ax, (direction, label, color) in zip(\n    axes, [(1, 'Purchases', 'steelblue'), (-1, 'Sales', 'coral')]\n):\n    subset = df_cars[df_cars['Direction'] == direction]\n    means  = [subset['CAR_immediate'].mean(), subset['CAR_drift'].mean()]\n    sems   = [stats.sem(subset['CAR_immediate'].dropna()),\n              stats.sem(subset['CAR_drift'].dropna())]\n    x_labels = ['CAR[-1,+1]\\n(Immediate)', 'CAR[+2,+20]\\n(Drift)']\n\n    bars = ax.bar(x_labels, means, yerr=[1.96 * s for s in sems],\n                  color=color, alpha=0.7, capsize=10, edgecolor='white', linewidth=1.5)\n    ax.axhline(0, color='black', linewidth=0.8)\n    ax.set_title(f'{label} (N={len(subset):,})')\n    ax.set_ylabel('Cumulative Abnormal Return')\n    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1%}'))\n    # Annotate bars\n    for bar, val in zip(bars, means):\n        if not np.isnan(val):\n            ax.text(bar.get_x() + bar.get_width()/2., val + 0.001 * np.sign(val),\n                    f'{val:.2%}', ha='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════════════════\n# H1 ROBUSTNESS CHECKS\n# ═══════════════════════════════════════════════════════════════════════════\n\ndef run_robustness(cars_df: pd.DataFrame) -> dict:\n    \"\"\"Robustness: winsorized, lag filter, subperiod splits.\"\"\"\n    p1, p99 = cars_df['CAR_drift'].quantile([0.01, 0.99])\n    cars_wins = cars_df.copy()\n    cars_wins['CAR_drift'] = cars_wins['CAR_drift'].clip(p1, p99)\n\n    # Lag data — merge DisclosureLag if not present\n    if 'DisclosureLag' not in cars_df.columns:\n        lag_data = df_enriched[['ReportDate', 'Ticker', 'DisclosureLag']].drop_duplicates()\n        cars_with_lag = cars_df.merge(lag_data, on=['ReportDate', 'Ticker'], how='left')\n    else:\n        cars_with_lag = cars_df.copy()\n\n    rd = pd.to_datetime(cars_df['ReportDate'])\n    midpoint = rd.median()\n\n    subsets = [\n        ('Full Sample',        cars_df),\n        ('Winsorized 1/99',    cars_wins),\n        ('Lag ≤ 45 days',      cars_with_lag[cars_with_lag.get('DisclosureLag', pd.Series(dtype=float)).le(45)]\n                               if 'DisclosureLag' in cars_with_lag.columns else cars_df),\n        (f'Pre-{midpoint.year}',  cars_df[rd < midpoint]),\n        (f'Post-{midpoint.year}', cars_df[rd >= midpoint]),\n    ]\n\n    results = {}\n    for label, subset in subsets:\n        buys = subset[subset['Direction'] == 1]['CAR_drift'].dropna()\n        if len(buys) < 10:\n            results[label] = {'n': len(buys), 'mean': np.nan, 't': np.nan, 'p': np.nan}\n            continue\n        t, p = stats.ttest_1samp(buys, 0)\n        results[label] = {'n': len(buys), 'mean': buys.mean(), 't': t, 'p': p}\n    return results\n\n\nrobustness = run_robustness(df_cars)\n\nprint('\\n' + '=' * 70)\nprint('H1 ROBUSTNESS — CAR[+2,+20] for Purchases')\nprint('=' * 70)\nprint(f'{\"Specification\":<25} {\"N\":>8} {\"Mean CAR\":>10} {\"t-stat\":>8} {\"p-value\":>12}')\nprint('-' * 70)\n\nfor label, res in robustness.items():\n    if np.isnan(res.get('mean', np.nan)):\n        print(f'{label:<25} {res[\"n\"]:>8,}  {\"N/A\":>10}')\n        continue\n    stars = '***' if res['p'] < 0.01 else '**' if res['p'] < 0.05 else '*' if res['p'] < 0.10 else ''\n    print(f'{label:<25} {res[\"n\"]:>8,} {res[\"mean\"]*100:>9.2f}% {res[\"t\"]:>8.3f} {res[\"p\"]:>8.4f}{stars}')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": "---\n## Phase 3: H2 — Committee Information Advantage\n\n**Hypothesis:** Legislators trading in sectors relevant to their committee assignments generate higher post-disclosure CARs than other trades.\n\n**Two required comparisons:**\n- **Comparison A (Within-member):** Same legislator, committee-relevant vs. non-relevant trades\n- **Comparison B (Cross-member):** Same sector, committee members vs. non-members\n\n**H2 outcome determines committee weighting in signal engine.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════════════════\n# MODULE 11: H2 — Committee Information Advantage\n# ═══════════════════════════════════════════════════════════════════════════\n\n# Join committee relevance flags to CAR dataset\nenrich_cols = ['ReportDate', 'Ticker', 'Is_Committee_Relevant', 'project_sector', 'Name']\ndf_cars_enriched = df_cars.merge(\n    df_enriched[enrich_cols].drop_duplicates(subset=['ReportDate', 'Ticker']),\n    on=['ReportDate', 'Ticker'], how='left'\n)\n\n\ndef run_h2_cross_member(df: pd.DataFrame) -> dict:\n    \"\"\"\n    Comparison B: For each sector, committee members vs. non-members.\n    Test: difference in mean CAR_drift.\n    \"\"\"\n    results = {}\n    for sector in SECTOR_CONFIG:\n        sdf = df[df['project_sector'] == sector]\n        cmte     = sdf[sdf['Is_Committee_Relevant'] == True]['CAR_drift'].dropna()\n        non_cmte = sdf[sdf['Is_Committee_Relevant'] == False]['CAR_drift'].dropna()\n\n        if len(cmte) < 5 or len(non_cmte) < 5:\n            results[sector] = {\n                'n_committee': len(cmte), 'n_non': len(non_cmte),\n                'mean_committee': np.nan, 'mean_non': np.nan,\n                'diff': np.nan, 't': np.nan, 'p': np.nan,\n            }\n            continue\n\n        t, p = stats.ttest_ind(cmte, non_cmte)\n        results[sector] = {\n            'n_committee':   len(cmte),\n            'n_non':         len(non_cmte),\n            'mean_committee':cmte.mean(),\n            'mean_non':      non_cmte.mean(),\n            'diff':          cmte.mean() - non_cmte.mean(),\n            't':             t,\n            'p':             p,\n        }\n    return results\n\n\ndef run_h2_within_member(df: pd.DataFrame) -> dict:\n    \"\"\"\n    Comparison A: Within-member — legislators who trade both relevant and non-relevant.\n    Compares mean CAR_drift for relevant vs. non-relevant trades per member.\n    \"\"\"\n    name_col = 'Name_x' if 'Name_x' in df.columns else 'Name'\n    members_with_both = (\n        df.groupby(name_col)['Is_Committee_Relevant']\n          .nunique()\n          .pipe(lambda s: s[s > 1].index)\n    )\n    df_both = df[df[name_col].isin(members_with_both)]\n\n    relevant     = df_both[df_both['Is_Committee_Relevant'] == True]['CAR_drift'].dropna()\n    non_relevant = df_both[df_both['Is_Committee_Relevant'] == False]['CAR_drift'].dropna()\n\n    if len(relevant) < 5 or len(non_relevant) < 5:\n        return {'n_relevant': len(relevant), 'n_non': len(non_relevant),\n                'diff': np.nan, 't': np.nan, 'p': np.nan}\n\n    t, p = stats.ttest_ind(relevant, non_relevant)\n    return {\n        'n_relevant':   len(relevant),\n        'n_non':        len(non_relevant),\n        'n_legislators':len(members_with_both),\n        'mean_relevant':relevant.mean(),\n        'mean_non':     non_relevant.mean(),\n        'diff':         relevant.mean() - non_relevant.mean(),\n        't':            t,\n        'p':            p,\n    }\n\n\ndef run_h2_regression(df: pd.DataFrame):\n    \"\"\"\n    OLS: CAR_drift = alpha + beta*Is_Committee_Relevant + controls\n    Cluster SE by legislator.\n    \"\"\"\n    name_col = 'Name_x' if 'Name_x' in df.columns else 'Name'\n    df_reg = df.dropna(subset=['CAR_drift', 'Is_Committee_Relevant']).copy()\n    df_reg['comm_int']  = df_reg['Is_Committee_Relevant'].astype(int)\n    df_reg['buy']       = (df_reg['Direction'] == 1).astype(int)\n    df_reg['log_amount']= np.log1p(df_reg.get('TotalAmount', pd.Series(1, index=df_reg.index)))\n\n    if len(df_reg) < 20:\n        print('Insufficient data for H2 regression.')\n        return None\n\n    try:\n        X = sm.add_constant(df_reg[['comm_int', 'buy', 'log_amount']])\n        y = df_reg['CAR_drift']\n        groups = df_reg[name_col] if name_col in df_reg.columns else df_reg.index\n        model = sm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': groups})\n        return model\n    except Exception as e:\n        print(f'H2 regression failed: {e}')\n        return None\n\n\n# ─── Run H2 ───\nh2_within = run_h2_within_member(df_cars_enriched)\nh2_cross  = run_h2_cross_member(df_cars_enriched)\nh2_reg    = run_h2_regression(df_cars_enriched)\n\n# Within-member\nprint('\\n' + '=' * 70)\nprint('H2: WITHIN-MEMBER COMPARISON')\nprint('=' * 70)\nif not np.isnan(h2_within.get('diff', np.nan)):\n    print(f\"Legislators trading in both relevant and non-relevant sectors: {h2_within.get('n_legislators', 'N/A')}\")\n    print(f\"Mean CAR_drift (Committee-Relevant): {h2_within['mean_relevant']*100:.3f}%\")\n    print(f\"Mean CAR_drift (Non-Relevant):       {h2_within['mean_non']*100:.3f}%\")\n    print(f\"Difference:                          {h2_within['diff']*100:.3f}%\")\n    print(f\"t-stat: {h2_within['t']:.3f} | p-value: {h2_within['p']:.4f}\")\nelse:\n    print('Insufficient within-member comparisons (requires legislators trading in both categories).')\n\n# Cross-member\nprint('\\n' + '=' * 80)\nprint('H2: CROSS-MEMBER COMPARISON BY SECTOR')\nprint('=' * 80)\nprint(f'{\"Sector\":<30} {\"N(Cmte)\":>8} {\"N(Non)\":>8} {\"Δ CAR\":>10} {\"t\":>8} {\"p\":>10}')\nprint('-' * 80)\nfor sector, res in h2_cross.items():\n    if np.isnan(res.get('diff', np.nan)):\n        print(f'{sector:<30} {res[\"n_committee\"]:>8} {res[\"n_non\"]:>8}  {\"N/A\":>10}')\n        continue\n    stars = '***' if res['p'] < 0.01 else '**' if res['p'] < 0.05 else '*' if res['p'] < 0.10 else ''\n    print(f'{sector:<30} {res[\"n_committee\"]:>8} {res[\"n_non\"]:>8} '\n          f'{res[\"diff\"]*100:>9.2f}%{stars} {res[\"t\"]:>8.3f} {res[\"p\"]:>10.4f}')\n\n# Regression\nif h2_reg is not None:\n    print('\\n' + '=' * 60)\n    print('H2 REGRESSION RESULTS (Clustered SE by Legislator)')\n    print('=' * 60)\n    coef  = h2_reg.params.get('comm_int', np.nan)\n    pval  = h2_reg.pvalues.get('comm_int', np.nan)\n    print(h2_reg.summary2().tables[1].round(4))\n    print('\\nINTERPRETATION:')\n    if not np.isnan(pval):\n        if pval < 0.05 and coef > 0:\n            print('H2 CONFIRMED: Committee-relevant trades significantly outperform (p < 0.05)')\n            print('DECISION: Apply committee filter in signal engine (already enforced in Module 12)')\n        else:\n            print('H2 NOT CONFIRMED: No significant committee information advantage')\n            print('DECISION: Committee filter retained for risk management; equal weighting applied')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "---\n## Phase 4: Signal Engine\n\n**Refactored from prior code — key fixes:**\n1. **Bug fix:** Committee filter actually applied (`Is_Committee_Relevant == True` only)\n2. **Bug fix:** `AmountMidpoint` used instead of raw `Amount` for volume calculations\n3. **Enhancement:** `current_date` parameter enables zero-lookahead historical backtesting\n4. **Enhancement:** `conviction_score` returned as continuous signal in `[-1, 1]`\n5. **Fix:** `conviction_thresh` parameter not hardcoded — read from caller\n\n**Signal logic:** For each sector, aggregate committee-relevant trades in `lookback_days` window ending at `current_date` (using `ReportDate`). Score = `net_volume / total_volume`. Gates: diversity (min members, min tickers) + volume (min transactions). Score ≥ threshold → BUY; ≤ −threshold → SELL; else FLAT."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════════════════\n# MODULE 12: Signal Generator\n# ═══════════════════════════════════════════════════════════════════════════\n\ndef generate_signals(\n    df_enriched: pd.DataFrame,\n    config: dict = None,\n    lookback_days: int = LOOKBACK_DAYS,\n    conviction_thresh: float = CONVICTION_THRESHOLD,\n    current_date=None,\n    committee_filter: bool = True,\n) -> tuple:\n    \"\"\"\n    Generate sector-level BUY/SELL/FLAT signals.\n\n    CRITICAL: Uses ReportDate only — NO LOOKAHEAD from TransactionDate (ASSUMPTION A5).\n    BUG FIX: committee_filter=True restricts to Is_Committee_Relevant trades only.\n    BUG FIX: Uses AmountMidpoint (not raw Amount) for volume weighting (ASSUMPTION A1).\n    ENHANCEMENT: conviction_score is continuous in [-1,1] for portfolio construction.\n    ENHANCEMENT: current_date enables zero-lookahead historical simulation.\n\n    Returns:\n        (sector_summary_df, detailed_trades_df)\n    \"\"\"\n    if config is None:\n        config = SECTOR_CONFIG\n\n    if current_date is None:\n        current_date = df_enriched['ReportDate'].max()\n    current_date   = pd.Timestamp(current_date)\n    lookback_start = current_date - pd.Timedelta(days=lookback_days)\n\n    # CRITICAL: Filter by ReportDate only — window ends at current_date\n    window_df = df_enriched[\n        (df_enriched['ReportDate'] >= lookback_start) &\n        (df_enriched['ReportDate'] <= current_date)\n    ].copy()\n\n    # BUG FIX: Committee filter\n    if committee_filter:\n        window_df = window_df[window_df['Is_Committee_Relevant'] == True].copy()\n\n    sector_results = []\n    detailed_frames = []\n\n    for sector, cfg in config.items():\n        sdf = window_df[window_df['project_sector'] == sector].copy()\n\n        if len(sdf) == 0:\n            sector_results.append({\n                'Sector':             sector,\n                'target_etf':         cfg['target_etf'],\n                'directive':          f'INSUFFICIENT DATA: No trades in {lookback_days}-day window',\n                'conviction_score':   0.0,\n                'total_transactions': 0,\n                'net_volume':         0.0,\n                'total_volume':       0.0,\n                'n_members':          0,\n                'n_tickers':          0,\n                'diversity_pass':     False,\n                'volume_pass':        False,\n            })\n            continue\n\n        # BUG FIX: Volume calculated with AmountMidpoint\n        sdf['signed_volume'] = sdf['Direction'] * sdf['AmountMidpoint']\n        net_volume   = sdf['signed_volume'].sum()\n        total_volume = sdf['AmountMidpoint'].sum()\n        n_members    = sdf['Name'].nunique()\n        n_tickers    = sdf['Ticker'].nunique()\n        n_txn        = len(sdf)\n\n        diversity_pass = (n_members >= cfg['tau_member']) and (n_tickers >= cfg['tau_ticker'])\n        volume_pass    = n_txn >= cfg['min_transactions']\n\n        # Conviction score: bounded [-1, 1]\n        conviction_score = (net_volume / total_volume) if total_volume > 0 else 0.0\n        conviction_score = float(np.clip(conviction_score, -1.0, 1.0))\n\n        # Directive string\n        if not volume_pass:\n            directive = f'INSUFFICIENT DATA: {n_txn} transactions < {cfg[\"min_transactions\"]} required'\n        elif not diversity_pass:\n            directive = f'FLAT: Diversity gate failed ({n_members} members, {n_tickers} tickers)'\n        elif conviction_score >= conviction_thresh:\n            directive = f'BUY LONG {cfg[\"target_etf\"]}'\n        elif conviction_score <= -conviction_thresh:\n            directive = f'SELL SHORT {cfg[\"target_etf\"]}'\n        else:\n            bias = 'bullish' if conviction_score > 0 else 'bearish'\n            directive = f'FLAT: Conviction {conviction_score:.2f} below threshold ({bias})'\n\n        sector_results.append({\n            'Sector':             sector,\n            'target_etf':         cfg['target_etf'],\n            'directive':          directive,\n            'conviction_score':   round(conviction_score, 4),\n            'total_transactions': n_txn,\n            'net_volume':         net_volume,\n            'total_volume':       total_volume,\n            'n_members':          n_members,\n            'n_tickers':          n_tickers,\n            'diversity_pass':     diversity_pass,\n            'volume_pass':        volume_pass,\n        })\n        detailed_frames.append(sdf)\n\n    sector_summary = pd.DataFrame(sector_results)\n    detailed_df    = pd.concat(detailed_frames, ignore_index=True) if detailed_frames else pd.DataFrame()\n    return sector_summary, detailed_df\n\n\n# ─── Current Signal Dashboard ───\ncurrent_signals, current_detail = generate_signals(\n    df_enriched,\n    lookback_days=LOOKBACK_DAYS,\n    conviction_thresh=CONVICTION_THRESHOLD,\n    committee_filter=True,\n)\n\nprint('\\n' + '=' * 90)\nprint(f'SIGNAL DASHBOARD — {df_enriched[\"ReportDate\"].max().date()}')\nprint(f'Lookback: {LOOKBACK_DAYS} days | Committee filter: ON | Threshold: {CONVICTION_THRESHOLD}')\nprint('=' * 90)\ndisp_cols = ['Sector', 'target_etf', 'directive', 'conviction_score',\n             'total_transactions', 'n_members', 'n_tickers']\nprint(current_signals[disp_cols].to_string(index=False))\n\nprint('\\n--- ACTIONABLE DIRECTIVES ---')\nactionable = current_signals[current_signals['directive'].str.startswith(('BUY', 'SELL'))]\nif actionable.empty:\n    print('No sectors currently exceed conviction threshold.')\nelse:\n    for _, row in actionable.iterrows():\n        print(f'  {row[\"Sector\"]}: {row[\"directive\"]} (score={row[\"conviction_score\"]:.3f})')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# ─── Historical Signal Heatmap ───\nprint('Generating historical signal heatmap (month-end signals)...')\nmonthly_dates = pd.date_range(SAMPLE_START, df_enriched['ReportDate'].max(), freq='ME')\n\nhist_signals = {}\nfor date in tqdm(monthly_dates, desc='Historical signals'):\n    sigs, _ = generate_signals(\n        df_enriched,\n        lookback_days=BACKTEST_LOOKBACK_DAYS,\n        conviction_thresh=CONVICTION_THRESHOLD,\n        current_date=date,\n        committee_filter=True,\n    )\n    for _, row in sigs.iterrows():\n        hist_signals[(date, row['Sector'])] = row['conviction_score']\n\nsignal_matrix = (\n    pd.DataFrame(\n        [(d, s, v) for (d, s), v in hist_signals.items()],\n        columns=['date', 'sector', 'conviction']\n    )\n    .pivot(index='date', columns='sector', values='conviction')\n    .fillna(0)\n)\n\nfig, ax = plt.subplots(figsize=(16, 5))\nsns.heatmap(\n    signal_matrix.T,\n    cmap='RdYlGn', center=0, vmin=-1, vmax=1,\n    ax=ax,\n    xticklabels=[\n        d.strftime('%Y-%m') if i % 6 == 0 else ''\n        for i, d in enumerate(signal_matrix.index)\n    ],\n    yticklabels=True,\n    cbar_kws={'label': 'Conviction Score'},\n    linewidths=0.3,\n)\nax.set_title(\n    'Historical Conviction Signal Heatmap by Sector\\n'\n    f'(Committee-relevant trades only | {BACKTEST_LOOKBACK_DAYS}-day lookback)',\n    fontsize=13, fontweight='bold'\n)\nax.set_xlabel('Month')\nax.set_ylabel('Sector')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": "---\n## Phase 5: Backtest\n\n**Primary Strategy — Variant A (Absolute Threshold):**\n- Monthly rebalancing at month-end\n- `generate_signals()` with 90-day lookback, committee filter ON\n- Equal-weight sectors where conviction ≥ 0.80 → LONG\n- Equal-weight sectors where conviction ≤ −0.80 → SHORT\n- Zero-signal months → all cash (return = 0)\n- Transaction cost: 10 bps one-way\n\n**ASSUMPTION A8:** Threshold-based variant is primary. The hypothesis is absolute: a sector with a strong enough signal is tradeable independently. Tercile ranking fails with sparse signals (the common case with 7 sectors and strict gating).\n\n**ASSUMPTION A9:** Strategy will be net-long most months — congressional buying clusters in bull markets. This is a signal property, not a bug. Factor regression alpha is the primary evaluation metric."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════════════════\n# MODULES 13-14: Backtest — Portfolio Construction & Performance\n# ═══════════════════════════════════════════════════════════════════════════\n\ndef run_backtest(\n    df_enriched: pd.DataFrame,\n    etf_returns: pd.DataFrame,\n    config: dict = None,\n    start_date: str = SAMPLE_START,\n    end_date: str = SAMPLE_END,\n    lookback_days: int = BACKTEST_LOOKBACK_DAYS,\n    conviction_thresh: float = CONVICTION_THRESHOLD,\n    rebalance_freq: str = 'ME',\n    tc_bps: int = TRANSACTION_COST_BPS,\n    variant: str = 'threshold',\n) -> pd.DataFrame:\n    \"\"\"\n    Full historical backtest with monthly rebalancing.\n\n    CRITICAL: Signal at each rebalance_date uses only data UP TO rebalance_date.\n              ETF returns for the holding period are applied AFTER the rebalance_date.\n              No future information used in signal generation.\n\n    variant: 'threshold' (primary) or 'tercile' (robustness)\n    \"\"\"\n    if config is None:\n        config = SECTOR_CONFIG\n\n    rebalance_dates = pd.date_range(start_date, end_date, freq=rebalance_freq)\n    etf_pivot = (\n        etf_returns\n        .pivot(index='date', columns='etf_ticker', values='ret')\n        .pipe(lambda df: df.set_index(pd.to_datetime(df.index)))\n        .sort_index()\n    )\n\n    records       = []\n    prev_positions = {}\n\n    for i, rebalance_date in enumerate(rebalance_dates[:-1]):\n        next_date = rebalance_dates[i + 1]\n\n        # CRITICAL: Signal uses only data up to rebalance_date\n        signals, _ = generate_signals(\n            df_enriched, config=config,\n            lookback_days=lookback_days,\n            conviction_thresh=conviction_thresh,\n            current_date=rebalance_date,\n            committee_filter=True,\n        )\n\n        # Determine positions\n        if variant == 'threshold':\n            long_etfs  = signals[\n                signals['directive'].str.startswith('BUY') &\n                signals['diversity_pass'] & signals['volume_pass']\n            ]['target_etf'].tolist()\n            short_etfs = signals[\n                signals['directive'].str.startswith('SELL') &\n                signals['diversity_pass'] & signals['volume_pass']\n            ]['target_etf'].tolist()\n        elif variant == 'tercile':\n            valid = signals[signals['diversity_pass'] & signals['volume_pass']]\n            if len(valid) < 3:\n                long_etfs, short_etfs = [], []\n            else:\n                n_each    = len(valid) // 3\n                sorted_v  = valid.sort_values('conviction_score', ascending=False)\n                long_etfs  = sorted_v.head(n_each)['target_etf'].tolist()\n                short_etfs = sorted_v.tail(n_each)['target_etf'].tolist()\n        else:\n            raise ValueError(f'Unknown variant: {variant}')\n\n        # Equal-weight positions\n        new_positions = {}\n        if long_etfs:\n            w = 1.0 / len(long_etfs)\n            for etf in long_etfs:\n                new_positions[etf] = new_positions.get(etf, 0) + w\n        if short_etfs:\n            w = -1.0 / len(short_etfs)\n            for etf in short_etfs:\n                new_positions[etf] = new_positions.get(etf, 0) + w\n\n        # Turnover\n        all_etfs = set(prev_positions) | set(new_positions)\n        turnover = sum(\n            abs(new_positions.get(e, 0) - prev_positions.get(e, 0))\n            for e in all_etfs\n        ) / 2\n\n        # Portfolio return: compound daily returns over holding period\n        period_rets = etf_pivot[\n            (etf_pivot.index > rebalance_date) & (etf_pivot.index <= next_date)\n        ]\n\n        if period_rets.empty or not new_positions:\n            port_ret_gross = 0.0\n        else:\n            daily_port = pd.Series(0.0, index=period_rets.index)\n            for etf, weight in new_positions.items():\n                if etf in period_rets.columns:\n                    daily_port += weight * period_rets[etf].fillna(0)\n            port_ret_gross = (1 + daily_port).prod() - 1\n\n        tc             = turnover * tc_bps / 10_000\n        port_ret_net   = port_ret_gross - tc\n\n        records.append({\n            'date':             next_date,\n            'rebalance_date':   rebalance_date,\n            'portfolio_ret_gross': port_ret_gross,\n            'portfolio_ret_net':   port_ret_net,\n            'long_etfs':        long_etfs,\n            'short_etfs':       short_etfs,\n            'n_long':           len(long_etfs),\n            'n_short':          len(short_etfs),\n            'n_valid_signals':  int(signals['diversity_pass'] & signals['volume_pass']).sum(),\n            'turnover':         turnover,\n        })\n        prev_positions = new_positions\n\n    return pd.DataFrame(records)\n\n\n# ─── Run Primary Backtest ───\nprint('Running backtest (Variant A — Threshold)...')\nbacktest_results = run_backtest(\n    df_enriched, etf_returns,\n    start_date=SAMPLE_START, end_date=SAMPLE_END,\n    lookback_days=BACKTEST_LOOKBACK_DAYS,\n    conviction_thresh=CONVICTION_THRESHOLD,\n    variant='threshold',\n)\nprint(f'Complete: {len(backtest_results)} monthly periods')\nprint(f'Months with positions: {(backtest_results[\"n_long\"] + backtest_results[\"n_short\"] > 0).sum()}')\nbacktest_results.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════════════════\n# Performance Analytics & Plots\n# ═══════════════════════════════════════════════════════════════════════════\n\ndef compute_performance(returns: pd.Series, rf_annual: float = 0.04) -> dict:\n    \"\"\"Annualized performance metrics for monthly return series.\"\"\"\n    periods = 12\n    rf_monthly = rf_annual / periods\n\n    ann_ret = (1 + returns).prod() ** (periods / len(returns)) - 1\n    ann_vol = returns.std() * np.sqrt(periods)\n    excess  = returns - rf_monthly\n    sharpe  = (excess.mean() / returns.std()) * np.sqrt(periods) if returns.std() > 0 else np.nan\n\n    cum = (1 + returns).cumprod()\n    rolling_max = cum.cummax()\n    dd = (cum - rolling_max) / rolling_max\n    max_dd = dd.min()\n    end_dd = dd.idxmin()\n    start_dd = cum[:end_dd].idxmax() if not cum[:end_dd].empty else end_dd\n\n    invested = (backtest_results['n_long'] + backtest_results['n_short'] > 0).mean()\n\n    return {\n        'annualized_return':       ann_ret,\n        'annualized_vol':          ann_vol,\n        'sharpe_ratio':            sharpe,\n        'max_drawdown':            max_dd,\n        'max_drawdown_start':      start_dd,\n        'max_drawdown_end':        end_dd,\n        'calmar_ratio':            ann_ret / abs(max_dd) if max_dd != 0 else np.nan,\n        'hit_rate':                (returns > 0).mean(),\n        'best_month':              returns.max(),\n        'worst_month':             returns.min(),\n        'n_months':                len(returns),\n        'pct_months_invested':     invested,\n        'avg_monthly_turnover':    backtest_results['turnover'].mean(),\n    }\n\n\nstrategy_returns = backtest_results.set_index('date')['portfolio_ret_net']\nperf = compute_performance(strategy_returns)\n\n# SPY benchmark (monthly, compounded from daily)\nspy_monthly = (\n    etf_returns[etf_returns['etf_ticker'] == 'SPY']\n    .set_index('date')['ret']\n    .resample('ME')\n    .apply(lambda x: (1 + x).prod() - 1)\n    .rename('SPY')\n)\n\nprint('\\n' + '=' * 60)\nprint('BACKTEST PERFORMANCE SUMMARY (Variant A — Threshold)')\nprint('=' * 60)\nfor k, v in perf.items():\n    if isinstance(v, float) and not np.isnan(v):\n        if 'return' in k or 'vol' in k or 'drawdown' in k or 'rate' in k or 'month' in k or 'turnover' in k:\n            print(f'  {k:<30} {v:>10.2%}')\n        else:\n            print(f'  {k:<30} {v:>10.3f}')\n    elif hasattr(v, 'date'):\n        print(f'  {k:<30} {v.date()}')\n    else:\n        print(f'  {k:<30} {v}')\nprint('=' * 60)\n\n# ─── Plots ───\nfig, axes = plt.subplots(3, 1, figsize=(16, 14))\nfig.suptitle('Backtest — Congressional Trading Signal Strategy',\n             fontsize=14, fontweight='bold')\n\n# 1. Cumulative returns\nax = axes[0]\ncum_strat = (1 + strategy_returns).cumprod()\ncum_spy   = (1 + spy_monthly.reindex(strategy_returns.index).fillna(0)).cumprod()\nax.plot(cum_strat.index, cum_strat.values, label='Congressional Signal Strategy',\n        linewidth=2, color='steelblue')\nax.plot(cum_spy.index, cum_spy.values, label='SPY Benchmark',\n        linewidth=1.5, color='coral', linestyle='--')\nax.set_title('Cumulative Returns (Net of 10 bps Transaction Costs)')\nax.set_ylabel('Growth of $1')\nax.legend()\nax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'${y:.2f}'))\n\n# 2. Drawdown\nax = axes[1]\ncum_ret = (1 + strategy_returns).cumprod()\nrolling_max = cum_ret.cummax()\ndd = (cum_ret - rolling_max) / rolling_max\nax.fill_between(dd.index, dd.values, 0, color='coral', alpha=0.6, label='Drawdown')\nax.set_title('Strategy Drawdown')\nax.set_ylabel('Drawdown')\nax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1%}'))\nax.legend()\n\n# 3. Monthly returns heatmap\nax = axes[2]\nmr = strategy_returns.copy()\nmr.index = pd.to_datetime(mr.index)\npivot = (\n    mr.to_frame('ret')\n      .assign(year=lambda d: d.index.year, month=lambda d: d.index.month)\n      .pivot(index='year', columns='month', values='ret')\n)\npivot.columns = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nsns.heatmap(pivot, cmap='RdYlGn', center=0, annot=True, fmt='.1%',\n            ax=ax, cbar_kws={'label': 'Monthly Return'}, linewidths=0.5)\nax.set_title('Monthly Returns Heatmap')\n\nplt.tight_layout()\nplt.show()\n\n# Composition chart\nfig, ax = plt.subplots(figsize=(10, 4))\nn_pos = backtest_results['n_long'] + backtest_results['n_short']\nax.hist(n_pos, bins=range(0, max(n_pos) + 2), edgecolor='white',\n        color='steelblue', alpha=0.8)\nax.set_title('Distribution of Active Sector Positions per Month')\nax.set_xlabel('Number of ETF Positions')\nax.set_ylabel('Number of Months')\nplt.tight_layout()\nplt.show()\n\n# ─── Robustness: Variant B (Tercile) ───\nprint('\\nRunning robustness check (Variant B — Tercile Ranking)...')\nbt_tercile = run_backtest(\n    df_enriched, etf_returns,\n    start_date=SAMPLE_START, end_date=SAMPLE_END,\n    lookback_days=BACKTEST_LOOKBACK_DAYS, conviction_thresh=CONVICTION_THRESHOLD,\n    variant='tercile',\n)\nperf_t = compute_performance(bt_tercile.set_index('date')['portfolio_ret_net'])\n\nprint(f'\\n{\"Metric\":<30} {\"Threshold (Primary)\":>22} {\"Tercile (Robustness)\":>22}')\nprint('-' * 78)\nfor k in ['annualized_return', 'annualized_vol', 'sharpe_ratio', 'max_drawdown']:\n    v_a = perf.get(k, np.nan)\n    v_b = perf_t.get(k, np.nan)\n    fmt = '.2%' if any(x in k for x in ['return', 'vol', 'drawdown']) else '.3f'\n    print(f'{k:<30} {v_a:>22{fmt}} {v_b:>22{fmt}}')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": "---\n## Phase 6: Factor Regression — Alpha Evaluation\n\n**Model:**\n$$R_{portfolio,t} - R_{f,t} = \\alpha + \\beta_1\\text{MktRF} + \\beta_2\\text{SMB} + \\beta_3\\text{HML} + \\beta_4\\text{RMW} + \\beta_5\\text{CMA} + \\beta_6\\text{Mom} + \\epsilon_t$$\n\n**Interpretation guide:**\n- `α > 0, p < 0.05` → Strategy generates genuine alpha beyond known risk factors\n- `α ≈ 0` → Returns are explained by factor exposures\n- High `MktRF` loading → Strategy is primarily a directional market bet (expected per ASSUMPTION A9)\n- High `Mom` loading → Strategy is momentum in disguise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "# ═══════════════════════════════════════════════════════════════════════════\n# MODULE 15: Factor Regression — FF5 + Momentum\n# ═══════════════════════════════════════════════════════════════════════════\n\ndef run_factor_regression(portfolio_returns: pd.Series,\n                          factor_data: pd.DataFrame,\n                          frequency: str = 'monthly'):\n    \"\"\"\n    OLS regression of excess portfolio returns on FF5 + Momentum.\n    portfolio_returns must be EXCESS returns (net of RF).\n    factor_data must contain: Mkt_RF, SMB, HML, RMW, CMA, Mom.\n    Returns (model, reg_data) or None.\n    \"\"\"\n    if factor_data.empty:\n        print('Factor data not available.')\n        return None\n\n    f = factor_data.copy()\n    # Normalize column names\n    f.columns = [c.strip().replace('-', '_').replace(' ', '') for c in f.columns]\n    mom_col = next((c for c in f.columns if c.lower() in ('mom', 'wml', 'umd')), None)\n    if mom_col and mom_col != 'Mom':\n        f = f.rename(columns={mom_col: 'Mom'})\n\n    # Align to monthly if needed\n    if frequency == 'monthly':\n        f = f.resample('ME').last() if f.index.freq != 'ME' else f\n        f.index = f.index + pd.offsets.MonthEnd(0)\n\n    required = [c for c in ['Mkt_RF', 'SMB', 'HML', 'RMW', 'CMA', 'Mom'] if c in f.columns]\n    common   = portfolio_returns.index.intersection(f.index)\n\n    if len(common) < 12:\n        print(f'Insufficient aligned months: {len(common)}')\n        return None\n\n    port = portfolio_returns.loc[common]\n    fac  = f.loc[common]\n\n    # Excess returns (subtract RF)\n    rf = fac['RF'] if 'RF' in fac.columns else pd.Series(0.04 / 12, index=common)\n    excess = port - rf\n\n    reg_data = fac[required].copy()\n    reg_data['excess_ret'] = excess\n    reg_data = reg_data.dropna()\n\n    if len(reg_data) < 12:\n        print('Insufficient data after alignment.')\n        return None\n\n    X   = sm.add_constant(reg_data[required])\n    y   = reg_data['excess_ret']\n    res = sm.OLS(y, X).fit(cov_type='HC3')\n    return res, reg_data\n\n\ndef format_regression_table(model) -> pd.DataFrame:\n    \"\"\"Format OLS results as a clean coefficient table.\"\"\"\n    def stars(p):\n        return '***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.10 else ''\n\n    tbl = pd.DataFrame({\n        'Coefficient': model.params,\n        'Std Error':   model.bse,\n        't-stat':      model.tvalues,\n        'p-value':     model.pvalues,\n        'CI Lower':    model.conf_int()[0],\n        'CI Upper':    model.conf_int()[1],\n    })\n    tbl['Sig'] = tbl['p-value'].apply(stars)\n    return tbl.round(4)\n\n\n# ─── Run Regression ───\nif not ff_factors.empty:\n    reg_result = run_factor_regression(strategy_returns, ff_factors, frequency='monthly')\nelse:\n    reg_result = None\n    print('Factor data unavailable — skipping regression.')\n    print('Ensure pandas_datareader is installed and run fetch_ff5_mom().')\n\nif reg_result is not None:\n    reg_model, reg_data = reg_result\n\n    print('\\n' + '=' * 70)\n    print('FAMA-FRENCH 5 FACTOR + MOMENTUM REGRESSION')\n    print(f'Sample: {reg_data.index.min().date()} to {reg_data.index.max().date()}')\n    print(f'N = {len(reg_data)} monthly observations | HC3 standard errors')\n    print('=' * 70)\n    coef_table = format_regression_table(reg_model)\n    print(coef_table.to_string())\n    print(f'\\nR²          = {reg_model.rsquared:.4f}')\n    print(f'Adjusted R² = {reg_model.rsquared_adj:.4f}')\n\n    alpha     = reg_model.params.get('const', np.nan)\n    alpha_t   = reg_model.tvalues.get('const', np.nan)\n    alpha_p   = reg_model.pvalues.get('const', np.nan)\n    alpha_ann = (1 + alpha) ** 12 - 1 if not np.isnan(alpha) else np.nan\n    mkt_beta  = reg_model.params.get('Mkt_RF', np.nan)\n\n    print('\\n' + '=' * 70)\n    print('ALPHA ASSESSMENT')\n    print('=' * 70)\n    print(f'Monthly alpha:     {alpha:.4f}  ({alpha*10000:.1f} bps/month)')\n    print(f'Annualized alpha:  {alpha_ann:.2%}')\n    print(f't-statistic:       {alpha_t:.3f}')\n    print(f'p-value:           {alpha_p:.4f}')\n    print(f'Market beta:       {mkt_beta:.3f}')\n\n    print('\\nINTERPRETATION:')\n    if not np.isnan(alpha_p):\n        if alpha_p < 0.05 and alpha > 0:\n            print('RESULT: SIGNIFICANT POSITIVE ALPHA — strategy adds value beyond risk factors (p < 0.05)')\n        elif alpha_p < 0.10 and alpha > 0:\n            print('RESULT: MARGINAL POSITIVE ALPHA — borderline significant (p < 0.10)')\n        elif alpha_p > 0.10 and alpha > 0:\n            print('RESULT: POSITIVE BUT INSIGNIFICANT ALPHA — returns partially explained by factor tilts')\n        elif alpha <= 0:\n            print('RESULT: NON-POSITIVE ALPHA — strategy does not add value after factor adjustment')\n\n    if not np.isnan(mkt_beta) and abs(mkt_beta) > 0.5:\n        print(f'\\nNOTE: High MktRF loading ({mkt_beta:.2f}) confirms directional market exposure')\n        print('      (expected per ASSUMPTION A9 — congressional buying clusters in bull markets)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "# ─── Factor Loading Visualization ───\nif reg_result is not None:\n    reg_model, reg_data = reg_result\n    coef_table = format_regression_table(reg_model)\n\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    fig.suptitle('Factor Regression Results — FF5 + Momentum', fontsize=14, fontweight='bold')\n\n    # Factor loadings with 95% CI\n    ax = axes[0]\n    params = coef_table[coef_table.index != 'const'].copy()\n    y_pos  = range(len(params))\n    colors = ['steelblue' if c > 0 else 'coral' for c in params['Coefficient']]\n    ax.barh(y_pos, params['Coefficient'],\n            xerr=1.96 * params['Std Error'],\n            color=colors, alpha=0.7, capsize=5, edgecolor='white')\n    ax.axvline(0, color='black', linewidth=0.8)\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(params.index)\n    ax.set_title('Factor Loadings (±1.96 SE)')\n    ax.set_xlabel('Coefficient')\n\n    # Alpha highlighted separately\n    alpha_val = coef_table.loc['const', 'Coefficient'] if 'const' in coef_table.index else 0\n    alpha_se  = coef_table.loc['const', 'Std Error']   if 'const' in coef_table.index else 0\n    ax.text(0.95, 0.05,\n            f'α = {alpha_val:.4f}\\n(ann. {(1+alpha_val)**12-1:.1%})',\n            transform=ax.transAxes, ha='right', va='bottom',\n            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n\n    # Fitted vs actual\n    ax = axes[1]\n    fitted = reg_model.fittedvalues\n    actual = reg_data['excess_ret']\n    ax.scatter(fitted, actual, alpha=0.5, color='steelblue', s=30)\n    lims = [min(fitted.min(), actual.min()), max(fitted.max(), actual.max())]\n    ax.plot(lims, lims, 'r--', linewidth=1, label='45° line')\n    ax.set_title('Fitted vs Actual Excess Returns')\n    ax.set_xlabel('Fitted (Factor Model)')\n    ax.set_ylabel('Actual Portfolio Excess Return')\n    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1%}'))\n    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1%}'))\n    ax.legend()\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": "---\n## Summary & Conclusions\n\n### Notebook: QPM Final Project\n**Charter Objective:** Build and validate a systematic ETF trading strategy on congressional trading disclosures\n\n---\n\n### Assumptions Made\n| ID | Assumption | Impact if Wrong |\n|----|-----------|----------------|\n| A1 | Amount field treated as midpoint of STOCK Act range | Conviction scores shift; threshold may need recalibration |\n| A2 | Restrict to 7 sectors with high government intervention | May miss alpha in other sectors |\n| A3 | GICS classification is current, not point-in-time | Some tickers misclassified in historical periods |\n| A4 | Committee assignments at Congress session level, not exact date | Some trades tagged to wrong committee |\n| A5 | Lookback window uses ReportDate only | Signal may be stale for trades with long disclosure lag |\n| A6 | Dedup: composite key includes Amount | Trade counts slightly inflated for truly identical trades |\n| A7 | Transaction cost: 10 bps one-way | If costs are higher, net returns decrease proportionally |\n| A8 | Threshold-based portfolio (Variant A) is primary strategy | Strategy carries directional market exposure |\n| A9 | Strategy net-long most months (congressional buying in bull markets) | High MktRF beta in factor regression |\n| A10 | API column names normalized immediately: Filed→ReportDate, Traded→TransactionDate | Pipeline crashes if rename is missed |\n\n---\n\n### H1 Result\n*[Update based on actual test results from cell above]*\n\n### H2 Result\n*[Update based on actual committee analysis — requires congress.gov API key for full analysis]*\n\n### Backtest Result\n*[Update based on actual backtest performance metrics]*\n\n### Alpha Assessment\n*[Update based on factor regression output]*\n\n---\n\n### Unresolved Questions\n1. **Committee roster quality:** Full H2 analysis requires `CONGRESS_API_KEY` from congress.gov\n2. **Point-in-time GICS:** Currently using current sector classification (ASSUMPTION A3)\n3. **Lookback sensitivity:** Primary analysis uses 90-day window per proposal; robustness checks needed\n4. **Delisting survivorship:** Tickers delisted during the sample period may bias event study results\n\n### Next Required Artifacts\n- `logs/assumptions.md` — Formal log with all assumption IDs, dates, and rationale\n- `logs/data_transformations.md` — Row counts at each pipeline stage\n- Backtest return time series saved for external validation\n- Factor regression output saved to file"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
