{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency_bootstrap_marker\n",
    "# ─── Dependency Bootstrap (auto-installs missing lightweight libs) ───\n",
    "import importlib.util, subprocess, sys\n",
    "required = ['requests', 'statsmodels', 'pyarrow', 'yfinance', 'seaborn', 'tqdm', 'pandas_datareader', 'fuzzywuzzy']\n",
    "missing = [m for m in required if importlib.util.find_spec(m) is None]\n",
    "if missing:\n",
    "    print('Installing missing packages:', missing)\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q'] + missing)\n",
    "    except Exception as e:\n",
    "        print('Auto-install failed — install manually if needed:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-00",
   "metadata": {},
   "source": [
    "# Congressional Trading Signal Strategy\n",
    "## QPM Final Project — Chicago Booth\n",
    "\n",
    "**Objective:** Build and validate a systematic ETF trading strategy that aggregates Congressional trading disclosures into sector-level signals, tests whether those signals contain exploitable alpha, and evaluates strategy performance against standard factor models.\n",
    "\n",
    "**Hypotheses:**\n",
    "- **H1 (Market Underreaction):** `CAR[-1,+1] ≈ 0` but `CAR[+2,+20] > 0` for purchases — the market underreacts to congressional trade disclosures\n",
    "- **H2 (Committee Advantage):** Committee-relevant trades generate higher CARs than non-relevant trades\n",
    "\n",
    "**Pipeline:** Foundation → H1 Event Study → H2 Committee Analysis → Signal Engine → Backtest → Factor Regression\n",
    "\n",
    "**Critical Constraint:** All signals use `ReportDate` (disclosure date) — NEVER `TransactionDate`. The STOCK Act allows up to 45-day disclosure lag; this is a feature of the signal, not a bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Imports ───\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    import statsmodels.formula.api as smf\n",
    "except ImportError:\n",
    "    sm = None\n",
    "    smf = None\n",
    "    print('statsmodels not available; regression modules disabled.')\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    import json as _json\n",
    "    import urllib.request as _urlreq\n",
    "    class _Resp:\n",
    "        def __init__(self, resp):\n",
    "            self._resp = resp\n",
    "            self.status_code = getattr(resp, 'status', None)\n",
    "            self.text = resp.read().decode('utf-8', errors='ignore')\n",
    "        def json(self):\n",
    "            return _json.loads(self.text)\n",
    "        def raise_for_status(self):\n",
    "            if self.status_code and self.status_code >= 400:\n",
    "                raise Exception(f'Status {self.status_code}')\n",
    "    class _RequestsShim:\n",
    "        def get(self, url, headers=None, timeout=None):\n",
    "            req = _urlreq.Request(url, headers=headers or {})\n",
    "            with _urlreq.urlopen(req, timeout=timeout) as r:\n",
    "                return _Resp(r)\n",
    "    requests = _RequestsShim()\n",
    "    print('requests not available; using urllib shim (limited).')\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except ImportError:\n",
    "    yf = None\n",
    "    print('yfinance not available; price downloads disabled.')\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except ImportError:\n",
    "    sns = None\n",
    "    print('seaborn not available; using matplotlib defaults.')\n",
    "\n",
    "if sns is None:\n",
    "    class _DummySeaborn:\n",
    "        def heatmap(self, data, cmap=None, center=None, vmin=None, vmax=None, ax=None,\n",
    "                   xticklabels=True, yticklabels=True, cbar_kws=None, linewidths=0,\n",
    "                   annot=False, fmt='.2g'):\n",
    "            import numpy as _np\n",
    "            import matplotlib.pyplot as _plt\n",
    "            ax = ax or _plt.gca()\n",
    "            arr = data.to_numpy() if hasattr(data, 'to_numpy') else _np.array(data)\n",
    "            im = ax.imshow(arr, cmap=cmap, vmin=vmin, vmax=vmax, aspect='auto')\n",
    "            if cbar_kws is None:\n",
    "                cbar_kws = {}\n",
    "            ax.figure.colorbar(im, ax=ax, **cbar_kws)\n",
    "            if xticklabels is not False:\n",
    "                labels = data.columns if hasattr(data, 'columns') else range(arr.shape[1])\n",
    "                ax.set_xticks(range(len(labels)))\n",
    "                ax.set_xticklabels(labels)\n",
    "            if yticklabels is not False:\n",
    "                labels = data.index if hasattr(data, 'index') else range(arr.shape[0])\n",
    "                ax.set_yticks(range(len(labels)))\n",
    "                ax.set_yticklabels(labels)\n",
    "            if annot:\n",
    "                for r in range(arr.shape[0]):\n",
    "                    for c in range(arr.shape[1]):\n",
    "                        ax.text(c, r, format(arr[r, c], fmt), ha='center', va='center')\n",
    "            return im\n",
    "    sns = _DummySeaborn()\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x, **k: x\n",
    "    print('tqdm not available; progress bars disabled.')\n",
    "\n",
    "try:\n",
    "    import pandas_datareader.data as web\n",
    "except ImportError:\n",
    "    web = None\n",
    "\n",
    "try:\n",
    "    from fuzzywuzzy import fuzz, process as fuzz_process\n",
    "except ImportError:\n",
    "    fuzz = None\n",
    "    fuzz_process = None\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# ─── API & Environment ───\n",
    "def _load_env_file(path: Path = Path('.env')) -> None:\n",
    "    \"\"\"Load KEY=VALUE pairs from .env into process env if missing.\"\"\"\n",
    "    if not path.exists():\n",
    "        return\n",
    "    for raw in path.read_text().splitlines():\n",
    "        line = raw.strip()\n",
    "        if not line or line.startswith('#') or '=' not in line:\n",
    "            continue\n",
    "        key, value = line.split('=', 1)\n",
    "        key = key.strip()\n",
    "        value = value.strip().strip('\\\"').strip(\"'\")\n",
    "        os.environ.setdefault(key, value)\n",
    "\n",
    "_load_env_file()\n",
    "QUIVER_API_KEY = (os.environ.get('QUIVER_API_KEY') or '').strip()\n",
    "CONGRESS_API_KEY = (os.environ.get('CONGRESS_API_KEY') or '').strip()\n",
    "QUIVER_BASE_URL = 'https://api.quiverquant.com/beta'\n",
    "\n",
    "# ─── Sample Period ───\n",
    "SAMPLE_START          = '2016-01-01'\n",
    "SAMPLE_END            = '2025-12-31'\n",
    "ESTIMATION_WINDOW_START = '2015-01-01'\n",
    "\n",
    "# ─── Event Study ───\n",
    "ESTIMATION_WINDOW      = (-250, -30)\n",
    "EVENT_WINDOW_IMMEDIATE = (-1, 1)\n",
    "EVENT_WINDOW_DRIFT     = (2, 20)\n",
    "MIN_ESTIMATION_DAYS    = 120\n",
    "\n",
    "# ─── Signal Engine ───\n",
    "LOOKBACK_DAYS           = 45\n",
    "CONVICTION_THRESHOLD    = 0.80\n",
    "BACKTEST_LOOKBACK_DAYS  = 90\n",
    "\n",
    "# ─── Backtest ───\n",
    "REBALANCE_FREQUENCY   = 'ME'\n",
    "TRANSACTION_COST_BPS  = 10\n",
    "\n",
    "# ─── Amount Midpoints — STOCK Act range mapping ───\n",
    "# ASSUMPTION A1: Use midpoint of each STOCK Act range as dollar estimate.\n",
    "# Raw Amount is a lower bound; midpoints reduce systematic downward bias.\n",
    "AMOUNT_MIDPOINTS = {\n",
    "    1001:     8000,\n",
    "    15001:    32500,\n",
    "    50001:    75000,\n",
    "    100001:   175000,\n",
    "    250001:   375000,\n",
    "    500001:   750000,\n",
    "    1000001:  3000000,\n",
    "    5000001:  15000000,\n",
    "    25000001: 37500000,\n",
    "    50000001: 50000001,\n",
    "}\n",
    "\n",
    "def get_amount_midpoint(amount):\n",
    "    \"\"\"Map STOCK Act lower bound to midpoint estimate (ASSUMPTION A1).\"\"\"\n",
    "    for lower in sorted(AMOUNT_MIDPOINTS.keys(), reverse=True):\n",
    "        if amount >= lower:\n",
    "            return AMOUNT_MIDPOINTS[lower]\n",
    "    return amount\n",
    "\n",
    "# ─── Sector Configuration ───\n",
    "# ASSUMPTION A2: Restrict to 7 sectors with high government intervention.\n",
    "# This is a design choice — document edge cases in assumptions.md.\n",
    "SECTOR_CONFIG = {\n",
    "    'Defense & Aerospace': {\n",
    "        'target_etf': 'XAR',\n",
    "        'committees': [\n",
    "            'House Armed Services', 'Senate Armed Services',\n",
    "            'House Appropriations', 'Senate Appropriations',\n",
    "            'House Foreign Affairs', 'Senate Foreign Relations',\n",
    "        ],\n",
    "        'gics_sector': 'Industrials',\n",
    "        'gics_industry_keywords': ['Aerospace', 'Defense'],\n",
    "        'tau_member': 2, 'tau_ticker': 3, 'min_transactions': 8,\n",
    "    },\n",
    "    'Healthcare & Pharmaceuticals': {\n",
    "        'target_etf': 'XHS',\n",
    "        'committees': [\n",
    "            'House Energy and Commerce', 'Senate HELP',\n",
    "            'House Ways and Means', 'Senate Finance',\n",
    "        ],\n",
    "        'gics_sector': 'Health Care',\n",
    "        'gics_industry_keywords': ['Health Care', 'Pharmaceuticals', 'Biotechnology', 'Medical'],\n",
    "        'tau_member': 2, 'tau_ticker': 3, 'min_transactions': 8,\n",
    "    },\n",
    "    'Technology': {\n",
    "        'target_etf': 'XLK',\n",
    "        'committees': [\n",
    "            'House Science, Space, and Technology', 'Senate Commerce',\n",
    "            'House Judiciary', 'Senate Judiciary',\n",
    "            'House Energy and Commerce',\n",
    "        ],\n",
    "        'gics_sector': 'Information Technology',\n",
    "        'gics_industry_keywords': ['Software', 'Technology', 'Semiconductor', 'Hardware', 'IT Services'],\n",
    "        'tau_member': 2, 'tau_ticker': 3, 'min_transactions': 8,\n",
    "    },\n",
    "    'Financial Services': {\n",
    "        'target_etf': 'XLF',\n",
    "        'committees': [\n",
    "            'House Financial Services', 'Senate Banking',\n",
    "            'House Ways and Means', 'Senate Finance',\n",
    "        ],\n",
    "        'gics_sector': 'Financials',\n",
    "        'gics_industry_keywords': ['Banks', 'Insurance', 'Capital Markets', 'Financial'],\n",
    "        'tau_member': 2, 'tau_ticker': 3, 'min_transactions': 8,\n",
    "    },\n",
    "    'Energy': {\n",
    "        'target_etf': 'XLE',\n",
    "        'committees': [\n",
    "            'House Energy and Commerce', 'Senate Energy and Natural Resources',\n",
    "            'House Natural Resources', 'Senate Environment and Public Works',\n",
    "        ],\n",
    "        'gics_sector': 'Energy',\n",
    "        'gics_industry_keywords': ['Oil', 'Gas', 'Energy'],\n",
    "        'tau_member': 2, 'tau_ticker': 3, 'min_transactions': 8,\n",
    "    },\n",
    "    'Utilities': {\n",
    "        'target_etf': 'XLU',\n",
    "        'committees': [\n",
    "            'House Energy and Commerce', 'Senate Energy and Natural Resources',\n",
    "            'House Natural Resources', 'Senate Environment and Public Works',\n",
    "        ],\n",
    "        'gics_sector': 'Utilities',\n",
    "        'gics_industry_keywords': ['Electric', 'Gas', 'Water', 'Utilities'],\n",
    "        'tau_member': 2, 'tau_ticker': 3, 'min_transactions': 5,\n",
    "    },\n",
    "    'Industrials': {\n",
    "        'target_etf': 'XLI',\n",
    "        'committees': [\n",
    "            'House Transportation and Infrastructure',\n",
    "            'Senate Commerce, Science, and Transportation',\n",
    "            'House Armed Services', 'Senate Armed Services',\n",
    "        ],\n",
    "        'gics_sector': 'Industrials',\n",
    "        'gics_industry_keywords': ['Industrial', 'Manufacturing', 'Transportation', 'Construction'],\n",
    "        'tau_member': 2, 'tau_ticker': 3, 'min_transactions': 8,\n",
    "    },\n",
    "}\n",
    "\n",
    "ETF_TO_SECTOR = {v['target_etf']: k for k, v in SECTOR_CONFIG.items()}\n",
    "\n",
    "# ─── Paths ───\n",
    "DATA_DIR      = Path('data')\n",
    "RAW_DIR       = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "EXTERNAL_DIR  = DATA_DIR / 'external'\n",
    "LOGS_DIR      = Path('logs')\n",
    "\n",
    "for d in [RAW_DIR, PROCESSED_DIR, EXTERNAL_DIR, LOGS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Configuration loaded.')\n",
    "print(f'Sample period: {SAMPLE_START} to {SAMPLE_END}')\n",
    "print(f'Sectors tracked: {list(SECTOR_CONFIG.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-02",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: Data Pipeline\n",
    "### Modules: Quiver Client → Sector Mapper → Committee Mapper → Returns → Enrichment\n",
    "\n",
    "**Definition of Done:**\n",
    "- All data sources fetched, validated, and persisted to `data/processed/`\n",
    "- Master enriched dataset joins trades → sectors → committees\n",
    "- Schema validation passes with zero nulls in critical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# MODULE 1: Quiver Quant API Client\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "#\n",
    "# Verified API schema (live endpoint, 2026-02-22):\n",
    "#   Columns: Representative, BioGuideID, ReportDate, TransactionDate, Ticker,\n",
    "#            Transaction, Range, House, Amount, Party, last_modified, TickerType,\n",
    "#            Description, ExcessReturn, PriceChange, SPYChange\n",
    "#   Auth:    X-CSRFToken header + User-Agent required (Cloudflare blocks without UA)\n",
    "#   Dates:   Already named ReportDate / TransactionDate — no rename needed\n",
    "#   Chamber: 'House' column contains 'Representatives' or 'Senate'\n",
    "#   Transactions: 'Purchase', 'Sale', 'Sale (Full)', 'Sale (Partial)', 'Exchange'\n",
    "#\n",
    "# NORMALIZATION applied immediately on fetch:\n",
    "#   Representative → Name\n",
    "#   House          → Chamber  ('Representatives' → 'House', 'Senate' stays)\n",
    "#   Sale (Full) / Sale (Partial) → Sale\n",
    "#   Exchange → dropped (not a directional trade)\n",
    "\n",
    "QUIVER_HEADERS = {\n",
    "    'accept':     'application/json',\n",
    "        'User-Agent': 'Mozilla/5.0',   # Required — Cloudflare blocks without User-Agent\n",
    "}\n",
    "\n",
    "\n",
    "def fetch_live_trades(api_key: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch congressional trades from Quiver Quant /live/congresstrading endpoint.\n",
    "    Returns normalized DataFrame (Name, Chamber, ReportDate, TransactionDate, ...).\n",
    "    \"\"\"\n",
    "    headers = {**QUIVER_HEADERS, 'Authorization': f'Token {api_key}'}\n",
    "    url = f'{QUIVER_BASE_URL}/live/congresstrading'\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        df = pd.DataFrame(resp.json())\n",
    "    except Exception as e:\n",
    "        print(f'[fetch_live_trades] Request failed: {e}')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return _normalize_columns(df)\n",
    "\n",
    "\n",
    "def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize Quiver API column names to internal schema.\n",
    "    Handles both current API schema and legacy parquet files.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Legacy: Filed/Traded → ReportDate/TransactionDate\n",
    "    if 'Filed' in df.columns:\n",
    "        df = df.rename(columns={'Filed': 'ReportDate', 'Traded': 'TransactionDate'})\n",
    "\n",
    "    # Defensive: raise if date columns are still missing\n",
    "    if 'ReportDate' not in df.columns:\n",
    "        raise ValueError(f'Missing ReportDate. Found columns: {list(df.columns)}')\n",
    "\n",
    "    # Representative → Name\n",
    "    if 'Representative' in df.columns and 'Name' not in df.columns:\n",
    "        df = df.rename(columns={'Representative': 'Name'})\n",
    "\n",
    "    # House → Chamber; normalize values\n",
    "    if 'House' in df.columns and 'Chamber' not in df.columns:\n",
    "        df = df.rename(columns={'House': 'Chamber'})\n",
    "    if 'Chamber' in df.columns:\n",
    "        df['Chamber'] = df['Chamber'].replace({'Representatives': 'House'})\n",
    "\n",
    "    # Normalize sale transaction types\n",
    "    df['Transaction'] = df['Transaction'].replace({\n",
    "        'Sale (Full)':    'Sale',\n",
    "        'Sale (Partial)': 'Sale',\n",
    "    })\n",
    "\n",
    "    # Parse dates\n",
    "    for col in ['ReportDate', 'TransactionDate']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_historical(parquet_path) -> pd.DataFrame:\n",
    "    \"\"\"Load historical trades from parquet; normalize column names.\"\"\"\n",
    "    path = Path(parquet_path)\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_parquet(path)\n",
    "    return _normalize_columns(df)\n",
    "\n",
    "\n",
    "def sync_and_deduplicate(df_hist: pd.DataFrame, df_live: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge historical + live data, then deduplicate.\n",
    "\n",
    "    Dedup strategy:\n",
    "      1. Native transaction ID ('id', 'transaction_id') if present\n",
    "      2. Composite key: (Name, Ticker, TransactionDate, Transaction, Amount)\n",
    "         — includes Amount to preserve different-sized same-day trades (bug fix)\n",
    "    \"\"\"\n",
    "    if df_hist.empty:\n",
    "        df = df_live.copy()\n",
    "    elif df_live.empty:\n",
    "        df = df_hist.copy()\n",
    "    else:\n",
    "        df = pd.concat([df_hist, df_live], ignore_index=True)\n",
    "\n",
    "    id_col = next((c for c in ['transaction_id', 'id'] if c in df.columns), None)\n",
    "    if id_col:\n",
    "        df = df.drop_duplicates(subset=[id_col])\n",
    "    else:\n",
    "        dedup_cols = [c for c in ['Name', 'Ticker', 'TransactionDate', 'Transaction', 'Amount']\n",
    "                      if c in df.columns]\n",
    "        df = df.drop_duplicates(subset=dedup_cols)\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def validate_and_enrich_raw(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Validate raw trades; add Direction, AmountMidpoint, DisclosureLag.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Keep only Purchase and Sale (drop Exchange and anything else)\n",
    "    df = df[df['Transaction'].isin({'Purchase', 'Sale'})].copy()\n",
    "\n",
    "    # Direction encoding\n",
    "    df['Direction'] = df['Transaction'].map({'Purchase': 1, 'Sale': -1})\n",
    "\n",
    "    # Amount midpoint (ASSUMPTION A1: midpoint of STOCK Act range)\n",
    "    if 'Amount' in df.columns:\n",
    "        df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce').fillna(0)\n",
    "        df['AmountMidpoint'] = df['Amount'].apply(get_amount_midpoint)\n",
    "\n",
    "    # Disclosure lag in calendar days\n",
    "    if 'TransactionDate' in df.columns:\n",
    "        df['DisclosureLag'] = (df['ReportDate'] - df['TransactionDate']).dt.days\n",
    "    else:\n",
    "        df['DisclosureLag'] = np.nan\n",
    "\n",
    "    # Drop rows missing critical columns\n",
    "    critical = [c for c in ['ReportDate', 'Ticker', 'Name', 'Transaction'] if c in df.columns]\n",
    "    df = df.dropna(subset=critical)\n",
    "\n",
    "    # Filter to sample period using ReportDate only (ASSUMPTION A5 — no lookahead)\n",
    "    df = df[\n",
    "        (df['ReportDate'] >= SAMPLE_START) &\n",
    "        (df['ReportDate'] <= SAMPLE_END)\n",
    "    ].copy()\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ─── Load Data ───\n",
    "print('Loading congressional trading data...')\n",
    "\n",
    "df_hist = load_historical(RAW_DIR / 'congress_trading.parquet')\n",
    "\n",
    "if QUIVER_API_KEY:\n",
    "    print(f'Fetching live data from Quiver Quant API...')\n",
    "    df_live = fetch_live_trades(QUIVER_API_KEY)\n",
    "    if not df_live.empty:\n",
    "        print(f'  Live records fetched: {len(df_live):,}')\n",
    "    df_raw = sync_and_deduplicate(df_hist, df_live)\n",
    "elif not df_hist.empty:\n",
    "    df_raw = df_hist.copy()\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        'No data source available. Set QUIVER_API_KEY or provide data/raw/congress_trading.parquet.'\n",
    "    )\n",
    "\n",
    "df_raw = validate_and_enrich_raw(df_raw)\n",
    "print(f'\\nRaw trades loaded and validated: {len(df_raw):,}')\n",
    "print(f'Date range: {df_raw[\"ReportDate\"].min().date()} to {df_raw[\"ReportDate\"].max().date()}')\n",
    "print(f'Purchases: {(df_raw[\"Direction\"]==1).sum():,} | Sales: {(df_raw[\"Direction\"]==-1).sum():,}')\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# MODULE 2: Sector Mapper — Ticker → GICS → Project Sector\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "GICS_CACHE_PATH = RAW_DIR / 'yfinance_gics_cache.json'\n",
    "\n",
    "def _load_gics_cache() -> dict:\n",
    "    if GICS_CACHE_PATH.exists():\n",
    "        with open(GICS_CACHE_PATH) as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def _save_gics_cache(cache: dict):\n",
    "    with open(GICS_CACHE_PATH, 'w') as f:\n",
    "        json.dump(cache, f)\n",
    "\n",
    "\n",
    "def get_gics_for_tickers(tickers: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch GICS sector/industry for each ticker via yfinance.\n",
    "    Results are cached to data/raw/yfinance_gics_cache.json.\n",
    "    ASSUMPTION A3: Uses current GICS classification (not point-in-time).\n",
    "    \"\"\"\n",
    "    cache = _load_gics_cache()\n",
    "    uncached = [t for t in tickers if t not in cache]\n",
    "\n",
    "    if uncached:\n",
    "        print(f'Fetching GICS classification for {len(uncached)} uncached tickers...')\n",
    "        for ticker in tqdm(uncached, desc='yfinance GICS'):\n",
    "            try:\n",
    "                info = yf.Ticker(ticker).info\n",
    "                cache[ticker] = {\n",
    "                    'gics_sector':       info.get('sector', 'Unknown'),\n",
    "                    'gics_industry':     info.get('industry', 'Unknown'),\n",
    "                    'gics_sub_industry': info.get('industryDisp', info.get('industry', 'Unknown')),\n",
    "                    'fetched_date':      datetime.now().isoformat()[:10],\n",
    "                }\n",
    "            except Exception:\n",
    "                cache[ticker] = {\n",
    "                    'gics_sector': 'Unknown', 'gics_industry': 'Unknown',\n",
    "                    'gics_sub_industry': 'Unknown', 'fetched_date': 'error',\n",
    "                }\n",
    "        _save_gics_cache(cache)\n",
    "\n",
    "    rows = []\n",
    "    for ticker in tickers:\n",
    "        r = cache.get(ticker, {})\n",
    "        rows.append({\n",
    "            'Ticker':           ticker,\n",
    "            'gics_sector':      r.get('gics_sector', 'Unknown'),\n",
    "            'gics_industry':    r.get('gics_industry', 'Unknown'),\n",
    "            'gics_sub_industry':r.get('gics_sub_industry', 'Unknown'),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def map_to_project_sector(gics_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Map GICS industry to one of the 7 project sectors using keyword matching.\n",
    "    Tickers matching no sector are labeled 'Other' and excluded from signal engine.\n",
    "    \"\"\"\n",
    "    df = gics_df.copy()\n",
    "    df['project_sector'] = 'Other'\n",
    "\n",
    "    for sector, cfg in SECTOR_CONFIG.items():\n",
    "        keywords = cfg['gics_industry_keywords']\n",
    "        kw_match = df['gics_industry'].apply(\n",
    "            lambda x: any(kw.lower() in str(x).lower() for kw in keywords)\n",
    "        )\n",
    "        # Only assign if not yet mapped\n",
    "        df.loc[kw_match & (df['project_sector'] == 'Other'), 'project_sector'] = sector\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ─── Run Sector Mapping ───\n",
    "print('Mapping tickers to GICS project sectors...')\n",
    "unique_tickers = df_raw['Ticker'].dropna().unique().tolist()\n",
    "gics_df       = get_gics_for_tickers(unique_tickers)\n",
    "ticker_sector_map = map_to_project_sector(gics_df)\n",
    "ticker_sector_map = ticker_sector_map.astype(str)\n",
    "import importlib\n",
    "import sys\n",
    "import pyarrow as pa\n",
    "\n",
    "# Avoid duplicate Arrow extension registration in reused notebook kernels.\n",
    "if 'pandas.core.arrays.arrow.extension_types' not in sys.modules:\n",
    "    for ext_name in ('pandas.interval', 'pandas.period'):\n",
    "        try:\n",
    "            pa.unregister_extension_type(ext_name)\n",
    "        except pa.ArrowKeyError:\n",
    "            pass\n",
    "    importlib.import_module('pandas.core.arrays.arrow.extension_types')\n",
    "ticker_sector_map.to_parquet(\n",
    "    PROCESSED_DIR / 'ticker_sector_map.parquet',\n",
    "    index=False,\n",
    "    engine='pyarrow',\n",
    "    coerce_timestamps='ms',\n",
    "    version='2.6',\n",
    "    use_deprecated_int96_timestamps=False,\n",
    "    use_dictionary=False,\n",
    ")\n",
    "\n",
    "print('\\nProject sector distribution (top tickers):')\n",
    "print(ticker_sector_map['project_sector'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# MODULE 3: Committee Mapper — Legislator → Committee Assignment\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# ASSUMPTION A4: Committee assignments matched at Congress session level,\n",
    "# not exact date. Mid-session changes are rare; document in assumptions.md.\n",
    "CONGRESS_SESSIONS = {\n",
    "    114: ('2015-01-03', '2017-01-03'),\n",
    "    115: ('2017-01-03', '2019-01-03'),\n",
    "    116: ('2019-01-03', '2021-01-03'),\n",
    "    117: ('2021-01-03', '2023-01-03'),\n",
    "    118: ('2023-01-03', '2025-01-03'),\n",
    "    119: ('2025-01-03', '2027-01-03'),\n",
    "}\n",
    "\n",
    "# Official committee name → config short name mapping\n",
    "COMMITTEE_NAME_MAP = {\n",
    "    'House Committee on Armed Services':           'House Armed Services',\n",
    "    'Senate Committee on Armed Services':          'Senate Armed Services',\n",
    "    'House Committee on Appropriations':           'House Appropriations',\n",
    "    'Senate Committee on Appropriations':          'Senate Appropriations',\n",
    "    'House Committee on Foreign Affairs':          'House Foreign Affairs',\n",
    "    'Senate Committee on Foreign Relations':       'Senate Foreign Relations',\n",
    "    'House Committee on Energy and Commerce':      'House Energy and Commerce',\n",
    "    'Senate Committee on Health, Education, Labor, and Pensions': 'Senate HELP',\n",
    "    'House Committee on Ways and Means':           'House Ways and Means',\n",
    "    'Senate Committee on Finance':                 'Senate Finance',\n",
    "    'House Committee on Science, Space, and Technology': 'House Science, Space, and Technology',\n",
    "    'Senate Committee on Commerce, Science, and Transportation': 'Senate Commerce',\n",
    "    'House Committee on the Judiciary':            'House Judiciary',\n",
    "    'Senate Committee on the Judiciary':           'Senate Judiciary',\n",
    "    'House Committee on Financial Services':       'House Financial Services',\n",
    "    'Senate Committee on Banking, Housing, and Urban Affairs': 'Senate Banking',\n",
    "    'Senate Committee on Energy and Natural Resources': 'Senate Energy and Natural Resources',\n",
    "    'House Committee on Natural Resources':        'House Natural Resources',\n",
    "    'Senate Committee on Environment and Public Works': 'Senate Environment and Public Works',\n",
    "    'House Committee on Transportation and Infrastructure': 'House Transportation and Infrastructure',\n",
    "}\n",
    "\n",
    "\n",
    "def get_congress_session(trade_date) -> int:\n",
    "    \"\"\"Return Congress session number for a given date.\"\"\"\n",
    "    ts = pd.Timestamp(trade_date)\n",
    "    for session, (start, end) in CONGRESS_SESSIONS.items():\n",
    "        if pd.Timestamp(start) <= ts < pd.Timestamp(end):\n",
    "            return session\n",
    "    return 119  # default to current\n",
    "\n",
    "\n",
    "def _get_congress_api_key() -> str:\n",
    "    \"\"\"Load CONGRESS_API_KEY from env, then .env fallback.\"\"\"\n",
    "    key = (os.environ.get('CONGRESS_API_KEY') or '').strip()\n",
    "    if key:\n",
    "        return key\n",
    "\n",
    "    env_path = Path('.env')\n",
    "    if env_path.exists():\n",
    "        for raw in env_path.read_text().splitlines():\n",
    "            line = raw.strip()\n",
    "            if not line or line.startswith('#') or '=' not in line:\n",
    "                continue\n",
    "            k, v = line.split('=', 1)\n",
    "            if k.strip() == 'CONGRESS_API_KEY':\n",
    "                key = v.strip().strip('\"').strip(\"'\")\n",
    "                if key:\n",
    "                    os.environ['CONGRESS_API_KEY'] = key\n",
    "                    return key\n",
    "    return ''\n",
    "\n",
    "\n",
    "def _append_api_key(url: str, api_key: str) -> str:\n",
    "    \"\"\"Attach api_key + format=json to congress.gov URLs if missing.\"\"\"\n",
    "    out = url\n",
    "    if 'api_key=' not in out:\n",
    "        out += ('&' if '?' in out else '?') + f'api_key={api_key}'\n",
    "    if 'format=' not in out:\n",
    "        out += '&format=json'\n",
    "    return out\n",
    "\n",
    "\n",
    "def _normalize_text(value: str) -> str:\n",
    "    \"\"\"Lowercase and keep alphanumeric tokens only.\"\"\"\n",
    "    chars = []\n",
    "    for ch in str(value).lower():\n",
    "        chars.append(ch if ch.isalnum() else ' ')\n",
    "    return ' '.join(''.join(chars).split())\n",
    "\n",
    "\n",
    "def _normalize_chamber(chamber: str) -> str:\n",
    "    c = _normalize_text(chamber)\n",
    "    if 'house' in c:\n",
    "        return 'house'\n",
    "    if 'senate' in c:\n",
    "        return 'senate'\n",
    "    return ''\n",
    "\n",
    "\n",
    "TARGET_COMMITTEE_RULES = [\n",
    "    {'chamber': 'house',  'keywords': ['armed', 'services'],                         'mapped': 'House Armed Services'},\n",
    "    {'chamber': 'senate', 'keywords': ['armed', 'services'],                         'mapped': 'Senate Armed Services'},\n",
    "    {'chamber': 'house',  'keywords': ['appropriations'],                            'mapped': 'House Appropriations'},\n",
    "    {'chamber': 'senate', 'keywords': ['appropriations'],                            'mapped': 'Senate Appropriations'},\n",
    "    {'chamber': 'house',  'keywords': ['foreign', 'affairs'],                        'mapped': 'House Foreign Affairs'},\n",
    "    {'chamber': 'senate', 'keywords': ['foreign', 'relations'],                      'mapped': 'Senate Foreign Relations'},\n",
    "    {'chamber': 'house',  'keywords': ['energy', 'commerce'],                        'mapped': 'House Energy and Commerce'},\n",
    "    {'chamber': 'senate', 'keywords': ['health', 'education', 'labor', 'pensions'], 'mapped': 'Senate HELP'},\n",
    "    {'chamber': 'house',  'keywords': ['ways', 'means'],                             'mapped': 'House Ways and Means'},\n",
    "    {'chamber': 'senate', 'keywords': ['finance'],                                   'mapped': 'Senate Finance'},\n",
    "    {'chamber': 'house',  'keywords': ['science', 'space', 'technology'],            'mapped': 'House Science, Space, and Technology'},\n",
    "    {'chamber': 'senate', 'keywords': ['commerce', 'science', 'transportation'],     'mapped': 'Senate Commerce'},\n",
    "    {'chamber': 'house',  'keywords': ['judiciary'],                                 'mapped': 'House Judiciary'},\n",
    "    {'chamber': 'senate', 'keywords': ['judiciary'],                                 'mapped': 'Senate Judiciary'},\n",
    "    {'chamber': 'house',  'keywords': ['financial', 'services'],                     'mapped': 'House Financial Services'},\n",
    "    {'chamber': 'senate', 'keywords': ['banking', 'housing', 'urban', 'affairs'],   'mapped': 'Senate Banking'},\n",
    "    {'chamber': 'senate', 'keywords': ['energy', 'natural', 'resources'],            'mapped': 'Senate Energy and Natural Resources'},\n",
    "    {'chamber': 'house',  'keywords': ['natural', 'resources'],                      'mapped': 'House Natural Resources'},\n",
    "    {'chamber': 'senate', 'keywords': ['environment', 'public', 'works'],            'mapped': 'Senate Environment and Public Works'},\n",
    "    {'chamber': 'house',  'keywords': ['transportation', 'infrastructure'],          'mapped': 'House Transportation and Infrastructure'},\n",
    "]\n",
    "\n",
    "\n",
    "def _map_committee_name(raw_name: str, chamber: str) -> str | None:\n",
    "    \"\"\"Map congress.gov committee names to project committee labels.\"\"\"\n",
    "    exact = COMMITTEE_NAME_MAP.get(raw_name)\n",
    "    if exact:\n",
    "        return exact\n",
    "\n",
    "    name_norm = _normalize_text(raw_name)\n",
    "    chamber_norm = _normalize_chamber(chamber)\n",
    "\n",
    "    # Project config is keyed to full committees, not subcommittees.\n",
    "    if 'subcommittee' in name_norm:\n",
    "        return None\n",
    "\n",
    "    for rule in TARGET_COMMITTEE_RULES:\n",
    "        if chamber_norm != rule['chamber']:\n",
    "            continue\n",
    "        if all(keyword in name_norm for keyword in rule['keywords']):\n",
    "            return rule['mapped']\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_COMMITTEE_ROSTER_FALLBACK_CACHE = None\n",
    "\n",
    "\n",
    "def _get_open_data_committee_roster() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build fallback roster from unitedstates/congress-legislators current committee data.\n",
    "    Used when Congress API omits committee membership lists.\n",
    "    \"\"\"\n",
    "    global _COMMITTEE_ROSTER_FALLBACK_CACHE\n",
    "    if _COMMITTEE_ROSTER_FALLBACK_CACHE is not None:\n",
    "        return _COMMITTEE_ROSTER_FALLBACK_CACHE.copy()\n",
    "\n",
    "    base_url = 'https://raw.githubusercontent.com/unitedstates/congress-legislators/gh-pages'\n",
    "    try:\n",
    "        committees_resp = requests.get(f'{base_url}/committees-current.json', timeout=30)\n",
    "        members_resp = requests.get(f'{base_url}/committee-membership-current.json', timeout=30)\n",
    "        legislators_resp = requests.get(f'{base_url}/legislators-current.json', timeout=30)\n",
    "    except Exception as e:\n",
    "        print(f'  Open-data fallback fetch error: {e}')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if (\n",
    "        committees_resp.status_code != 200\n",
    "        or members_resp.status_code != 200\n",
    "        or legislators_resp.status_code != 200\n",
    "    ):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    committees = committees_resp.json()\n",
    "    committee_membership = members_resp.json()\n",
    "    legislators = legislators_resp.json()\n",
    "\n",
    "    bioguide_to_name = {}\n",
    "    for leg in legislators:\n",
    "        leg_id = (leg.get('id') or {}).get('bioguide')\n",
    "        name_info = leg.get('name') or {}\n",
    "        display_name = name_info.get('official_full')\n",
    "        if not display_name:\n",
    "            display_name = ' '.join(\n",
    "                part\n",
    "                for part in [name_info.get('first'), name_info.get('middle'), name_info.get('last')]\n",
    "                if part\n",
    "            ).strip()\n",
    "        if leg_id and display_name:\n",
    "            bioguide_to_name[leg_id] = display_name\n",
    "\n",
    "    rows = []\n",
    "    for cmte in committees:\n",
    "        committee_id = cmte.get('thomas_id')\n",
    "        if not committee_id:\n",
    "            continue\n",
    "\n",
    "        chamber_type = str(cmte.get('type', '')).lower()\n",
    "        chamber = 'House' if chamber_type == 'house' else 'Senate' if chamber_type == 'senate' else str(cmte.get('chamber', ''))\n",
    "\n",
    "        mapped_name = _map_committee_name(cmte.get('name', ''), chamber)\n",
    "        if not mapped_name:\n",
    "            continue\n",
    "\n",
    "        members = committee_membership.get(committee_id, [])\n",
    "        for member in members:\n",
    "            member_name = str(member.get('name') or '').strip()\n",
    "            if not member_name:\n",
    "                member_name = bioguide_to_name.get(member.get('bioguide'), '')\n",
    "            if not member_name:\n",
    "                continue\n",
    "            rows.append({\n",
    "                'member_name': member_name,\n",
    "                'committee_name': mapped_name,\n",
    "                'chamber': chamber,\n",
    "            })\n",
    "\n",
    "    fallback = pd.DataFrame(rows)\n",
    "    if not fallback.empty:\n",
    "        fallback = fallback.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    _COMMITTEE_ROSTER_FALLBACK_CACHE = fallback\n",
    "    return fallback.copy()\n",
    "\n",
    "\n",
    "def _extract_member_names(payload: dict) -> list:\n",
    "    \"\"\"Recursively extract member names from congress API payloads.\"\"\"\n",
    "    names = []\n",
    "\n",
    "    def walk(node):\n",
    "        if isinstance(node, dict):\n",
    "            for key, value in node.items():\n",
    "                lk = str(key).lower()\n",
    "                if lk in {'members', 'member', 'committee_members', 'committeemembers'} and isinstance(value, list):\n",
    "                    for item in value:\n",
    "                        if isinstance(item, str):\n",
    "                            names.append(item)\n",
    "                        elif isinstance(item, dict):\n",
    "                            name = (\n",
    "                                item.get('name')\n",
    "                                or item.get('memberName')\n",
    "                                or item.get('fullName')\n",
    "                                or (item.get('member', {}) or {}).get('name')\n",
    "                            )\n",
    "                            if name:\n",
    "                                names.append(str(name))\n",
    "                elif isinstance(value, (dict, list)):\n",
    "                    walk(value)\n",
    "        elif isinstance(node, list):\n",
    "            for item in node:\n",
    "                if isinstance(item, (dict, list)):\n",
    "                    walk(item)\n",
    "\n",
    "    walk(payload)\n",
    "    deduped = []\n",
    "    seen = set()\n",
    "    for name in names:\n",
    "        n = str(name).strip()\n",
    "        if n and n not in seen:\n",
    "            seen.add(n)\n",
    "            deduped.append(n)\n",
    "    return deduped\n",
    "\n",
    "\n",
    "def fetch_committee_rosters_from_api(congress_session: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch committee rosters from congress.gov API.\n",
    "    Falls back to committee detail URLs when member lists are not inline.\n",
    "    \"\"\"\n",
    "    api_key = _get_congress_api_key()\n",
    "    if not api_key:\n",
    "        if congress_session == min(CONGRESS_SESSIONS):\n",
    "            print('  CONGRESS_API_KEY missing in environment and .env.')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    url = (f'https://api.congress.gov/v3/committee?congress={congress_session}'\n",
    "           f'&api_key={api_key}&format=json&limit=250')\n",
    "\n",
    "    results = []\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=30)\n",
    "        if resp.status_code != 200:\n",
    "            print(f'  Congress API returned HTTP {resp.status_code} for session {congress_session}.')\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        payload = resp.json()\n",
    "        committees = payload.get('committees', [])\n",
    "\n",
    "        target_committees = []\n",
    "        for cmte in committees:\n",
    "            mapped_name = _map_committee_name(cmte.get('name', ''), cmte.get('chamber', ''))\n",
    "            if mapped_name:\n",
    "                cmte_copy = dict(cmte)\n",
    "                cmte_copy['_mapped_name'] = mapped_name\n",
    "                target_committees.append(cmte_copy)\n",
    "\n",
    "        if congress_session == min(CONGRESS_SESSIONS):\n",
    "            print(f'  Committees returned: {len(committees)} total; {len(target_committees)} mapped targets')\n",
    "            if target_committees:\n",
    "                sample_mapped = [(c.get('name', 'Unknown'), c.get('_mapped_name')) for c in target_committees[:5]]\n",
    "                print(f'  Sample mapped committees: {sample_mapped}')\n",
    "            elif committees:\n",
    "                sample_names = [c.get('name', 'Unknown') for c in committees[:5]]\n",
    "                print(f'  Sample committee names: {sample_names}')\n",
    "\n",
    "        for cmte in target_committees:\n",
    "            raw_name = cmte.get('name', '')\n",
    "            cmte_name = cmte.get('_mapped_name', raw_name)\n",
    "            chamber = cmte.get('chamber', '')\n",
    "\n",
    "            member_names = _extract_member_names(cmte)\n",
    "\n",
    "            if not member_names:\n",
    "                detail_urls = []\n",
    "                for key in ('url', 'api_uri'):\n",
    "                    if cmte.get(key):\n",
    "                        detail_urls.append(cmte.get(key))\n",
    "\n",
    "                members_field = cmte.get('members')\n",
    "                if isinstance(members_field, dict):\n",
    "                    for key in ('url', 'api_uri'):\n",
    "                        if members_field.get(key):\n",
    "                            detail_urls.append(members_field.get(key))\n",
    "                elif isinstance(members_field, str):\n",
    "                    detail_urls.append(members_field)\n",
    "\n",
    "                for detail_url in detail_urls:\n",
    "                    try:\n",
    "                        dresp = requests.get(_append_api_key(detail_url, api_key), timeout=30)\n",
    "                        if dresp.status_code == 200:\n",
    "                            member_names = _extract_member_names(dresp.json())\n",
    "                            if member_names:\n",
    "                                break\n",
    "                        elif congress_session == min(CONGRESS_SESSIONS):\n",
    "                            print(f'  Detail fetch HTTP {dresp.status_code} for {raw_name}.')\n",
    "                    except Exception as detail_err:\n",
    "                        if congress_session == min(CONGRESS_SESSIONS):\n",
    "                            print(f'  Detail fetch failed for {raw_name}: {detail_err}')\n",
    "\n",
    "            for member_name in member_names:\n",
    "                results.append({\n",
    "                    'member_name': member_name,\n",
    "                    'committee_name': cmte_name,\n",
    "                    'congress_session': congress_session,\n",
    "                    'chamber': chamber,\n",
    "                })\n",
    "\n",
    "        if congress_session == min(CONGRESS_SESSIONS):\n",
    "            print(f'  Extracted assignments from API payload: {len(results):,}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'  Congress API error for session {congress_session}: {e}')\n",
    "\n",
    "    if results:\n",
    "        return pd.DataFrame(results).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    fallback_df = _get_open_data_committee_roster()\n",
    "    if not fallback_df.empty:\n",
    "        fallback_df = fallback_df.copy()\n",
    "        fallback_df['congress_session'] = congress_session\n",
    "        if congress_session == min(CONGRESS_SESSIONS):\n",
    "            print('  Congress API does not expose committee membership; using open-data fallback roster.')\n",
    "            print('  Fallback assumption: current committee assignments are projected across sessions.')\n",
    "            print(f'  Open-data fallback assignments: {len(fallback_df):,}')\n",
    "        return fallback_df\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def assign_committees_to_trades(trades_df: pd.DataFrame,\n",
    "                                roster_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Tag each trade with Is_Committee_Relevant.\n",
    "\n",
    "    If roster is available: fuzzy-match legislator names (threshold >= 85),\n",
    "    look up committee assignments at time of trade, check against SECTOR_CONFIG.\n",
    "    If roster is empty: conservative fallback — all False (logged as warning).\n",
    "    \"\"\"\n",
    "    df = trades_df.copy()\n",
    "    if 'TransactionDate' in df.columns:\n",
    "        df['Congress_Session'] = df['TransactionDate'].apply(get_congress_session)\n",
    "    else:\n",
    "        df['Congress_Session'] = 119\n",
    "\n",
    "    if roster_df.empty or 'member_name' not in roster_df.columns:\n",
    "        df['Committee_List']        = [[] for _ in range(len(df))]\n",
    "        df['Is_Committee_Relevant'] = False\n",
    "        print('WARNING: No committee roster available. All trades marked non-committee-relevant.')\n",
    "        print('         Set CONGRESS_API_KEY to enable H2 analysis.')\n",
    "        return df\n",
    "\n",
    "    # Build name lookup via fuzzy matching\n",
    "    roster_names = roster_df['member_name'].str.lower().str.strip().unique().tolist()\n",
    "    name_cache = {}\n",
    "    for name in df['Name'].str.lower().str.strip().unique():\n",
    "        if fuzz_process:\n",
    "            match, score = fuzz_process.extractOne(name, roster_names,\n",
    "                                                   scorer=fuzz.token_sort_ratio)\n",
    "            name_cache[name] = match if score >= 85 else None\n",
    "        else:\n",
    "            name_cache[name] = None\n",
    "\n",
    "    def get_committees(row):\n",
    "        matched = name_cache.get(str(row['Name']).lower().strip())\n",
    "        if matched is None:\n",
    "            return []\n",
    "        mask = (\n",
    "            (roster_df['member_name'].str.lower().str.strip() == matched) &\n",
    "            (roster_df['congress_session'] == row['Congress_Session'])\n",
    "        )\n",
    "        return roster_df.loc[mask, 'committee_name'].tolist()\n",
    "\n",
    "    def is_relevant(row):\n",
    "        sector = row.get('project_sector', 'Other')\n",
    "        if sector == 'Other' or sector not in SECTOR_CONFIG:\n",
    "            return False\n",
    "        sector_committees = set(SECTOR_CONFIG[sector]['committees'])\n",
    "        return bool(sector_committees & set(row.get('Committee_List', [])))\n",
    "\n",
    "    df['Committee_List']        = df.apply(get_committees, axis=1)\n",
    "    df['Is_Committee_Relevant'] = df.apply(is_relevant, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ─── Run Committee Mapping ───\n",
    "print('Fetching committee rosters...')\n",
    "roster_frames = []\n",
    "for session in CONGRESS_SESSIONS:\n",
    "    r = fetch_committee_rosters_from_api(session)\n",
    "    if not r.empty:\n",
    "        roster_frames.append(r)\n",
    "\n",
    "if roster_frames:\n",
    "    committee_roster = pd.concat(roster_frames, ignore_index=True)\n",
    "    committee_roster.to_parquet(PROCESSED_DIR / 'committee_roster.parquet', index=False)\n",
    "    print(f'Committee roster loaded: {len(committee_roster):,} assignments')\n",
    "else:\n",
    "    committee_roster = pd.DataFrame()\n",
    "    print('No committee roster from API — using fallback (Is_Committee_Relevant = False).')\n",
    "    print('Set CONGRESS_API_KEY env variable to enable full committee analysis.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# MODULES 4-6: Stock Returns, ETF Returns, Fama-French Factors\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# ─── Module 4: Stock Returns ───\n",
    "def fetch_stock_returns(tickers: list, start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch daily adjusted-close returns for event study.\n",
    "    Returns computed as (adj_close_t / adj_close_{t-1}) - 1.\n",
    "    Caches to data/raw/stock_returns_cache.parquet.\n",
    "    \"\"\"\n",
    "    cache_path = RAW_DIR / 'stock_returns_cache.parquet'\n",
    "\n",
    "    if cache_path.exists():\n",
    "        cached = pd.read_parquet(cache_path)\n",
    "        cached_tickers = set(cached['Ticker'].unique())\n",
    "        need_tickers = [t for t in tickers if t not in cached_tickers]\n",
    "    else:\n",
    "        cached = pd.DataFrame()\n",
    "        need_tickers = tickers\n",
    "\n",
    "    if yf is None:\n",
    "        print('yfinance not available; using cached stock returns if present.')\n",
    "        return cached\n",
    "\n",
    "    if need_tickers:\n",
    "        print(f'Fetching returns for {len(need_tickers)} new tickers...')\n",
    "        frames = []\n",
    "        batch_size = 50\n",
    "        for i in tqdm(range(0, len(need_tickers), batch_size), desc='Stock returns'):\n",
    "            batch = need_tickers[i:i + batch_size]\n",
    "            try:\n",
    "                prices = yf.download(batch, start=start, end=end,\n",
    "                                     auto_adjust=True, progress=False)['Close']\n",
    "                if isinstance(prices, pd.Series):\n",
    "                    prices = prices.to_frame(batch[0])\n",
    "                ret = prices.pct_change().dropna(how='all')\n",
    "                df_long = (\n",
    "                    ret.reset_index()\n",
    "                       .melt(id_vars='Date', var_name='Ticker', value_name='ret')\n",
    "                       .rename(columns={'Date': 'date'})\n",
    "                       .dropna(subset=['ret'])\n",
    "                )\n",
    "                frames.append(df_long)\n",
    "            except Exception as e:\n",
    "                print(f'  Batch {i}: {e}')\n",
    "\n",
    "        if frames:\n",
    "            new_data = pd.concat(frames, ignore_index=True)\n",
    "            new_data['date'] = pd.to_datetime(new_data['date'])\n",
    "            stock_returns = (\n",
    "                pd.concat([cached, new_data], ignore_index=True)\n",
    "                  .drop_duplicates(subset=['date', 'Ticker'])\n",
    "            )\n",
    "            stock_returns.to_parquet(cache_path, index=False)\n",
    "        else:\n",
    "            stock_returns = cached\n",
    "    else:\n",
    "        stock_returns = cached\n",
    "        print(f'All {len(tickers)} tickers found in cache.')\n",
    "\n",
    "    stock_returns['date'] = pd.to_datetime(stock_returns['date'])\n",
    "    return stock_returns\n",
    "\n",
    "# ─── Module 5: ETF Returns ───\n",
    "def fetch_etf_returns(etf_tickers: list, start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch daily returns for sector ETFs + SPY benchmark.\"\"\"\n",
    "    all_etfs = list(set(etf_tickers + ['SPY']))\n",
    "    print(f'Fetching ETF returns for: {all_etfs}')\n",
    "    if yf is None:\n",
    "        print('yfinance not available; using cached ETF returns if present.')\n",
    "        cache = EXTERNAL_DIR / 'etf_returns.parquet'\n",
    "        return pd.read_parquet(cache) if cache.exists() else pd.DataFrame(columns=['date','etf_ticker','ret'])\n",
    "    try:\n",
    "        prices = yf.download(all_etfs, start=start, end=end,\n",
    "                             auto_adjust=True, progress=False)['Close']\n",
    "        if isinstance(prices, pd.Series):\n",
    "            prices = prices.to_frame(all_etfs[0])\n",
    "        ret = prices.pct_change().dropna(how='all')\n",
    "        df_long = (\n",
    "            ret.reset_index()\n",
    "               .melt(id_vars='Date', var_name='etf_ticker', value_name='ret')\n",
    "               .rename(columns={'Date': 'date'})\n",
    "        )\n",
    "        df_long['date'] = pd.to_datetime(df_long['date'])\n",
    "        df_long.to_parquet(EXTERNAL_DIR / 'etf_returns.parquet', index=False)\n",
    "        print(f'ETF returns: {len(df_long):,} daily observations')\n",
    "        return df_long\n",
    "    except Exception as e:\n",
    "        print(f'ETF fetch failed: {e}')\n",
    "        return pd.DataFrame(columns=['date', 'etf_ticker', 'ret'])\n",
    "\n",
    "\n",
    "# ─── Module 6: Fama-French Factors ───\n",
    "def fetch_ff5_mom(start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download FF5 + Momentum daily factors from Kenneth French Data Library.\n",
    "    Values are in % — divided by 100 for decimal returns.\n",
    "    COLUMN NAMES standardized: Mkt-RF → Mkt_RF, Mom column normalized.\n",
    "    \"\"\"\n",
    "    ff_path = EXTERNAL_DIR / 'ff5_mom_factors.csv'\n",
    "    if ff_path.exists():\n",
    "        df = pd.read_csv(ff_path, index_col='date', parse_dates=True)\n",
    "        print(f'FF5+Mom factors loaded from cache: {len(df):,} rows')\n",
    "        return df\n",
    "\n",
    "    global web\n",
    "    if web is None:\n",
    "        try:\n",
    "            import pandas_datareader.data as web_retry\n",
    "            web = web_retry\n",
    "            print('pandas_datareader loaded at runtime.')\n",
    "        except ImportError:\n",
    "            print('pandas_datareader not installed — skipping factor download.')\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    print('Downloading Fama-French 5 + Momentum factors (daily)...')\n",
    "    try:\n",
    "        ff5 = web.DataReader('F-F_Research_Data_5_Factors_2x3_daily',\n",
    "                             'famafrench', start=start, end=end)[0]\n",
    "        mom = web.DataReader('F-F_Momentum_Factor_daily',\n",
    "                             'famafrench', start=start, end=end)[0]\n",
    "        ff5.index = pd.to_datetime(ff5.index, format='%Y%m%d')\n",
    "        mom.index = pd.to_datetime(mom.index, format='%Y%m%d')\n",
    "        df = ff5.join(mom, how='inner') / 100.0\n",
    "        df.columns = [c.strip().replace('-', '_').replace(' ', '') for c in df.columns]\n",
    "        # Normalize momentum column name\n",
    "        mom_col = next((c for c in df.columns if 'mom' in c.lower() or c == 'WML'), None)\n",
    "        if mom_col and mom_col != 'Mom':\n",
    "            df = df.rename(columns={mom_col: 'Mom'})\n",
    "        df.index.name = 'date'\n",
    "        df.to_csv(ff_path)\n",
    "        print(f'FF5+Mom factors: {len(df):,} daily observations')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'Daily factors failed ({e}), trying monthly...')\n",
    "        try:\n",
    "            ff5 = web.DataReader('F-F_Research_Data_5_Factors_2x3',\n",
    "                                 'famafrench', start=start, end=end)[0]\n",
    "            mom = web.DataReader('F-F_Momentum_Factor',\n",
    "                                 'famafrench', start=start, end=end)[0]\n",
    "            ff5.index = pd.to_datetime(ff5.index.to_timestamp())\n",
    "            mom.index = pd.to_datetime(mom.index.to_timestamp())\n",
    "            df = ff5.join(mom, how='inner') / 100.0\n",
    "            df.columns = [c.strip().replace('-', '_').replace(' ', '') for c in df.columns]\n",
    "            mom_col = next((c for c in df.columns if 'mom' in c.lower() or c == 'WML'), None)\n",
    "            if mom_col and mom_col != 'Mom':\n",
    "                df = df.rename(columns={mom_col: 'Mom'})\n",
    "            df.index.name = 'date'\n",
    "            df.to_csv(ff_path)\n",
    "            print(f'FF5+Mom monthly factors: {len(df):,} rows')\n",
    "            return df\n",
    "        except Exception as e2:\n",
    "            print(f'Factor download unavailable: {e2}')\n",
    "            return pd.DataFrame()\n",
    "\n",
    "\n",
    "# ─── Fetch all return data ───\n",
    "etf_tickers   = [cfg['target_etf'] for cfg in SECTOR_CONFIG.values()]\n",
    "etf_returns   = fetch_etf_returns(etf_tickers, ESTIMATION_WINDOW_START, SAMPLE_END)\n",
    "ff_factors    = fetch_ff5_mom(ESTIMATION_WINDOW_START, SAMPLE_END)\n",
    "\n",
    "# Market returns from SPY\n",
    "spy_daily = (\n",
    "    etf_returns[etf_returns['etf_ticker'] == 'SPY']\n",
    "    .set_index('date')['ret']\n",
    "    .rename('mkt_ret')\n",
    ")\n",
    "\n",
    "print(f'\\nData fetch complete:')\n",
    "print(f'  ETF returns: {len(etf_returns):,} rows')\n",
    "print(f'  Market (SPY) returns: {len(spy_daily):,} days')\n",
    "print(f'  FF5+Mom factors: {len(ff_factors):,} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# MODULE 7: Enrichment Pipeline — Master Join\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def run_enrichment_pipeline(df_raw: pd.DataFrame,\n",
    "                            ticker_sector_map: pd.DataFrame,\n",
    "                            committee_roster: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Master join: trades → GICS sectors → project sectors → committee flags → ETF mapping.\n",
    "    Validates final schema and logs counts to console.\n",
    "    Saves to data/processed/trades_enriched.parquet.\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "    n_raw = len(df)\n",
    "\n",
    "    # Step 1: Join sector mapping\n",
    "    df = df.merge(\n",
    "        ticker_sector_map[['Ticker', 'gics_sector', 'gics_industry', 'project_sector']],\n",
    "        on='Ticker', how='left'\n",
    "    )\n",
    "    df['project_sector'] = df['project_sector'].fillna('Other')\n",
    "    n_sector_matched = (df['project_sector'] != 'Other').sum()\n",
    "\n",
    "    # Step 2: Add target_etf\n",
    "    sector_to_etf = {k: v['target_etf'] for k, v in SECTOR_CONFIG.items()}\n",
    "    df['target_etf'] = df['project_sector'].map(sector_to_etf)\n",
    "\n",
    "    # Step 3: Assign committee memberships\n",
    "    df = assign_committees_to_trades(df, committee_roster)\n",
    "    n_committee_relevant = df['Is_Committee_Relevant'].sum()\n",
    "\n",
    "    # Step 4: Ensure all required columns exist\n",
    "    required = [\n",
    "        'ReportDate', 'TransactionDate', 'Ticker', 'Name',\n",
    "        'Transaction', 'Direction', 'Amount', 'AmountMidpoint',\n",
    "        'DisclosureLag', 'Chamber', 'gics_sector', 'gics_industry',\n",
    "        'project_sector', 'Committee_List', 'Is_Committee_Relevant', 'target_etf',\n",
    "    ]\n",
    "    for col in required:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan if col not in ['Committee_List'] else [[] for _ in range(len(df))]\n",
    "\n",
    "    df.to_parquet(PROCESSED_DIR / 'trades_enriched.parquet', index=False)\n",
    "\n",
    "    # Validation report\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('ENRICHMENT VALIDATION REPORT')\n",
    "    print('=' * 60)\n",
    "    print(f'  Total raw trades:              {n_raw:>10,}')\n",
    "    print(f'  Matched to project sector:     {n_sector_matched:>10,} ({n_sector_matched/n_raw*100:.1f}%)')\n",
    "    print(f'  \"Other\" sector (excluded):     {(df[\"project_sector\"]==\"Other\").sum():>10,}')\n",
    "    print(f'  Committee-relevant trades:     {n_committee_relevant:>10,} ({n_committee_relevant/n_raw*100:.1f}%)')\n",
    "    print(f'  Final enriched dataset rows:   {len(df):>10,}')\n",
    "    print('=' * 60)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_enriched = run_enrichment_pipeline(df_raw, ticker_sector_map, committee_roster)\n",
    "df_enriched.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-08",
   "metadata": {},
   "source": [
    "---\n",
    "### Data Exploration & Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Congressional Trading Data — Exploratory Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Monthly trade disclosures over time\n",
    "ax = axes[0, 0]\n",
    "monthly_trades = df_enriched.set_index('ReportDate').resample('ME').size()\n",
    "ax.plot(monthly_trades.index, monthly_trades.values, linewidth=1.5, color='steelblue')\n",
    "ax.set_title('Monthly Trade Disclosures Over Time')\n",
    "ax.set_xlabel('Report Date')\n",
    "ax.set_ylabel('Number of Trades')\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "# 2. Purchase vs. Sale\n",
    "ax = axes[0, 1]\n",
    "dir_counts = df_enriched['Transaction'].value_counts()\n",
    "bars = ax.bar(dir_counts.index, dir_counts.values,\n",
    "              color=['steelblue', 'coral'], edgecolor='white')\n",
    "ax.set_title('Purchase vs. Sale Distribution')\n",
    "ax.set_ylabel('Count')\n",
    "for bar, val in zip(bars, dir_counts.values):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2., bar.get_height() * 1.02,\n",
    "            f'{val:,}', ha='center', fontsize=10)\n",
    "\n",
    "# 3. Project sector breakdown\n",
    "ax = axes[0, 2]\n",
    "sector_counts = df_enriched['project_sector'].value_counts()\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(sector_counts)))\n",
    "ax.barh(sector_counts.index, sector_counts.values, color=colors)\n",
    "ax.set_title('Trades by Project Sector')\n",
    "ax.set_xlabel('Count')\n",
    "\n",
    "# 4. Disclosure lag\n",
    "ax = axes[1, 0]\n",
    "lags = df_enriched['DisclosureLag'].dropna().clip(0, 120)\n",
    "ax.hist(lags, bins=60, color='steelblue', alpha=0.7, edgecolor='white')\n",
    "ax.axvline(45, color='red', linestyle='--', label='45-day STOCK Act deadline')\n",
    "ax.set_title('Disclosure Lag Distribution')\n",
    "ax.set_xlabel('Calendar Days (ReportDate − TransactionDate)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "\n",
    "# 5. Amount distribution\n",
    "ax = axes[1, 1]\n",
    "amounts = df_enriched['AmountMidpoint'].dropna()\n",
    "amounts_pos = amounts[amounts > 0]\n",
    "ax.hist(np.log10(amounts_pos), bins=50, color='mediumpurple', alpha=0.7, edgecolor='white')\n",
    "ax.set_title('Trade Amount Distribution (log₁₀ scale)')\n",
    "ax.set_xlabel('log₁₀(AmountMidpoint USD)')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "# 6. Committee relevance pie\n",
    "ax = axes[1, 2]\n",
    "comm_counts = (\n",
    "    df_enriched['Is_Committee_Relevant']\n",
    "    .fillna(False)\n",
    "    .astype(bool)\n",
    "    .value_counts()\n",
    "    .reindex([False, True], fill_value=0)\n",
    ")\n",
    "labels = ['Non-Relevant', 'Committee-Relevant']\n",
    "if comm_counts.sum() == 0:\n",
    "    ax.text(0.5, 0.5, 'No committee data', ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "else:\n",
    "    ax.pie(comm_counts.values, labels=labels, colors=['lightcoral', 'steelblue'],\n",
    "           autopct='%1.1f%%', startangle=90)\n",
    "ax.set_title('Committee Relevance of Trades')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('DATA QUALITY SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'Unique legislators: {df_enriched[\"Name\"].nunique():,}')\n",
    "print(f'Unique tickers:     {df_enriched[\"Ticker\"].nunique():,}')\n",
    "print(f'Date range:         {df_enriched[\"ReportDate\"].min().date()} → {df_enriched[\"ReportDate\"].max().date()}')\n",
    "if 'DisclosureLag' in df_enriched.columns:\n",
    "    print(f'Median lag (days):  {df_enriched[\"DisclosureLag\"].median():.0f}')\n",
    "    print(f'Lag > 45 days:      {(df_enriched[\"DisclosureLag\"] > 45).sum():,} '\n",
    "          f'({(df_enriched[\"DisclosureLag\"] > 45).mean()*100:.1f}%)')\n",
    "print(f'Purchases:          {(df_enriched[\"Direction\"] == 1).sum():,}')\n",
    "print(f'Sales:              {(df_enriched[\"Direction\"] == -1).sum():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: H1 — Market Underreaction Event Study\n",
    "\n",
    "**Hypothesis:** If `CAR[-1,+1] ≈ 0` but `CAR[+2,+20] > 0` for purchases after disclosure, the market underreacts to congressional trade disclosures.\n",
    "\n",
    "**Event date = `ReportDate` (disclosure date) — NEVER `TransactionDate`.**\n",
    "\n",
    "**Definition of Done:**\n",
    "- CAR[-1,+1] and CAR[+2,+20] computed for all disclosure events\n",
    "- Cross-sectional t-test with buy/sell separation and Patell standardized test\n",
    "- Clear accept/reject conclusion for H1\n",
    "- Robustness: winsorized, lag ≤ 45 days, subperiod splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# MODULE 8: Market Model Estimation\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Fetch stock returns for all unique tickers in the trade dataset\n",
    "print('Fetching stock returns for event study...')\n",
    "unique_tickers = df_enriched['Ticker'].dropna().unique().tolist()\n",
    "stock_returns_df = fetch_stock_returns(unique_tickers, ESTIMATION_WINDOW_START, SAMPLE_END)\n",
    "\n",
    "# Build wide returns matrix (date × ticker) for fast indexing\n",
    "stock_rets = stock_returns_df.pivot(index='date', columns='Ticker', values='ret')\n",
    "stock_rets.index = pd.to_datetime(stock_rets.index)\n",
    "stock_rets = stock_rets.sort_index()\n",
    "\n",
    "print(f'Return matrix: {stock_rets.shape[0]:,} dates × {stock_rets.shape[1]:,} tickers')\n",
    "\n",
    "\n",
    "def fit_market_model(event_date: pd.Timestamp,\n",
    "                    ticker: str,\n",
    "                    stock_rets_matrix: pd.DataFrame,\n",
    "                    mkt_returns: pd.Series,\n",
    "                    estimation_window: tuple = (-250, -30),\n",
    "                    min_obs: int = 120) -> dict:\n",
    "    if sm is None:\n",
    "        return None\n",
    "    \"\"\"\n",
    "    OLS: stock_ret_t = alpha + beta * mkt_ret_t + epsilon\n",
    "    Estimation window in TRADING DAYS relative to event_date.\n",
    "    Returns None if insufficient data.\n",
    "    \"\"\"\n",
    "    if ticker not in stock_rets_matrix.columns:\n",
    "        return None\n",
    "\n",
    "    all_dates = list(stock_rets_matrix.index)\n",
    "    dates_before = [d for d in all_dates if d <= event_date]\n",
    "    if not dates_before:\n",
    "        return None\n",
    "\n",
    "    event_idx = all_dates.index(dates_before[-1])\n",
    "    start_idx = max(0, event_idx + estimation_window[0])\n",
    "    end_idx   = max(0, event_idx + estimation_window[1])\n",
    "\n",
    "    if end_idx <= start_idx:\n",
    "        return None\n",
    "\n",
    "    est_dates  = stock_rets_matrix.index[start_idx:end_idx]\n",
    "    stock_r    = stock_rets_matrix.loc[est_dates, ticker].dropna()\n",
    "    market_r   = mkt_returns.reindex(est_dates).dropna()\n",
    "    common     = stock_r.index.intersection(market_r.index)\n",
    "\n",
    "    if len(common) < min_obs:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        y = stock_r.loc[common].values\n",
    "        X = sm.add_constant(market_r.loc[common].values)\n",
    "        res = sm.OLS(y, X).fit()\n",
    "        return {\n",
    "            'alpha':     res.params[0],\n",
    "            'beta':      res.params[1],\n",
    "            'sigma':     res.resid.std(),\n",
    "            'n_obs':     len(common),\n",
    "            'r_squared': res.rsquared,\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ─── Aggregate to disclosure-level events ───\n",
    "# CRITICAL: Event date = ReportDate (disclosure date)\n",
    "events = (\n",
    "    df_enriched\n",
    "    .groupby(['ReportDate', 'Ticker', 'Direction'])\n",
    "    .agg(TotalAmount=('AmountMidpoint', 'sum'), n_trades=('Direction', 'count'))\n",
    "    .reset_index()\n",
    ")\n",
    "print(f'\\nTotal disclosure events: {len(events):,}')\n",
    "print(f'  Purchases: {(events[\"Direction\"] == 1).sum():,}')\n",
    "print(f'  Sales:     {(events[\"Direction\"] == -1).sum():,}')\n",
    "\n",
    "# ─── Fit market models ───\n",
    "print('\\nFitting market models (may take several minutes)...')\n",
    "market_models = {}\n",
    "failed = 0\n",
    "\n",
    "for _, row in tqdm(events.iterrows(), total=len(events), desc='Market Models'):\n",
    "    key = (pd.Timestamp(row['ReportDate']), row['Ticker'])\n",
    "    result = fit_market_model(\n",
    "        event_date=key[0], ticker=key[1],\n",
    "        stock_rets_matrix=stock_rets,\n",
    "        mkt_returns=spy_daily,\n",
    "        estimation_window=ESTIMATION_WINDOW,\n",
    "        min_obs=MIN_ESTIMATION_DAYS,\n",
    "    )\n",
    "    if result is not None:\n",
    "        market_models[key] = result\n",
    "    else:\n",
    "        failed += 1\n",
    "\n",
    "print(f'\\nMarket models fitted:   {len(market_models):,}')\n",
    "print(f'Events excluded:        {failed:,} ({failed/len(events)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# MODULE 9: Abnormal Returns & CAR Computation\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def compute_car(event_date: pd.Timestamp, ticker: str,\n",
    "                mm: dict,\n",
    "                stock_rets_matrix: pd.DataFrame,\n",
    "                mkt_returns: pd.Series,\n",
    "                window: tuple) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute CAR[window[0], window[1]] in trading days relative to event_date.\n",
    "    AR_t = actual_ret_t − (alpha + beta × mkt_ret_t)\n",
    "\n",
    "    Returns (car, n_days). If stock is delisted during window, CAR is truncated\n",
    "    (NOT forward-filled). Returns (nan, 0) on failure.\n",
    "    \"\"\"\n",
    "    if ticker not in stock_rets_matrix.columns:\n",
    "        return np.nan, 0\n",
    "\n",
    "    try:\n",
    "        all_dates = list(stock_rets_matrix.index)\n",
    "        dates_before = [d for d in all_dates if d <= event_date]\n",
    "        if not dates_before:\n",
    "            return np.nan, 0\n",
    "\n",
    "        event_idx = all_dates.index(dates_before[-1])\n",
    "        start_idx = max(0, event_idx + window[0])\n",
    "        end_idx   = min(len(all_dates) - 1, event_idx + window[1])\n",
    "\n",
    "        if start_idx > end_idx:\n",
    "            return np.nan, 0\n",
    "\n",
    "        win_dates = stock_rets_matrix.index[start_idx:end_idx + 1]\n",
    "        stock_r   = stock_rets_matrix.loc[win_dates, ticker].dropna()\n",
    "        market_r  = mkt_returns.reindex(win_dates).dropna()\n",
    "        common    = stock_r.index.intersection(market_r.index)\n",
    "\n",
    "        if len(common) == 0:\n",
    "            return np.nan, 0\n",
    "\n",
    "        ar  = stock_r.loc[common] - (mm['alpha'] + mm['beta'] * market_r.loc[common])\n",
    "        return ar.sum(), len(common)\n",
    "    except Exception:\n",
    "        return np.nan, 0\n",
    "\n",
    "\n",
    "# ─── Compute CARs for all fitted events ───\n",
    "print('Computing CARs...')\n",
    "car_rows = []\n",
    "\n",
    "for _, row in tqdm(events.iterrows(), total=len(events), desc='CAR Computation'):\n",
    "    key = (pd.Timestamp(row['ReportDate']), row['Ticker'])\n",
    "    if key not in market_models:\n",
    "        continue\n",
    "\n",
    "    mm = market_models[key]\n",
    "    car_imm,   n_imm   = compute_car(key[0], key[1], mm, stock_rets, spy_daily, EVENT_WINDOW_IMMEDIATE)\n",
    "    car_drift, n_drift = compute_car(key[0], key[1], mm, stock_rets, spy_daily, EVENT_WINDOW_DRIFT)\n",
    "    ar_day0,   _       = compute_car(key[0], key[1], mm, stock_rets, spy_daily, (0, 0))\n",
    "\n",
    "    car_rows.append({\n",
    "        'ReportDate':      row['ReportDate'],\n",
    "        'Ticker':          row['Ticker'],\n",
    "        'Direction':       row['Direction'],\n",
    "        'Transaction':     'Purchase' if row['Direction'] == 1 else 'Sale',\n",
    "        'TotalAmount':     row['TotalAmount'],\n",
    "        'mm_alpha':        mm['alpha'],\n",
    "        'mm_beta':         mm['beta'],\n",
    "        'mm_sigma':        mm['sigma'],\n",
    "        'mm_n_obs':        mm['n_obs'],\n",
    "        'mm_r2':           mm['r_squared'],\n",
    "        'CAR_immediate':   car_imm,\n",
    "        'CAR_drift':       car_drift,\n",
    "        'AR_day0':         ar_day0,\n",
    "        'n_days_immediate':n_imm,\n",
    "        'n_days_drift':    n_drift,\n",
    "    })\n",
    "\n",
    "df_cars = pd.DataFrame(car_rows).dropna(subset=['CAR_immediate', 'CAR_drift'])\n",
    "print(f'\\nEvents with valid CARs: {len(df_cars):,}')\n",
    "print('\\nCAR summary statistics:')\n",
    "print(df_cars[['CAR_immediate', 'CAR_drift', 'AR_day0']].describe().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# MODULE 10: H1 Statistical Tests\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def run_h1_tests(cars_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Cross-sectional t-tests: H0: mean CAR = 0 for each (direction, window) combination.\n",
    "    Also computes simplified Patell standardized test.\n",
    "    Reports: t-stat, p-value, n, mean_car, pct_positive.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for direction, label in [(1, 'buys'), (-1, 'sells')]:\n",
    "        subset = cars_df[cars_df['Direction'] == direction].copy()\n",
    "        for window_name, col, n_days_col in [\n",
    "            ('immediate', 'CAR_immediate', 'n_days_immediate'),\n",
    "            ('drift',     'CAR_drift',     'n_days_drift'),\n",
    "        ]:\n",
    "            cars = subset[col].dropna()\n",
    "            key  = f'{label}_{window_name}'\n",
    "            n    = len(cars)\n",
    "            if n < 10:\n",
    "                results[key] = {'mean_car': np.nan, 't_stat': np.nan,\n",
    "                                'p_value': np.nan, 'n': n,\n",
    "                                'pct_positive': np.nan, 't_patell': np.nan}\n",
    "                continue\n",
    "\n",
    "            t_stat, p_value = stats.ttest_1samp(cars, 0)\n",
    "\n",
    "            # Simplified Patell (1976) standardized test\n",
    "            sigma = subset.loc[cars.index, 'mm_sigma']\n",
    "            n_obs = subset.loc[cars.index, 'mm_n_obs']\n",
    "            n_win = subset.loc[cars.index, n_days_col].clip(lower=1)\n",
    "            std_cars = cars / (sigma * np.sqrt(n_win) + 1e-12)\n",
    "            t_patell = std_cars.mean() * np.sqrt(n) if n > 0 else np.nan\n",
    "\n",
    "            results[key] = {\n",
    "                'mean_car':    cars.mean(),\n",
    "                'median_car':  cars.median(),\n",
    "                't_stat':      t_stat,\n",
    "                'p_value':     p_value,\n",
    "                't_patell':    t_patell,\n",
    "                'n':           n,\n",
    "                'pct_positive':  (cars > 0).mean(),\n",
    "            }\n",
    "    return results\n",
    "\n",
    "\n",
    "h1_results = run_h1_tests(df_cars)\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('H1 EVENT STUDY — Market Underreaction to Congressional Trade Disclosures')\n",
    "print('=' * 80)\n",
    "\n",
    "for key, res in h1_results.items():\n",
    "    label, window = key.split('_', 1)\n",
    "    win_str = 'CAR[-1,+1]' if window == 'immediate' else 'CAR[+2,+20]'\n",
    "    stars = ''\n",
    "    if not np.isnan(res.get('p_value', np.nan)):\n",
    "        p = res['p_value']\n",
    "        stars = '***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.10 else ''\n",
    "\n",
    "    print(f'\\n{label.upper()} — {win_str}:')\n",
    "    print(f'  Mean CAR:    {res[\"mean_car\"]*100 if not np.isnan(res.get(\"mean_car\",np.nan)) else \"N/A\":>8} %{stars}')\n",
    "    print(f'  t-statistic: {res.get(\"t_stat\",np.nan):>8.3f}')\n",
    "    print(f'  p-value:     {res.get(\"p_value\",np.nan):>8.4f}')\n",
    "    print(f'  Patell t:    {res.get(\"t_patell\",np.nan):>8.3f}')\n",
    "    print(f'  N events:    {res.get(\"n\",0):>8,}')\n",
    "    print(f'  % Positive:  {res.get(\"pct_positive\",np.nan)*100 if not np.isnan(res.get(\"pct_positive\",np.nan)) else \"N/A\":>7.1f} %')\n",
    "\n",
    "# Interpretation\n",
    "print('\\n' + '=' * 80)\n",
    "print('H1 INTERPRETATION')\n",
    "print('=' * 80)\n",
    "bi_p = h1_results.get('buys_immediate', {}).get('p_value', 1)\n",
    "bd_p = h1_results.get('buys_drift',     {}).get('p_value', 1)\n",
    "bd_m = h1_results.get('buys_drift',     {}).get('mean_car', 0)\n",
    "\n",
    "if not np.isnan(bd_p):\n",
    "    if bd_p < 0.05 and bd_m > 0 and (np.isnan(bi_p) or bi_p > 0.10):\n",
    "        print('RESULT: UNDERREACTION CONFIRMED — CAR[-1,+1] ≈ 0, CAR[+2,+20] > 0 (p < 0.05)')\n",
    "        print('        Market initially ignores disclosures, then gradually incorporates information.')\n",
    "    elif bd_p < 0.05 and bd_m > 0:\n",
    "        print('RESULT: DRIFT SIGNIFICANT — but market also reacts at disclosure (both windows active)')\n",
    "    elif bd_p > 0.10:\n",
    "        print('RESULT: NO EXPLOITABLE SIGNAL — CAR[+2,+20] not statistically significant')\n",
    "    else:\n",
    "        print('RESULT: NEGATIVE DRIFT — unexpected direction; inspect data quality')\n",
    "else:\n",
    "    print('RESULT: Insufficient data for H1 determination.')\n",
    "\n",
    "# ─── Visualization ───\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('H1 Event Study: CAR Around Disclosure Date (Event = ReportDate)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "for ax, (direction, label, color) in zip(\n",
    "    axes, [(1, 'Purchases', 'steelblue'), (-1, 'Sales', 'coral')]\n",
    "):\n",
    "    subset = df_cars[df_cars['Direction'] == direction]\n",
    "    means  = [subset['CAR_immediate'].mean(), subset['CAR_drift'].mean()]\n",
    "    sems   = [stats.sem(subset['CAR_immediate'].dropna()),\n",
    "              stats.sem(subset['CAR_drift'].dropna())]\n",
    "    x_labels = ['CAR[-1,+1]\\n(Immediate)', 'CAR[+2,+20]\\n(Drift)']\n",
    "\n",
    "    bars = ax.bar(x_labels, means, yerr=[1.96 * s for s in sems],\n",
    "                  color=color, alpha=0.7, capsize=10, edgecolor='white', linewidth=1.5)\n",
    "    ax.axhline(0, color='black', linewidth=0.8)\n",
    "    ax.set_title(f'{label} (N={len(subset):,})')\n",
    "    ax.set_ylabel('Cumulative Abnormal Return')\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1%}'))\n",
    "    # Annotate bars\n",
    "    for bar, val in zip(bars, means):\n",
    "        if not np.isnan(val):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., val + 0.001 * np.sign(val),\n",
    "                    f'{val:.2%}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# H1 ROBUSTNESS CHECKS\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def run_robustness(cars_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Robustness: winsorized, lag filter, subperiod splits.\"\"\"\n",
    "    p1, p99 = cars_df['CAR_drift'].quantile([0.01, 0.99])\n",
    "    cars_wins = cars_df.copy()\n",
    "    cars_wins['CAR_drift'] = cars_wins['CAR_drift'].clip(p1, p99)\n",
    "\n",
    "    # Lag data — merge DisclosureLag if not present\n",
    "    if 'DisclosureLag' not in cars_df.columns:\n",
    "        lag_data = df_enriched[['ReportDate', 'Ticker', 'DisclosureLag']].drop_duplicates()\n",
    "        cars_with_lag = cars_df.merge(lag_data, on=['ReportDate', 'Ticker'], how='left')\n",
    "    else:\n",
    "        cars_with_lag = cars_df.copy()\n",
    "\n",
    "    rd = pd.to_datetime(cars_df['ReportDate'])\n",
    "    midpoint = rd.median()\n",
    "\n",
    "    subsets = [\n",
    "        ('Full Sample',        cars_df),\n",
    "        ('Winsorized 1/99',    cars_wins),\n",
    "        ('Lag ≤ 45 days',      cars_with_lag[cars_with_lag.get('DisclosureLag', pd.Series(dtype=float)).le(45)]\n",
    "                               if 'DisclosureLag' in cars_with_lag.columns else cars_df),\n",
    "        (f'Pre-{midpoint.year}',  cars_df[rd < midpoint]),\n",
    "        (f'Post-{midpoint.year}', cars_df[rd >= midpoint]),\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "    for label, subset in subsets:\n",
    "        buys = subset[subset['Direction'] == 1]['CAR_drift'].dropna()\n",
    "        if len(buys) < 10:\n",
    "            results[label] = {'n': len(buys), 'mean': np.nan, 't': np.nan, 'p': np.nan}\n",
    "            continue\n",
    "        t, p = stats.ttest_1samp(buys, 0)\n",
    "        results[label] = {'n': len(buys), 'mean': buys.mean(), 't': t, 'p': p}\n",
    "    return results\n",
    "\n",
    "\n",
    "robustness = run_robustness(df_cars)\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('H1 ROBUSTNESS — CAR[+2,+20] for Purchases')\n",
    "print('=' * 70)\n",
    "print(f'{\"Specification\":<25} {\"N\":>8} {\"Mean CAR\":>10} {\"t-stat\":>8} {\"p-value\":>12}')\n",
    "print('-' * 70)\n",
    "\n",
    "for label, res in robustness.items():\n",
    "    if np.isnan(res.get('mean', np.nan)):\n",
    "        print(f'{label:<25} {res[\"n\"]:>8,}  {\"N/A\":>10}')\n",
    "        continue\n",
    "    stars = '***' if res['p'] < 0.01 else '**' if res['p'] < 0.05 else '*' if res['p'] < 0.10 else ''\n",
    "    print(f'{label:<25} {res[\"n\"]:>8,} {res[\"mean\"]*100:>9.2f}% {res[\"t\"]:>8.3f} {res[\"p\"]:>8.4f}{stars}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: H2 — Committee Information Advantage\n",
    "\n",
    "**Hypothesis:** Legislators trading in sectors relevant to their committee assignments generate higher post-disclosure CARs than other trades.\n",
    "\n",
    "**Two required comparisons:**\n",
    "- **Comparison A (Within-member):** Same legislator, committee-relevant vs. non-relevant trades\n",
    "- **Comparison B (Cross-member):** Same sector, committee members vs. non-members\n",
    "\n",
    "**H2 outcome determines committee weighting in signal engine.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# MODULE 11: H2 — Committee Information Advantage\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Join committee relevance flags to CAR dataset\n",
    "enrich_cols = ['ReportDate', 'Ticker', 'Is_Committee_Relevant', 'project_sector', 'Name']\n",
    "df_cars_enriched = df_cars.merge(\n",
    "    df_enriched[enrich_cols].drop_duplicates(subset=['ReportDate', 'Ticker']),\n",
    "    on=['ReportDate', 'Ticker'], how='left'\n",
    ")\n",
    "\n",
    "\n",
    "def run_h2_cross_member(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Comparison B: For each sector, committee members vs. non-members.\n",
    "    Test: difference in mean CAR_drift.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for sector in SECTOR_CONFIG:\n",
    "        sdf = df[df['project_sector'] == sector]\n",
    "        cmte     = sdf[sdf['Is_Committee_Relevant'] == True]['CAR_drift'].dropna()\n",
    "        non_cmte = sdf[sdf['Is_Committee_Relevant'] == False]['CAR_drift'].dropna()\n",
    "\n",
    "        if len(cmte) < 5 or len(non_cmte) < 5:\n",
    "            results[sector] = {\n",
    "                'n_committee': len(cmte), 'n_non': len(non_cmte),\n",
    "                'mean_committee': np.nan, 'mean_non': np.nan,\n",
    "                'diff': np.nan, 't': np.nan, 'p': np.nan,\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        t, p = stats.ttest_ind(cmte, non_cmte)\n",
    "        results[sector] = {\n",
    "            'n_committee':   len(cmte),\n",
    "            'n_non':         len(non_cmte),\n",
    "            'mean_committee':cmte.mean(),\n",
    "            'mean_non':      non_cmte.mean(),\n",
    "            'diff':          cmte.mean() - non_cmte.mean(),\n",
    "            't':             t,\n",
    "            'p':             p,\n",
    "        }\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_h2_within_member(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Comparison A: Within-member — legislators who trade both relevant and non-relevant.\n",
    "    Compares mean CAR_drift for relevant vs. non-relevant trades per member.\n",
    "    \"\"\"\n",
    "    name_col = 'Name_x' if 'Name_x' in df.columns else 'Name'\n",
    "    members_with_both = (\n",
    "        df.groupby(name_col)['Is_Committee_Relevant']\n",
    "          .nunique()\n",
    "          .pipe(lambda s: s[s > 1].index)\n",
    "    )\n",
    "    df_both = df[df[name_col].isin(members_with_both)]\n",
    "\n",
    "    relevant     = df_both[df_both['Is_Committee_Relevant'] == True]['CAR_drift'].dropna()\n",
    "    non_relevant = df_both[df_both['Is_Committee_Relevant'] == False]['CAR_drift'].dropna()\n",
    "\n",
    "    if len(relevant) < 5 or len(non_relevant) < 5:\n",
    "        return {'n_relevant': len(relevant), 'n_non': len(non_relevant),\n",
    "                'diff': np.nan, 't': np.nan, 'p': np.nan}\n",
    "\n",
    "    t, p = stats.ttest_ind(relevant, non_relevant)\n",
    "    return {\n",
    "        'n_relevant':   len(relevant),\n",
    "        'n_non':        len(non_relevant),\n",
    "        'n_legislators':len(members_with_both),\n",
    "        'mean_relevant':relevant.mean(),\n",
    "        'mean_non':     non_relevant.mean(),\n",
    "        'diff':         relevant.mean() - non_relevant.mean(),\n",
    "        't':            t,\n",
    "        'p':            p,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_h2_regression(df: pd.DataFrame):\n",
    "    if sm is None:\n",
    "        print('statsmodels unavailable; skipping H2 regression.')\n",
    "        return None\n",
    "    \"\"\"\n",
    "    OLS: CAR_drift = alpha + beta*Is_Committee_Relevant + controls\n",
    "    Cluster SE by legislator.\n",
    "    \"\"\"\n",
    "    name_col = 'Name_x' if 'Name_x' in df.columns else 'Name'\n",
    "    df_reg = df.dropna(subset=['CAR_drift', 'Is_Committee_Relevant']).copy()\n",
    "    df_reg['comm_int']  = df_reg['Is_Committee_Relevant'].astype(int)\n",
    "    df_reg['buy']       = (df_reg['Direction'] == 1).astype(int)\n",
    "    df_reg['log_amount']= np.log1p(df_reg.get('TotalAmount', pd.Series(1, index=df_reg.index)))\n",
    "\n",
    "    if len(df_reg) < 20:\n",
    "        print('Insufficient data for H2 regression.')\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        X = sm.add_constant(df_reg[['comm_int', 'buy', 'log_amount']])\n",
    "        y = df_reg['CAR_drift']\n",
    "        groups = df_reg[name_col] if name_col in df_reg.columns else df_reg.index\n",
    "        model = sm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': groups})\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f'H2 regression failed: {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "# ─── Run H2 ───\n",
    "h2_within = run_h2_within_member(df_cars_enriched)\n",
    "h2_cross  = run_h2_cross_member(df_cars_enriched)\n",
    "h2_reg    = run_h2_regression(df_cars_enriched)\n",
    "\n",
    "# Within-member\n",
    "print('\\n' + '=' * 70)\n",
    "print('H2: WITHIN-MEMBER COMPARISON')\n",
    "print('=' * 70)\n",
    "if not np.isnan(h2_within.get('diff', np.nan)):\n",
    "    print(f\"Legislators trading in both relevant and non-relevant sectors: {h2_within.get('n_legislators', 'N/A')}\")\n",
    "    print(f\"Mean CAR_drift (Committee-Relevant): {h2_within['mean_relevant']*100:.3f}%\")\n",
    "    print(f\"Mean CAR_drift (Non-Relevant):       {h2_within['mean_non']*100:.3f}%\")\n",
    "    print(f\"Difference:                          {h2_within['diff']*100:.3f}%\")\n",
    "    print(f\"t-stat: {h2_within['t']:.3f} | p-value: {h2_within['p']:.4f}\")\n",
    "else:\n",
    "    print('Insufficient within-member comparisons (requires legislators trading in both categories).')\n",
    "\n",
    "# Cross-member\n",
    "print('\\n' + '=' * 80)\n",
    "print('H2: CROSS-MEMBER COMPARISON BY SECTOR')\n",
    "print('=' * 80)\n",
    "print(f'{\"Sector\":<30} {\"N(Cmte)\":>8} {\"N(Non)\":>8} {\"Δ CAR\":>10} {\"t\":>8} {\"p\":>10}')\n",
    "print('-' * 80)\n",
    "for sector, res in h2_cross.items():\n",
    "    if np.isnan(res.get('diff', np.nan)):\n",
    "        print(f'{sector:<30} {res[\"n_committee\"]:>8} {res[\"n_non\"]:>8}  {\"N/A\":>10}')\n",
    "        continue\n",
    "    stars = '***' if res['p'] < 0.01 else '**' if res['p'] < 0.05 else '*' if res['p'] < 0.10 else ''\n",
    "    print(f'{sector:<30} {res[\"n_committee\"]:>8} {res[\"n_non\"]:>8} '\n",
    "          f'{res[\"diff\"]*100:>9.2f}%{stars} {res[\"t\"]:>8.3f} {res[\"p\"]:>10.4f}')\n",
    "\n",
    "# Regression\n",
    "if h2_reg is not None:\n",
    "    print('\\n' + '=' * 60)\n",
    "    print('H2 REGRESSION RESULTS (Clustered SE by Legislator)')\n",
    "    print('=' * 60)\n",
    "    coef  = h2_reg.params.get('comm_int', np.nan)\n",
    "    pval  = h2_reg.pvalues.get('comm_int', np.nan)\n",
    "    print(h2_reg.summary2().tables[1].round(4))\n",
    "    print('\\nINTERPRETATION:')\n",
    "    if not np.isnan(pval):\n",
    "        if pval < 0.05 and coef > 0:\n",
    "            print('H2 CONFIRMED: Committee-relevant trades significantly outperform (p < 0.05)')\n",
    "            print('DECISION: Apply committee filter in signal engine (already enforced in Module 12)')\n",
    "        else:\n",
    "            print('H2 NOT CONFIRMED: No significant committee information advantage')\n",
    "            print('DECISION: Committee filter retained for risk management; equal weighting applied')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4: Signal Engine\n",
    "\n",
    "**Refactored from prior code — key fixes:**\n",
    "1. **Bug fix:** Committee filter actually applied (`Is_Committee_Relevant == True` only)\n",
    "2. **Bug fix:** `AmountMidpoint` used instead of raw `Amount` for volume calculations\n",
    "3. **Enhancement:** `current_date` parameter enables zero-lookahead historical backtesting\n",
    "4. **Enhancement:** `conviction_score` returned as continuous signal in `[-1, 1]`\n",
    "5. **Fix:** `conviction_thresh` parameter not hardcoded — read from caller\n",
    "\n",
    "**Signal logic:** For each sector, aggregate committee-relevant trades in `lookback_days` window ending at `current_date` (using `ReportDate`). Score = `net_volume / total_volume`. Gates: diversity (min members, min tickers) + volume (min transactions). Score ≥ threshold → BUY; ≤ −threshold → SELL; else FLAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# MODULE 12: Signal Generator\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def generate_signals(\n",
    "    df_enriched: pd.DataFrame,\n",
    "    config: dict = None,\n",
    "    lookback_days: int = LOOKBACK_DAYS,\n",
    "    conviction_thresh: float = CONVICTION_THRESHOLD,\n",
    "    current_date=None,\n",
    "    committee_filter: bool = True,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Generate sector-level BUY/SELL/FLAT signals.\n",
    "\n",
    "    CRITICAL: Uses ReportDate only — NO LOOKAHEAD from TransactionDate (ASSUMPTION A5).\n",
    "    BUG FIX: committee_filter=True restricts to Is_Committee_Relevant trades only.\n",
    "    BUG FIX: Uses AmountMidpoint (not raw Amount) for volume weighting (ASSUMPTION A1).\n",
    "    ENHANCEMENT: conviction_score is continuous in [-1,1] for portfolio construction.\n",
    "    ENHANCEMENT: current_date enables zero-lookahead historical simulation.\n",
    "\n",
    "    Returns:\n",
    "        (sector_summary_df, detailed_trades_df)\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = SECTOR_CONFIG\n",
    "\n",
    "    if current_date is None:\n",
    "        current_date = df_enriched['ReportDate'].max()\n",
    "    current_date   = pd.Timestamp(current_date)\n",
    "    lookback_start = current_date - pd.Timedelta(days=lookback_days)\n",
    "\n",
    "    # CRITICAL: Filter by ReportDate only — window ends at current_date\n",
    "    window_df = df_enriched[\n",
    "        (df_enriched['ReportDate'] >= lookback_start) &\n",
    "        (df_enriched['ReportDate'] <= current_date)\n",
    "    ].copy()\n",
    "\n",
    "    # BUG FIX: Committee filter\n",
    "    if committee_filter:\n",
    "        window_df = window_df[window_df['Is_Committee_Relevant'] == True].copy()\n",
    "\n",
    "    sector_results = []\n",
    "    detailed_frames = []\n",
    "\n",
    "    for sector, cfg in config.items():\n",
    "        sdf = window_df[window_df['project_sector'] == sector].copy()\n",
    "\n",
    "        if len(sdf) == 0:\n",
    "            sector_results.append({\n",
    "                'Sector':             sector,\n",
    "                'target_etf':         cfg['target_etf'],\n",
    "                'directive':          f'INSUFFICIENT DATA: No trades in {lookback_days}-day window',\n",
    "                'conviction_score':   0.0,\n",
    "                'total_transactions': 0,\n",
    "                'net_volume':         0.0,\n",
    "                'total_volume':       0.0,\n",
    "                'n_members':          0,\n",
    "                'n_tickers':          0,\n",
    "                'diversity_pass':     False,\n",
    "                'volume_pass':        False,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # BUG FIX: Volume calculated with AmountMidpoint\n",
    "        sdf['signed_volume'] = sdf['Direction'] * sdf['AmountMidpoint']\n",
    "        net_volume   = sdf['signed_volume'].sum()\n",
    "        total_volume = sdf['AmountMidpoint'].sum()\n",
    "        n_members    = sdf['Name'].nunique()\n",
    "        n_tickers    = sdf['Ticker'].nunique()\n",
    "        n_txn        = len(sdf)\n",
    "\n",
    "        diversity_pass = (n_members >= cfg['tau_member']) and (n_tickers >= cfg['tau_ticker'])\n",
    "        volume_pass    = n_txn >= cfg['min_transactions']\n",
    "\n",
    "        # Conviction score: bounded [-1, 1]\n",
    "        conviction_score = (net_volume / total_volume) if total_volume > 0 else 0.0\n",
    "        conviction_score = float(np.clip(conviction_score, -1.0, 1.0))\n",
    "\n",
    "        # Directive string\n",
    "        if not volume_pass:\n",
    "            directive = f'INSUFFICIENT DATA: {n_txn} transactions < {cfg[\"min_transactions\"]} required'\n",
    "        elif not diversity_pass:\n",
    "            directive = f'FLAT: Diversity gate failed ({n_members} members, {n_tickers} tickers)'\n",
    "        elif conviction_score >= conviction_thresh:\n",
    "            directive = f'BUY LONG {cfg[\"target_etf\"]}'\n",
    "        elif conviction_score <= -conviction_thresh:\n",
    "            directive = f'SELL SHORT {cfg[\"target_etf\"]}'\n",
    "        else:\n",
    "            bias = 'bullish' if conviction_score > 0 else 'bearish'\n",
    "            directive = f'FLAT: Conviction {conviction_score:.2f} below threshold ({bias})'\n",
    "\n",
    "        sector_results.append({\n",
    "            'Sector':             sector,\n",
    "            'target_etf':         cfg['target_etf'],\n",
    "            'directive':          directive,\n",
    "            'conviction_score':   round(conviction_score, 4),\n",
    "            'total_transactions': n_txn,\n",
    "            'net_volume':         net_volume,\n",
    "            'total_volume':       total_volume,\n",
    "            'n_members':          n_members,\n",
    "            'n_tickers':          n_tickers,\n",
    "            'diversity_pass':     diversity_pass,\n",
    "            'volume_pass':        volume_pass,\n",
    "        })\n",
    "        detailed_frames.append(sdf)\n",
    "\n",
    "    sector_summary = pd.DataFrame(sector_results)\n",
    "    detailed_df    = pd.concat(detailed_frames, ignore_index=True) if detailed_frames else pd.DataFrame()\n",
    "    return sector_summary, detailed_df\n",
    "\n",
    "\n",
    "# ─── Current Signal Dashboard ───\n",
    "current_signals, current_detail = generate_signals(\n",
    "    df_enriched,\n",
    "    lookback_days=LOOKBACK_DAYS,\n",
    "    conviction_thresh=CONVICTION_THRESHOLD,\n",
    "    committee_filter=True,\n",
    ")\n",
    "\n",
    "print('\\n' + '=' * 90)\n",
    "print(f'SIGNAL DASHBOARD — {df_enriched[\"ReportDate\"].max().date()}')\n",
    "print(f'Lookback: {LOOKBACK_DAYS} days | Committee filter: ON | Threshold: {CONVICTION_THRESHOLD}')\n",
    "print('=' * 90)\n",
    "disp_cols = ['Sector', 'target_etf', 'directive', 'conviction_score',\n",
    "             'total_transactions', 'n_members', 'n_tickers']\n",
    "print(current_signals[disp_cols].to_string(index=False))\n",
    "\n",
    "print('\\n--- ACTIONABLE DIRECTIVES ---')\n",
    "actionable = current_signals[current_signals['directive'].str.startswith(('BUY', 'SELL'))]\n",
    "if actionable.empty:\n",
    "    print('No sectors currently exceed conviction threshold.')\n",
    "else:\n",
    "    for _, row in actionable.iterrows():\n",
    "        print(f'  {row[\"Sector\"]}: {row[\"directive\"]} (score={row[\"conviction_score\"]:.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Historical Signal Heatmap ───\n",
    "print('Generating historical signal heatmap (month-end signals)...')\n",
    "monthly_dates = pd.date_range(SAMPLE_START, df_enriched['ReportDate'].max(), freq='ME')\n",
    "\n",
    "hist_signals = {}\n",
    "for date in tqdm(monthly_dates, desc='Historical signals'):\n",
    "    sigs, _ = generate_signals(\n",
    "        df_enriched,\n",
    "        lookback_days=BACKTEST_LOOKBACK_DAYS,\n",
    "        conviction_thresh=CONVICTION_THRESHOLD,\n",
    "        current_date=date,\n",
    "        committee_filter=True,\n",
    "    )\n",
    "    for _, row in sigs.iterrows():\n",
    "        hist_signals[(date, row['Sector'])] = row['conviction_score']\n",
    "\n",
    "signal_matrix = (\n",
    "    pd.DataFrame(\n",
    "        [(d, s, v) for (d, s), v in hist_signals.items()],\n",
    "        columns=['date', 'sector', 'conviction']\n",
    "    )\n",
    "    .pivot(index='date', columns='sector', values='conviction')\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "sns.heatmap(\n",
    "    signal_matrix.T,\n",
    "    cmap='RdYlGn', center=0, vmin=-1, vmax=1,\n",
    "    ax=ax,\n",
    "    xticklabels=[\n",
    "        d.strftime('%Y-%m') if i % 6 == 0 else ''\n",
    "        for i, d in enumerate(signal_matrix.index)\n",
    "    ],\n",
    "    yticklabels=True,\n",
    "    cbar_kws={'label': 'Conviction Score'},\n",
    "    linewidths=0.3,\n",
    ")\n",
    "ax.set_title(\n",
    "    'Historical Conviction Signal Heatmap by Sector\\n'\n",
    "    f'(Committee-relevant trades only | {BACKTEST_LOOKBACK_DAYS}-day lookback)',\n",
    "    fontsize=13, fontweight='bold'\n",
    ")\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Sector')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 5: Backtest\n",
    "\n",
    "**Primary Strategy — Variant A (Absolute Threshold):**\n",
    "- Monthly rebalancing at month-end\n",
    "- `generate_signals()` with 90-day lookback, committee filter ON\n",
    "- Equal-weight sectors where conviction ≥ 0.80 → LONG\n",
    "- Equal-weight sectors where conviction ≤ −0.80 → SHORT\n",
    "- Zero-signal months → all cash (return = 0)\n",
    "- Transaction cost: 10 bps one-way\n",
    "\n",
    "**ASSUMPTION A8:** Threshold-based variant is primary. The hypothesis is absolute: a sector with a strong enough signal is tradeable independently. Tercile ranking fails with sparse signals (the common case with 7 sectors and strict gating).\n",
    "\n",
    "**ASSUMPTION A9:** Strategy will be net-long most months — congressional buying clusters in bull markets. This is a signal property, not a bug. Factor regression alpha is the primary evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# MODULES 13-14: Backtest — Portfolio Construction & Performance\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def run_backtest(\n",
    "    df_enriched: pd.DataFrame,\n",
    "    etf_returns: pd.DataFrame,\n",
    "    config: dict = None,\n",
    "    start_date: str = SAMPLE_START,\n",
    "    end_date: str = SAMPLE_END,\n",
    "    lookback_days: int = BACKTEST_LOOKBACK_DAYS,\n",
    "    conviction_thresh: float = CONVICTION_THRESHOLD,\n",
    "    rebalance_freq: str = 'ME',\n",
    "    tc_bps: int = TRANSACTION_COST_BPS,\n",
    "    variant: str = 'threshold',\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Full historical backtest with monthly rebalancing.\n",
    "\n",
    "    CRITICAL: Signal at each rebalance_date uses only data UP TO rebalance_date.\n",
    "              ETF returns for the holding period are applied AFTER the rebalance_date.\n",
    "              No future information used in signal generation.\n",
    "\n",
    "    variant: 'threshold' (primary) or 'tercile' (robustness)\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = SECTOR_CONFIG\n",
    "\n",
    "    rebalance_dates = pd.date_range(start_date, end_date, freq=rebalance_freq)\n",
    "    etf_pivot = (\n",
    "        etf_returns\n",
    "        .pivot(index='date', columns='etf_ticker', values='ret')\n",
    "        .pipe(lambda df: df.set_index(pd.to_datetime(df.index)))\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    records       = []\n",
    "    prev_positions = {}\n",
    "\n",
    "    for i, rebalance_date in enumerate(rebalance_dates[:-1]):\n",
    "        next_date = rebalance_dates[i + 1]\n",
    "\n",
    "        # CRITICAL: Signal uses only data up to rebalance_date\n",
    "        signals, _ = generate_signals(\n",
    "            df_enriched, config=config,\n",
    "            lookback_days=lookback_days,\n",
    "            conviction_thresh=conviction_thresh,\n",
    "            current_date=rebalance_date,\n",
    "            committee_filter=True,\n",
    "        )\n",
    "\n",
    "        # Determine positions\n",
    "        if variant == 'threshold':\n",
    "            long_etfs  = signals[\n",
    "                signals['directive'].str.startswith('BUY') &\n",
    "                signals['diversity_pass'] & signals['volume_pass']\n",
    "            ]['target_etf'].tolist()\n",
    "            short_etfs = signals[\n",
    "                signals['directive'].str.startswith('SELL') &\n",
    "                signals['diversity_pass'] & signals['volume_pass']\n",
    "            ]['target_etf'].tolist()\n",
    "        elif variant == 'tercile':\n",
    "            valid = signals[signals['diversity_pass'] & signals['volume_pass']]\n",
    "            if len(valid) < 3:\n",
    "                long_etfs, short_etfs = [], []\n",
    "            else:\n",
    "                n_each    = len(valid) // 3\n",
    "                sorted_v  = valid.sort_values('conviction_score', ascending=False)\n",
    "                long_etfs  = sorted_v.head(n_each)['target_etf'].tolist()\n",
    "                short_etfs = sorted_v.tail(n_each)['target_etf'].tolist()\n",
    "        else:\n",
    "            raise ValueError(f'Unknown variant: {variant}')\n",
    "\n",
    "        # Equal-weight positions\n",
    "        new_positions = {}\n",
    "        if long_etfs:\n",
    "            w = 1.0 / len(long_etfs)\n",
    "            for etf in long_etfs:\n",
    "                new_positions[etf] = new_positions.get(etf, 0) + w\n",
    "        if short_etfs:\n",
    "            w = -1.0 / len(short_etfs)\n",
    "            for etf in short_etfs:\n",
    "                new_positions[etf] = new_positions.get(etf, 0) + w\n",
    "\n",
    "        # Turnover\n",
    "        all_etfs = set(prev_positions) | set(new_positions)\n",
    "        turnover = sum(\n",
    "            abs(new_positions.get(e, 0) - prev_positions.get(e, 0))\n",
    "            for e in all_etfs\n",
    "        ) / 2\n",
    "\n",
    "        # Portfolio return: compound daily returns over holding period\n",
    "        period_rets = etf_pivot[\n",
    "            (etf_pivot.index > rebalance_date) & (etf_pivot.index <= next_date)\n",
    "        ]\n",
    "\n",
    "        if period_rets.empty or not new_positions:\n",
    "            port_ret_gross = 0.0\n",
    "        else:\n",
    "            daily_port = pd.Series(0.0, index=period_rets.index)\n",
    "            for etf, weight in new_positions.items():\n",
    "                if etf in period_rets.columns:\n",
    "                    daily_port += weight * period_rets[etf].fillna(0)\n",
    "            port_ret_gross = (1 + daily_port).prod() - 1\n",
    "\n",
    "        tc             = turnover * tc_bps / 10_000\n",
    "        port_ret_net   = port_ret_gross - tc\n",
    "\n",
    "        records.append({\n",
    "            'date':             next_date,\n",
    "            'rebalance_date':   rebalance_date,\n",
    "            'portfolio_ret_gross': port_ret_gross,\n",
    "            'portfolio_ret_net':   port_ret_net,\n",
    "            'long_etfs':        long_etfs,\n",
    "            'short_etfs':       short_etfs,\n",
    "            'n_long':           len(long_etfs),\n",
    "            'n_short':          len(short_etfs),\n",
    "            'n_valid_signals':  int((signals['diversity_pass'] & signals['volume_pass']).sum()),\n",
    "            'turnover':         turnover,\n",
    "        })\n",
    "        prev_positions = new_positions\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# ─── Run Primary Backtest ───\n",
    "print('Running backtest (Variant A — Threshold)...')\n",
    "backtest_results = run_backtest(\n",
    "    df_enriched, etf_returns,\n",
    "    start_date=SAMPLE_START, end_date=SAMPLE_END,\n",
    "    lookback_days=BACKTEST_LOOKBACK_DAYS,\n",
    "    conviction_thresh=CONVICTION_THRESHOLD,\n",
    "    variant='threshold',\n",
    ")\n",
    "print(f'Complete: {len(backtest_results)} monthly periods')\n",
    "print(f'Months with positions: {(backtest_results[\"n_long\"] + backtest_results[\"n_short\"] > 0).sum()}')\n",
    "backtest_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# Performance Analytics & Plots\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def compute_performance(returns: pd.Series, rf_annual: float = 0.04) -> dict:\n",
    "    \"\"\"Annualized performance metrics for monthly return series.\"\"\"\n",
    "    periods = 12\n",
    "    rf_monthly = rf_annual / periods\n",
    "\n",
    "    ann_ret = (1 + returns).prod() ** (periods / len(returns)) - 1\n",
    "    ann_vol = returns.std() * np.sqrt(periods)\n",
    "    excess  = returns - rf_monthly\n",
    "    sharpe  = (excess.mean() / returns.std()) * np.sqrt(periods) if returns.std() > 0 else np.nan\n",
    "\n",
    "    cum = (1 + returns).cumprod()\n",
    "    rolling_max = cum.cummax()\n",
    "    dd = (cum - rolling_max) / rolling_max\n",
    "    max_dd = dd.min()\n",
    "    end_dd = dd.idxmin()\n",
    "    start_dd = cum[:end_dd].idxmax() if not cum[:end_dd].empty else end_dd\n",
    "\n",
    "    invested = (backtest_results['n_long'] + backtest_results['n_short'] > 0).mean()\n",
    "\n",
    "    return {\n",
    "        'annualized_return':       ann_ret,\n",
    "        'annualized_vol':          ann_vol,\n",
    "        'sharpe_ratio':            sharpe,\n",
    "        'max_drawdown':            max_dd,\n",
    "        'max_drawdown_start':      start_dd,\n",
    "        'max_drawdown_end':        end_dd,\n",
    "        'calmar_ratio':            ann_ret / abs(max_dd) if max_dd != 0 else np.nan,\n",
    "        'hit_rate':                (returns > 0).mean(),\n",
    "        'best_month':              returns.max(),\n",
    "        'worst_month':             returns.min(),\n",
    "        'n_months':                len(returns),\n",
    "        'pct_months_invested':     invested,\n",
    "        'avg_monthly_turnover':    backtest_results['turnover'].mean(),\n",
    "    }\n",
    "\n",
    "\n",
    "strategy_returns = backtest_results.set_index('date')['portfolio_ret_net']\n",
    "perf = compute_performance(strategy_returns)\n",
    "\n",
    "# SPY benchmark (monthly, compounded from daily)\n",
    "spy_monthly = (\n",
    "    etf_returns[etf_returns['etf_ticker'] == 'SPY']\n",
    "    .set_index('date')['ret']\n",
    "    .resample('ME')\n",
    "    .apply(lambda x: (1 + x).prod() - 1)\n",
    "    .rename('SPY')\n",
    ")\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('BACKTEST PERFORMANCE SUMMARY (Variant A — Threshold)')\n",
    "print('=' * 60)\n",
    "for k, v in perf.items():\n",
    "    if isinstance(v, float) and not np.isnan(v):\n",
    "        if 'return' in k or 'vol' in k or 'drawdown' in k or 'rate' in k or 'month' in k or 'turnover' in k:\n",
    "            print(f'  {k:<30} {v:>10.2%}')\n",
    "        else:\n",
    "            print(f'  {k:<30} {v:>10.3f}')\n",
    "    elif hasattr(v, 'date'):\n",
    "        print(f'  {k:<30} {v.date()}')\n",
    "    else:\n",
    "        print(f'  {k:<30} {v}')\n",
    "print('=' * 60)\n",
    "\n",
    "# ─── Plots ───\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 14))\n",
    "fig.suptitle('Backtest — Congressional Trading Signal Strategy',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# 1. Cumulative returns\n",
    "ax = axes[0]\n",
    "cum_strat = (1 + strategy_returns).cumprod()\n",
    "cum_spy   = (1 + spy_monthly.reindex(strategy_returns.index).fillna(0)).cumprod()\n",
    "ax.plot(cum_strat.index, cum_strat.values, label='Congressional Signal Strategy',\n",
    "        linewidth=2, color='steelblue')\n",
    "ax.plot(cum_spy.index, cum_spy.values, label='SPY Benchmark',\n",
    "        linewidth=1.5, color='coral', linestyle='--')\n",
    "ax.set_title('Cumulative Returns (Net of 10 bps Transaction Costs)')\n",
    "ax.set_ylabel('Growth of $1')\n",
    "ax.legend()\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'${y:.2f}'))\n",
    "\n",
    "# 2. Drawdown\n",
    "ax = axes[1]\n",
    "cum_ret = (1 + strategy_returns).cumprod()\n",
    "rolling_max = cum_ret.cummax()\n",
    "dd = (cum_ret - rolling_max) / rolling_max\n",
    "ax.fill_between(dd.index, dd.values, 0, color='coral', alpha=0.6, label='Drawdown')\n",
    "ax.set_title('Strategy Drawdown')\n",
    "ax.set_ylabel('Drawdown')\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1%}'))\n",
    "ax.legend()\n",
    "\n",
    "# 3. Monthly returns heatmap\n",
    "ax = axes[2]\n",
    "mr = strategy_returns.copy()\n",
    "mr.index = pd.to_datetime(mr.index)\n",
    "pivot = (\n",
    "    mr.to_frame('ret')\n",
    "      .assign(year=lambda d: d.index.year, month=lambda d: d.index.month)\n",
    "      .pivot(index='year', columns='month', values='ret')\n",
    ")\n",
    "pivot.columns = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "sns.heatmap(pivot, cmap='RdYlGn', center=0, annot=True, fmt='.1%',\n",
    "            ax=ax, cbar_kws={'label': 'Monthly Return'}, linewidths=0.5)\n",
    "ax.set_title('Monthly Returns Heatmap')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Composition chart\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "n_pos = backtest_results['n_long'] + backtest_results['n_short']\n",
    "ax.hist(n_pos, bins=range(0, max(n_pos) + 2), edgecolor='white',\n",
    "        color='steelblue', alpha=0.8)\n",
    "ax.set_title('Distribution of Active Sector Positions per Month')\n",
    "ax.set_xlabel('Number of ETF Positions')\n",
    "ax.set_ylabel('Number of Months')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ─── Robustness: Variant B (Tercile) ───\n",
    "print('\\nRunning robustness check (Variant B — Tercile Ranking)...')\n",
    "bt_tercile = run_backtest(\n",
    "    df_enriched, etf_returns,\n",
    "    start_date=SAMPLE_START, end_date=SAMPLE_END,\n",
    "    lookback_days=BACKTEST_LOOKBACK_DAYS, conviction_thresh=CONVICTION_THRESHOLD,\n",
    "    variant='tercile',\n",
    ")\n",
    "perf_t = compute_performance(bt_tercile.set_index('date')['portfolio_ret_net'])\n",
    "\n",
    "print(f'\\n{\"Metric\":<30} {\"Threshold (Primary)\":>22} {\"Tercile (Robustness)\":>22}')\n",
    "print('-' * 78)\n",
    "for k in ['annualized_return', 'annualized_vol', 'sharpe_ratio', 'max_drawdown']:\n",
    "    v_a = perf.get(k, np.nan)\n",
    "    v_b = perf_t.get(k, np.nan)\n",
    "    fmt = '.2%' if any(x in k for x in ['return', 'vol', 'drawdown']) else '.3f'\n",
    "    print(f'{k:<30} {v_a:>22{fmt}} {v_b:>22{fmt}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 6: Factor Regression — Alpha Evaluation\n",
    "\n",
    "**Model:**\n",
    "$$R_{portfolio,t} - R_{f,t} = \\alpha + \\beta_1\\text{MktRF} + \\beta_2\\text{SMB} + \\beta_3\\text{HML} + \\beta_4\\text{RMW} + \\beta_5\\text{CMA} + \\beta_6\\text{Mom} + \\epsilon_t$$\n",
    "\n",
    "**Interpretation guide:**\n",
    "- `α > 0, p < 0.05` → Strategy generates genuine alpha beyond known risk factors\n",
    "- `α ≈ 0` → Returns are explained by factor exposures\n",
    "- High `MktRF` loading → Strategy is primarily a directional market bet (expected per ASSUMPTION A9)\n",
    "- High `Mom` loading → Strategy is momentum in disguise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas_datareader not installed — skipping factor download.\n",
      "Factor data unavailable — skipping regression.\n",
      "No FF5+Momentum data from cache or download. Check network access to Ken French data library.\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# MODULE 15: Factor Regression — FF5 + Momentum\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def run_factor_regression(portfolio_returns: pd.Series,\n",
    "                          factor_data: pd.DataFrame,\n",
    "                          frequency: str = 'monthly'):\n",
    "    if sm is None:\n",
    "        print('statsmodels unavailable; skipping factor regression.')\n",
    "        return None\n",
    "    \"\"\"\n",
    "    OLS regression of excess portfolio returns on FF5 + Momentum.\n",
    "    portfolio_returns must be EXCESS returns (net of RF).\n",
    "    factor_data must contain: Mkt_RF, SMB, HML, RMW, CMA, Mom.\n",
    "    Returns (model, reg_data) or None.\n",
    "    \"\"\"\n",
    "    if factor_data.empty:\n",
    "        print('Factor data not available.')\n",
    "        return None\n",
    "\n",
    "    f = factor_data.copy()\n",
    "    # Normalize column names\n",
    "    f.columns = [c.strip().replace('-', '_').replace(' ', '') for c in f.columns]\n",
    "    mom_col = next((c for c in f.columns if c.lower() in ('mom', 'wml', 'umd')), None)\n",
    "    if mom_col and mom_col != 'Mom':\n",
    "        f = f.rename(columns={mom_col: 'Mom'})\n",
    "\n",
    "    # Align to monthly if needed\n",
    "    if frequency == 'monthly':\n",
    "        f = f.resample('ME').last() if f.index.freq != 'ME' else f\n",
    "        f.index = f.index + pd.offsets.MonthEnd(0)\n",
    "\n",
    "    required = [c for c in ['Mkt_RF', 'SMB', 'HML', 'RMW', 'CMA', 'Mom'] if c in f.columns]\n",
    "    common   = portfolio_returns.index.intersection(f.index)\n",
    "\n",
    "    if len(common) < 12:\n",
    "        print(f'Insufficient aligned months: {len(common)}')\n",
    "        return None\n",
    "\n",
    "    port = portfolio_returns.loc[common]\n",
    "    fac  = f.loc[common]\n",
    "\n",
    "    # Excess returns (subtract RF)\n",
    "    rf = fac['RF'] if 'RF' in fac.columns else pd.Series(0.04 / 12, index=common)\n",
    "    excess = port - rf\n",
    "\n",
    "    reg_data = fac[required].copy()\n",
    "    reg_data['excess_ret'] = excess\n",
    "    reg_data = reg_data.dropna()\n",
    "\n",
    "    if len(reg_data) < 12:\n",
    "        print('Insufficient data after alignment.')\n",
    "        return None\n",
    "\n",
    "    X   = sm.add_constant(reg_data[required])\n",
    "    y   = reg_data['excess_ret']\n",
    "    res = sm.OLS(y, X).fit(cov_type='HC3')\n",
    "    return res, reg_data\n",
    "\n",
    "\n",
    "def format_regression_table(model) -> pd.DataFrame:\n",
    "    \"\"\"Format OLS results as a clean coefficient table.\"\"\"\n",
    "    def stars(p):\n",
    "        return '***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.10 else ''\n",
    "\n",
    "    tbl = pd.DataFrame({\n",
    "        'Coefficient': model.params,\n",
    "        'Std Error':   model.bse,\n",
    "        't-stat':      model.tvalues,\n",
    "        'p-value':     model.pvalues,\n",
    "        'CI Lower':    model.conf_int()[0],\n",
    "        'CI Upper':    model.conf_int()[1],\n",
    "    })\n",
    "    tbl['Sig'] = tbl['p-value'].apply(stars)\n",
    "    return tbl.round(4)\n",
    "\n",
    "\n",
    "# ─── Run Regression ───\n",
    "ff_for_reg = ff_factors.copy() if isinstance(ff_factors, pd.DataFrame) else pd.DataFrame()\n",
    "\n",
    "if ff_for_reg.empty:\n",
    "    ff_cache_path = EXTERNAL_DIR / 'ff5_mom_factors.csv'\n",
    "    if ff_cache_path.exists():\n",
    "        ff_for_reg = pd.read_csv(ff_cache_path, index_col='date', parse_dates=True)\n",
    "        print(f'Loaded factor cache for regression: {len(ff_for_reg):,} rows')\n",
    "\n",
    "if ff_for_reg.empty:\n",
    "    ff_for_reg = fetch_ff5_mom(ESTIMATION_WINDOW_START, SAMPLE_END)\n",
    "\n",
    "if not ff_for_reg.empty:\n",
    "    reg_result = run_factor_regression(strategy_returns, ff_for_reg, frequency='monthly')\n",
    "else:\n",
    "    reg_result = None\n",
    "    print('Factor data unavailable — skipping regression.')\n",
    "    print('No FF5+Momentum data from cache or download. Check network access to Ken French data library.')\n",
    "\n",
    "if reg_result is not None:\n",
    "    reg_model, reg_data = reg_result\n",
    "\n",
    "    print('\\n' + '=' * 70)\n",
    "    print('FAMA-FRENCH 5 FACTOR + MOMENTUM REGRESSION')\n",
    "    print(f'Sample: {reg_data.index.min().date()} to {reg_data.index.max().date()}')\n",
    "    print(f'N = {len(reg_data)} monthly observations | HC3 standard errors')\n",
    "    print('=' * 70)\n",
    "    coef_table = format_regression_table(reg_model)\n",
    "    print(coef_table.to_string())\n",
    "    print(f'\\nR²          = {reg_model.rsquared:.4f}')\n",
    "    print(f'Adjusted R² = {reg_model.rsquared_adj:.4f}')\n",
    "\n",
    "    alpha     = reg_model.params.get('const', np.nan)\n",
    "    alpha_t   = reg_model.tvalues.get('const', np.nan)\n",
    "    alpha_p   = reg_model.pvalues.get('const', np.nan)\n",
    "    alpha_ann = (1 + alpha) ** 12 - 1 if not np.isnan(alpha) else np.nan\n",
    "    mkt_beta  = reg_model.params.get('Mkt_RF', np.nan)\n",
    "\n",
    "    print('\\n' + '=' * 70)\n",
    "    print('ALPHA ASSESSMENT')\n",
    "    print('=' * 70)\n",
    "    print(f'Monthly alpha:     {alpha:.4f}  ({alpha*10000:.1f} bps/month)')\n",
    "    print(f'Annualized alpha:  {alpha_ann:.2%}')\n",
    "    print(f't-statistic:       {alpha_t:.3f}')\n",
    "    print(f'p-value:           {alpha_p:.4f}')\n",
    "    print(f'Market beta:       {mkt_beta:.3f}')\n",
    "\n",
    "    print('\\nINTERPRETATION:')\n",
    "    if not np.isnan(alpha_p):\n",
    "        if alpha_p < 0.05 and alpha > 0:\n",
    "            print('RESULT: SIGNIFICANT POSITIVE ALPHA — strategy adds value beyond risk factors (p < 0.05)')\n",
    "        elif alpha_p < 0.10 and alpha > 0:\n",
    "            print('RESULT: MARGINAL POSITIVE ALPHA — borderline significant (p < 0.10)')\n",
    "        elif alpha_p > 0.10 and alpha > 0:\n",
    "            print('RESULT: POSITIVE BUT INSIGNIFICANT ALPHA — returns partially explained by factor tilts')\n",
    "        elif alpha <= 0:\n",
    "            print('RESULT: NON-POSITIVE ALPHA — strategy does not add value after factor adjustment')\n",
    "\n",
    "    if not np.isnan(mkt_beta) and abs(mkt_beta) > 0.5:\n",
    "        print(f'\\nNOTE: High MktRF loading ({mkt_beta:.2f}) confirms directional market exposure')\n",
    "        print('      (expected per ASSUMPTION A9 — congressional buying clusters in bull markets)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Factor Loading Visualization ───\n",
    "if reg_result is not None:\n",
    "    reg_model, reg_data = reg_result\n",
    "    coef_table = format_regression_table(reg_model)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle('Factor Regression Results — FF5 + Momentum', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Factor loadings with 95% CI\n",
    "    ax = axes[0]\n",
    "    params = coef_table[coef_table.index != 'const'].copy()\n",
    "    y_pos  = range(len(params))\n",
    "    colors = ['steelblue' if c > 0 else 'coral' for c in params['Coefficient']]\n",
    "    ax.barh(y_pos, params['Coefficient'],\n",
    "            xerr=1.96 * params['Std Error'],\n",
    "            color=colors, alpha=0.7, capsize=5, edgecolor='white')\n",
    "    ax.axvline(0, color='black', linewidth=0.8)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(params.index)\n",
    "    ax.set_title('Factor Loadings (±1.96 SE)')\n",
    "    ax.set_xlabel('Coefficient')\n",
    "\n",
    "    # Alpha highlighted separately\n",
    "    alpha_val = coef_table.loc['const', 'Coefficient'] if 'const' in coef_table.index else 0\n",
    "    alpha_se  = coef_table.loc['const', 'Std Error']   if 'const' in coef_table.index else 0\n",
    "    ax.text(0.95, 0.05,\n",
    "            f'α = {alpha_val:.4f}\\n(ann. {(1+alpha_val)**12-1:.1%})',\n",
    "            transform=ax.transAxes, ha='right', va='bottom',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "    # Fitted vs actual\n",
    "    ax = axes[1]\n",
    "    fitted = reg_model.fittedvalues\n",
    "    actual = reg_data['excess_ret']\n",
    "    ax.scatter(fitted, actual, alpha=0.5, color='steelblue', s=30)\n",
    "    lims = [min(fitted.min(), actual.min()), max(fitted.max(), actual.max())]\n",
    "    ax.plot(lims, lims, 'r--', linewidth=1, label='45° line')\n",
    "    ax.set_title('Fitted vs Actual Excess Returns')\n",
    "    ax.set_xlabel('Fitted (Factor Model)')\n",
    "    ax.set_ylabel('Actual Portfolio Excess Return')\n",
    "    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1%}'))\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1%}'))\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "\n",
    "### Assumptions Made\n",
    "| ID | Assumption | Impact if Wrong |\n",
    "|----|-----------|----------------|\n",
    "| A1 | Amount field treated as midpoint of STOCK Act range | Conviction scores shift; threshold may need recalibration |\n",
    "| A2 | Restrict to 7 sectors with high government intervention | May miss alpha in other sectors |\n",
    "| A3 | GICS classification is current, not point-in-time | Some tickers misclassified in historical periods |\n",
    "| A4 | Committee assignments at Congress session level, not exact date | Some trades tagged to wrong committee |\n",
    "| A5 | Lookback window uses ReportDate only | Signal may be stale for trades with long disclosure lag |\n",
    "| A6 | Dedup: composite key includes Amount | Trade counts slightly inflated for truly identical trades |\n",
    "| A7 | Transaction cost: 10 bps one-way | If costs are higher, net returns decrease proportionally |\n",
    "| A8 | Threshold-based portfolio (Variant A) is primary strategy | Strategy carries directional market exposure |\n",
    "| A9 | Strategy net-long most months (congressional buying in bull markets) | High MktRF beta in factor regression |\n",
    "| A10 | API column names normalized immediately: Filed→ReportDate, Traded→TransactionDate | Pipeline crashes if rename is missed |\n",
    "\n",
    "\n",
    "### Unresolved Questions\n",
    "1. **Committee roster quality:** Full H2 analysis requires `CONGRESS_API_KEY` from congress.gov\n",
    "2. **Point-in-time GICS:** Currently using current sector classification (ASSUMPTION A3)\n",
    "3. **Lookback sensitivity:** Primary analysis uses 90-day window per proposal; robustness checks needed\n",
    "4. **Delisting survivorship:** Tickers delisted during the sample period may bias event study results\n",
    "\n",
    "### Next Required Artifacts\n",
    "- `logs/assumptions.md` — Formal log with all assumption IDs, dates, and rationale\n",
    "- `logs/data_transformations.md` — Row counts at each pipeline stage\n",
    "- Backtest return time series saved for external validation\n",
    "- Factor regression output saved to file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
