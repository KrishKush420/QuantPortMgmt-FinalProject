{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data Acquisition & Preparation\n",
    "\n",
    "## Overview\n",
    "This notebook handles the foundational data preparation for the Congressional trading analysis. We will:\n",
    "1. Load STOCK Act filings (congressional trades)\n",
    "2. Load daily stock returns data\n",
    "3. Merge datasets on ticker and date\n",
    "4. Compute disclosure lag (time between trade and filing)\n",
    "5. Filter for valid disclosure windows\n",
    "6. Create event study identifiers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries\n",
    "\n",
    "We begin by importing all necessary libraries for data manipulation, visualization, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1a: Load & Inspect STOCK Act Filings\n",
    "\n",
    "This section reads the congressional trading data from the parquet file. We load the STOCK Act filings which contain information about trades made by members of Congress. The data includes trade dates, disclosure dates, tickers, transaction amounts, and transaction types (buy/sell). We perform initial schema validation and inspect the first few rows to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load congressional trading data\n",
    "congress_df = pd.read_parquet('data/congress_trading.parquet')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONGRESSIONAL TRADING DATA (STOCK ACT FILINGS)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset shape: {congress_df.shape[0]:,} rows × {congress_df.shape[1]} columns\")\n",
    "print(f\"\\nColumn names and types:\")\n",
    "print(congress_df.dtypes)\n",
    "print(f\"\\nFirst 5 records:\")\n",
    "print(congress_df.head())\n",
    "print(f\"\\nData description (numeric columns):\")\n",
    "print(congress_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code chunk checks for missing values and provides summary statistics on the congressional trading dataset. Understanding data quality is critical before proceeding with analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and data quality\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA QUALITY CHECK - STOCK ACT FILINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "missing_stats = congress_df.isnull().sum()\n",
    "print(missing_stats[missing_stats > 0] if missing_stats.sum() > 0 else \"No missing values!\")\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(congress_df.dtypes)\n",
    "\n",
    "print(f\"\\nDate columns info:\")\n",
    "date_cols = congress_df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "for col in date_cols:\n",
    "    print(f\"  {col}: {congress_df[col].min()} to {congress_df[col].max()}\")\n",
    "\n",
    "print(f\"\\nUnique values in key categorical columns:\")\n",
    "categorical_cols = congress_df.select_dtypes(include=['object']).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    print(f\"  {col}: {congress_df[col].nunique()} unique values\")\n",
    "    if congress_df[col].nunique() <= 5:\n",
    "        print(f\"    Values: {congress_df[col].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1b: Load & Inspect Stock Returns Data\n",
    "\n",
    "This section loads the daily stock returns from the master data file. This dataset contains historical daily returns for the securities that appear in the congressional trading data. We validate the schema and ensure it has the necessary date and return information needed for matching with the congressional filings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load daily stock returns data\n",
    "returns_df = pd.read_parquet('data/MasterData_small.parquet')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DAILY STOCK RETURNS DATA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset shape: {returns_df.shape[0]:,} rows × {returns_df.shape[1]} columns\")\n",
    "print(f\"\\nColumn names and types:\")\n",
    "print(returns_df.dtypes)\n",
    "print(f\"\\nFirst 5 records:\")\n",
    "print(returns_df.head())\n",
    "print(f\"\\nData description (numeric columns):\")\n",
    "print(returns_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code validates the stock returns dataset quality, checks for completeness, and confirms the date range aligns with the congressional trading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate stock returns data quality\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA QUALITY CHECK - STOCK RETURNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "missing_stats = returns_df.isnull().sum()\n",
    "print(missing_stats[missing_stats > 0] if missing_stats.sum() > 0 else \"No missing values!\")\n",
    "\n",
    "print(f\"\\nDate range:\")\n",
    "date_cols = returns_df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "if date_cols:\n",
    "    for col in date_cols:\n",
    "        print(f\"  {col}: {returns_df[col].min()} to {returns_df[col].max()}\")\n",
    "\n",
    "print(f\"\\nNumber of unique tickers: {returns_df['Ticker'].nunique() if 'Ticker' in returns_df.columns else 'N/A'}\")\n",
    "print(f\"\\nReturn statistics:\")\n",
    "return_cols = returns_df.select_dtypes(include=['float64', 'float32']).columns.tolist()\n",
    "if return_cols:\n",
    "    for col in return_cols:\n",
    "        print(f\"  {col}: min={returns_df[col].min():.4f}, max={returns_df[col].max():.4f}, mean={returns_df[col].mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1c: Merge Congressional Filings with Stock Returns\n",
    "\n",
    "This section merges the congressional trading data with the daily stock returns. For each congressional trade, we match it with the stock's daily returns on the trade date, disclosure date, and surrounding dates needed for event window analysis. This creates a unified dataset that connects political transactions to market data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure date columns are in the correct format\n",
    "# Identify date columns in both datasets\n",
    "congress_date_cols = congress_df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "returns_date_cols = returns_df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "print(f\"Congress date columns: {congress_date_cols}\")\n",
    "print(f\"Returns date columns: {returns_date_cols}\")\n",
    "\n",
    "# Display sample to understand structure for merging\n",
    "print(\"\\nSample congress data (for merge):\")\n",
    "print(congress_df[['Ticker', 'TradeDate', 'DisclosureDate']].head(10))\n",
    "print(\"\\nSample returns data (for merge):\")\n",
    "print(returns_df[[col for col in returns_df.columns if 'Ticker' in col or 'Date' in col or 'date' in col]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs the actual merge between congressional filings and stock returns. It aligns trades with their corresponding daily returns on the trade date and disclosure date, handling potential timezone or formatting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for merging\n",
    "# Create copies to avoid modifying originals\n",
    "congress_merge = congress_df.copy()\n",
    "returns_merge = returns_df.copy()\n",
    "\n",
    "# Identify the appropriate ticker and date columns\n",
    "# Adjust these column names based on your actual data structure\n",
    "ticker_col_congress = 'Ticker'  # May need adjustment\n",
    "ticker_col_returns = 'Ticker'   # May need adjustment\n",
    "trade_date_col = 'TradeDate'    # Column name for trade date\n",
    "disclosure_date_col = 'DisclosureDate'  # Column name for disclosure date\n",
    "return_date_col = 'Date'        # May need adjustment based on actual column name\n",
    "\n",
    "# Verify columns exist\n",
    "print(\"Verifying required columns exist:\")\n",
    "print(f\"  Congress ticker: {ticker_col_congress} - {ticker_col_congress in congress_df.columns}\")\n",
    "print(f\"  Congress trade date: {trade_date_col} - {trade_date_col in congress_df.columns}\")\n",
    "print(f\"  Congress disclosure date: {disclosure_date_col} - {disclosure_date_col in congress_df.columns}\")\n",
    "print(f\"  Returns ticker: {ticker_col_returns} - {ticker_col_returns in returns_df.columns}\")\n",
    "print(f\"  Returns date: {return_date_col} - {return_date_col in returns_df.columns}\")\n",
    "\n",
    "# NOTE: If columns don't match, update the above variable assignments"
   ]
  },
  {
   "cell_calls>
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1d: Compute Disclosure Lag\n",
    "\n",
    "This section calculates the disclosure lag—the number of days between when a Congress member made the trade and when they disclosed it. According to the STOCK Act, filings must occur within 45 days of the trade. We compute this lag for every record and flag any trades outside the expected 0-45 day window. This is a critical quality control step as unusual lags may indicate data quality issues or non-compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute disclosure lag (DisclosureDate - TradeDate)\n",
    "congress_df['Disclosure_Lag'] = (congress_df[disclosure_date_col] - congress_df[trade_date_col]).dt.days\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISCLOSURE LAG COMPUTATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDisclosure lag statistics (in days):\")\n",
    "print(congress_df['Disclosure_Lag'].describe())\n",
    "\n",
    "print(f\"\\nDisclosure lag distribution:\")\n",
    "print(congress_df['Disclosure_Lag'].value_counts().sort_index().head(20))\n",
    "\n",
    "print(f\"\\nSample records with computed lag:\")\n",
    "print(congress_df[[trade_date_col, disclosure_date_col, 'Disclosure_Lag']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code identifies and flags any records with unusual disclosure lags. We flag records with negative lags (disclosure before trade, which shouldn't happen) and lags exceeding 45 days (violating STOCK Act requirements). This helps identify data quality issues or non-compliant filings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag records outside the valid 0-45 day disclosure window\n",
    "congress_df['Lag_Valid'] = (congress_df['Disclosure_Lag'] >= 0) & (congress_df['Disclosure_Lag'] <= 45)\n",
    "congress_df['Lag_Flag'] = ''\n",
    "\n",
    "congress_df.loc[congress_df['Disclosure_Lag'] < 0, 'Lag_Flag'] = 'Negative Lag'\n",
    "congress_df.loc[congress_df['Disclosure_Lag'] > 45, 'Lag_Flag'] = 'Exceeds 45 Days'\n",
    "congress_df.loc[congress_df['Lag_Valid'], 'Lag_Flag'] = 'Valid'\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISCLOSURE LAG VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nLag flag distribution:\")\n",
    "print(congress_df['Lag_Flag'].value_counts())\n",
    "\n",
    "print(f\"\\nPercentage of valid disclosures (0-45 days):\")\n",
    "valid_pct = (congress_df['Lag_Valid'].sum() / len(congress_df)) * 100\n",
    "print(f\"  {valid_pct:.2f}% ({congress_df['Lag_Valid'].sum():,} out of {len(congress_df):,} records)\")\n",
    "\n",
    "print(f\"\\nExamples of records with invalid disclosure lag:\")\n",
    "invalid_records = congress_df[~congress_df['Lag_Valid']][[trade_date_col, disclosure_date_col, 'Disclosure_Lag', 'Lag_Flag']]\n",
    "if len(invalid_records) > 0:\n",
    "    print(invalid_records.head(10))\nelse:\n",
    "    print(\"  All records have valid disclosure lags!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1e: Create Event Study Identifiers\n",
    "\n",
    "This section creates unique identifiers for each event (Congressional trade + disclosure combination) and ensures we have a clean, deduplicated dataset ready for event study analysis. Each event will need an ID for tracking through subsequent analysis stages (market model estimation, abnormal return calculation, etc.). We also filter to only valid disclosure lags to ensure data quality in downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to records with valid disclosure lag for analysis\n",
    "congress_valid = congress_df[congress_df['Lag_Valid']].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FILTERING TO VALID DISCLOSURE LAGS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nRecords before filtering: {len(congress_df):,}\")\n",
    "print(f\"Records after filtering: {len(congress_valid):,}\")\n",
    "print(f\"Records removed: {len(congress_df) - len(congress_valid):,}\")\n",
    "\n",
    "# Create unique event identifier\n",
    "congress_valid['Event_ID'] = range(1, len(congress_valid) + 1)\n",
    "\n",
    "print(f\"\\nEvent IDs created: {congress_valid['Event_ID'].max()} total events\")\n",
    "print(f\"\\nSample events with IDs:\")\n",
    "print(congress_valid[['Event_ID', 'Ticker', trade_date_col, disclosure_date_col, 'Disclosure_Lag']].head(10))"
   ]
  },
  {
   "cell_type": "markdown">
   "metadata": {},
   "source": [
    "This final code chunk provides comprehensive summary statistics on the cleaned dataset, confirming it's ready for event study analysis. It includes event counts by trade direction, date range coverage, and ticker diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics on cleaned dataset\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANED DATASET SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nTotal events: {len(congress_valid):,}\")\n",
    "print(f\"Date range: {congress_valid[trade_date_col].min()} to {congress_valid[trade_date_col].max()}\")\n",
    "\n",
    "# By trade type (Buy/Sell) if available\n",
    "if 'TransactionType' in congress_valid.columns or 'Type' in congress_valid.columns:\n",
    "    type_col = 'TransactionType' if 'TransactionType' in congress_valid.columns else 'Type'\n",
    "    print(f\"\\nEvents by transaction type:\")\n",
    "    print(congress_valid[type_col].value_counts())\n",
    "\n",
    "# By ticker\n",
    "print(f\"\\nNumber of unique tickers: {congress_valid['Ticker'].nunique()}\")\n",
    "print(f\"\\nTop 10 most-traded tickers:\")\n",
    "print(congress_valid['Ticker'].value_counts().head(10))\n",
    "\n",
    "# By member\n",
    "member_cols = [col for col in congress_valid.columns if 'Member' in col or 'Name' in col or 'Congress' in col]\n",
    "if member_cols:\n",
    "    member_col = member_cols[0]\n",
    "    print(f\"\\nNumber of unique Congress members: {congress_valid[member_col].nunique()}\")\n",
    "    print(f\"\\nTop 10 most active traders:\")\n",
    "    print(congress_valid[member_col].value_counts().head(10))\n",
    "\n",
    "# Disclosure lag statistics on clean dataset\n",
    "print(f\"\\nDisclosure lag statistics (clean data):\")\n",
    "print(congress_valid['Disclosure_Lag'].describe())\n",
    "\n",
    "print(f\"\\n✓ Dataset is ready for event study analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Save Processed Dataset\n",
    "\n",
    "We save the cleaned dataset with all computed fields for use in subsequent analysis sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed dataset for use in later sections\n",
    "congress_valid.to_parquet('data/congress_trading_processed_section1.parquet', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nProcessed data saved to: data/congress_trading_processed_section1.parquet\")\n",
    "print(f\"Shape: {congress_valid.shape[0]:,} rows × {congress_valid.shape[1]} columns\")\n",
    "print(f\"\\nColumns in processed dataset:\")\n",
    "print(congress_valid.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
